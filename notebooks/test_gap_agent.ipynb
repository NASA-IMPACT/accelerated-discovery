{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15ebaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa8d6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from akd.agents.gap_analysis.gap_analysis import GapAgent, GapAgentConfig, GapInputSchema\n",
    "from akd.tools.scrapers import DoclingScraperConfig\n",
    "from akd.tools.search import SearxNGSearchTool, SearxNGSearchToolInputSchema, SemanticScholarSearchToolConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ddbf90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to view sources after the agent runs\n",
    "def print_sources(sources):\n",
    "    for k, v in sources.items():\n",
    "        print('title: ', v['title'])\n",
    "        print('section_title: ', v['section_title'])\n",
    "        print('url: ', v['url'])\n",
    "        print('locally generated content: ', v['content'])\n",
    "        print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da80c5",
   "metadata": {},
   "source": [
    "# Initialise Gap Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8632b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy your docling and semantic scholar configs here\n",
    "# Please make sure to set the export_type to 'html', gap agent works with this setting.\n",
    "# This will be modified when support is added for markdown\n",
    "\n",
    "docling_config = DoclingScraperConfig(do_table_structure=True, pdf_mode='accurate', export_type='html', debug=False)\n",
    "s2_config = SemanticScholarSearchToolConfig(debug=False, external_id=\"ARXIV\", fields = [\"paperId\", \"title\", \"externalIds\", \"isOpenAccess\", \"openAccessPdf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f1b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_agent_config = GapAgentConfig(docling_config=docling_config,\n",
    "                                  s2_tool_config=s2_config,\n",
    "                                 # use a larger model for better results   \n",
    "                                  model_name='gpt-4o-mini',\n",
    "                                  debug=True)\n",
    "\n",
    "gap_agent = GapAgent(gap_agent_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0105e9",
   "metadata": {},
   "source": [
    "# Describe your query and perform a search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'using large languagage models for tabular data extraction'\n",
    "\n",
    "# If you want to search for gaps in the particular topic, you can modify the query.capitalize\n",
    "# However, please note that this might bias the search towards gaps (search might return survey papers for example) instead of finding papers to investigate\n",
    "# query = 'existing theoretical gaps in using large languagage models for tabular data extraction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59fac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-18 09:48:26.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd._base\u001b[0m:\u001b[36marun\u001b[0m:\u001b[36m231\u001b[0m - \u001b[34m\u001b[1mRunning SearxNGSearchTool with params: queries=['using large languagage models for tabular data extraction'] category=None max_results=5\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:26.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_arun\u001b[0m:\u001b[36m293\u001b[0m - \u001b[1müîç SearxNG SEARCH QUERIES (1 total):\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:26.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_arun\u001b[0m:\u001b[36m295\u001b[0m - \u001b[1m  1. 'using large languagage models for tabular data extraction'\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:26.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_arun\u001b[0m:\u001b[36m296\u001b[0m - \u001b[1müéØ Target results per query: 7\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:26.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_arun\u001b[0m:\u001b[36m297\u001b[0m - \u001b[1müìÇ Category: None\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:26.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_arun\u001b[0m:\u001b[36m298\u001b[0m - \u001b[1müîß Engines: ['arxiv']\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:26.794\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_fetch_search_results_paginated\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mFetching page 1 for query: using large languagage models for tabular data extraction\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:27.425\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_fetch_search_results_paginated\u001b[0m:\u001b[36m237\u001b[0m - \u001b[34m\u001b[1mFetched 10 results for page 1\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:27.526\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_fetch_search_results_paginated\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mFetching page 2 for query: using large languagage models for tabular data extraction\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:27.978\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_fetch_search_results_paginated\u001b[0m:\u001b[36m237\u001b[0m - \u001b[34m\u001b[1mFetched 0 results for page 2\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:28.080\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_fetch_search_results_paginated\u001b[0m:\u001b[36m228\u001b[0m - \u001b[34m\u001b[1mFetching page 3 for query: using large languagage models for tabular data extraction\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:29.416\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_fetch_search_results_paginated\u001b[0m:\u001b[36m237\u001b[0m - \u001b[34m\u001b[1mFetched 10 results for page 3\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:29.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_fetch_search_results_paginated\u001b[0m:\u001b[36m256\u001b[0m - \u001b[34m\u001b[1mFetched 8 results across 3 pages for query: using large languagage models for tabular data extraction\u001b[0m\n",
      "\u001b[32m2025-08-18 09:48:29.520\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.tools.search.searxng_search\u001b[0m:\u001b[36m_arun\u001b[0m:\u001b[36m321\u001b[0m - \u001b[34m\u001b[1m[{'template': 'paper.html', 'url': 'http://arxiv.org/abs/2406.12031v2', 'title': 'Large Scale Transfer Learning for Tabular Data via Language Modeling', 'publishedDate': '2024-06-17T18:58:20', 'content': 'Tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. We release our model, code, and data along with the publication of this paper.', 'doi': 'None', 'authors': ['Josh Gardner', 'Juan C. Perdomo', 'Ludwig Schmidt'], 'journal': None, 'tags': ['cs.LG', 'cs.AI', 'cs.CL'], 'comments': 'NeurIPS 2024 camera-ready updates', 'pdf_url': 'http://arxiv.org/pdf/2406.12031v2', 'engine': 'arxiv', 'parsed_url': ['http', 'arxiv.org', '/abs/2406.12031v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [1], 'score': 1.0, 'category': 'science', 'query': 'using large languagage models for tabular data extraction'}, {'template': 'paper.html', 'url': 'http://arxiv.org/abs/2409.13882v2', 'title': 'Tabular Data Generation using Binary Diffusion', 'publishedDate': '2024-09-20T20:22:28', 'content': 'Generating synthetic tabular data is critical in machine learning, especially when real data is limited or sensitive. Traditional generative models often face challenges due to the unique characteristics of tabular data, such as mixed data types and varied distributions, and require complex preprocessing or large pretrained models. In this paper, we introduce a novel, lossless binary transformation method that converts any tabular data into fixed-size binary representations, and a corresponding new generative model called Binary Diffusion, specifically designed for binary data. Binary Diffusion leverages the simplicity of XOR operations for noise addition and removal and employs binary cross-entropy loss for training. Our approach eliminates the need for extensive preprocessing, complex noise parameter tuning, and pretraining on large datasets. We evaluate our model on several popular tabular benchmark datasets, demonstrating that Binary Diffusion outperforms existing state-of-the-art models on Travel, Adult Income, and Diabetes datasets while being significantly smaller in size. Code and models are available at: https://github.com/vkinakh/binary-diffusion-tabular', 'doi': 'None', 'authors': ['Vitaliy Kinakh', 'Slava Voloshynovskiy'], 'journal': None, 'tags': ['cs.LG', 'cs.AI'], 'comments': 'Accepted to 3rd Table Representation Learning Workshop @ NeurIPS 2024', 'pdf_url': 'http://arxiv.org/pdf/2409.13882v2', 'engine': 'arxiv', 'parsed_url': ['http', 'arxiv.org', '/abs/2409.13882v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [1], 'score': 1.0, 'category': 'science', 'query': 'using large languagage models for tabular data extraction'}, {'template': 'paper.html', 'url': 'http://arxiv.org/abs/2505.07453v2', 'title': 'How well do LLMs reason over tabular data, really?', 'publishedDate': '2025-05-12T11:35:28', 'content': \"Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM's realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM's performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.\", 'doi': 'None', 'authors': ['Cornelius Wolff', 'Madelon Hulsebos'], 'journal': None, 'tags': ['cs.AI'], 'comments': '10 pages, 4 figures', 'pdf_url': 'http://arxiv.org/pdf/2505.07453v2', 'engine': 'arxiv', 'parsed_url': ['http', 'arxiv.org', '/abs/2505.07453v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [2], 'score': 0.5, 'category': 'science', 'query': 'using large languagage models for tabular data extraction'}, {'template': 'paper.html', 'url': 'http://arxiv.org/abs/2501.00057v2', 'title': 'VisTabNet: Adapting Vision Transformers for Tabular Data', 'publishedDate': '2024-12-28T13:40:46', 'content': \"Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is challenging to transfer large-scale pre-trained models to downstream tasks defined on small tabular datasets. To address this, we propose VisTabNet -- a cross-modal transfer learning method, which allows for adapting Vision Transformer (ViT) with pre-trained weights to process tabular data. By projecting tabular inputs to patch embeddings acceptable by ViT, we can directly apply a pre-trained Transformer Encoder to tabular inputs. This approach eliminates the conceptual cost of designing a suitable architecture for processing tabular data, while reducing the computational cost of training the model from scratch. Experimental results on multiple small tabular datasets (less than 1k samples) demonstrate VisTabNet's superiority, outperforming both traditional ensemble methods and recent deep learning models. The proposed method goes beyond conventional transfer learning practice and shows that pre-trained image models can be transferred to solve tabular problems, extending the boundaries of transfer learning. We share our example implementation as a GitHub repository available at https://github.com/wwydmanski/VisTabNet.\", 'doi': 'None', 'authors': ['Witold Wydma≈Ñski', 'Ulvi Movsum-zada', 'Jacek Tabor', 'Marek ≈ömieja'], 'journal': None, 'tags': ['cs.LG', 'cs.AI', 'cs.CV'], 'comments': None, 'pdf_url': 'http://arxiv.org/pdf/2501.00057v2', 'engine': 'arxiv', 'parsed_url': ['http', 'arxiv.org', '/abs/2501.00057v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [2], 'score': 0.5, 'category': 'science', 'query': 'using large languagage models for tabular data extraction'}, {'template': 'paper.html', 'url': 'http://arxiv.org/abs/2507.17259v1', 'title': 'Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs', 'publishedDate': '2025-07-23T06:56:34', 'content': 'Large language models (LLMs) are increasingly trained on tabular data, which, unlike unstructured text, often contains personally identifiable information (PII) in a highly structured and explicit format. As a result, privacy risks arise, since sensitive records can be inadvertently retained by the model and exposed through data extraction or membership inference attacks (MIAs). While existing MIA methods primarily target textual content, their efficacy and threat implications may differ when applied to structured data, due to its limited content, diverse data types, unique value distributions, and column-level semantics. In this paper, we present Tab-MIA, a benchmark dataset for evaluating MIAs on tabular data in LLMs and demonstrate how it can be used. Tab-MIA comprises five data collections, each represented in six different encoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation of state-of-the-art MIA methods on LLMs finetuned with tabular data across multiple encoding formats. In the evaluation, we analyze the memorization behavior of pretrained LLMs on structured data derived from Wikipedia tables. Our findings show that LLMs memorize tabular data in ways that vary across encoding formats, making them susceptible to extraction via MIAs. Even when fine-tuned for as few as three epochs, models exhibit high vulnerability, with AUROC scores approaching 90% in most cases. Tab-MIA enables systematic evaluation of these risks and provides a foundation for developing privacy-preserving methods for tabular data in LLMs.', 'doi': 'None', 'authors': ['Eyal German', 'Sagiv Antebi', 'Daniel Samira', 'Asaf Shabtai', 'Yuval Elovici'], 'journal': None, 'tags': ['cs.CR', 'cs.CL'], 'comments': None, 'pdf_url': 'http://arxiv.org/pdf/2507.17259v1', 'engine': 'arxiv', 'parsed_url': ['http', 'arxiv.org', '/abs/2507.17259v1', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [3], 'score': 0.3333333333333333, 'category': 'science', 'query': 'using large languagage models for tabular data extraction'}]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Set the search engine here\n",
    "# It is recommended to use any engine that will return pdf_url in its search results\n",
    "search_tool = SearxNGSearchTool.from_params(engines=['arxiv'], debug=True)\n",
    "\n",
    "# Set category to None to adhere to the selected search engine\n",
    "# Set max_results to a higher value for better analysis\n",
    "search_output = await search_tool.arun(SearxNGSearchToolInputSchema(queries=[query], category=None, max_results=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6aaab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2406.12031v2 Large Scale Transfer Learning for Tabular Data via Language Modeling\n",
      "http://arxiv.org/abs/2409.13882v2 Tabular Data Generation using Binary Diffusion\n",
      "http://arxiv.org/abs/2505.07453v2 How well do LLMs reason over tabular data, really?\n",
      "http://arxiv.org/abs/2501.00057v2 VisTabNet: Adapting Vision Transformers for Tabular Data\n",
      "http://arxiv.org/abs/2507.17259v1 Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs\n"
     ]
    }
   ],
   "source": [
    "# View search results\n",
    "for res in search_output.results:\n",
    "    print(res.url, res.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f7671",
   "metadata": {},
   "source": [
    "# Run the gap agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758ca119",
   "metadata": {},
   "source": [
    "Please note that the tool is built with the following idea in mind:\n",
    "- User performs a literature search and retrieves a collection of results.\n",
    "- The agent examines these results to identify and analyze the specified research gap.\n",
    "\n",
    "If you start your search with the gap in mind, be aware that the search may return results that are not directly related to your intended topic but could still help in assessing the gap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc966eb6",
   "metadata": {},
   "source": [
    "## To test for pre-defined gaps\n",
    "The following pre-defined gaps are supported now: knowledge, evidence, theoretical, methodological, population, geographical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41812fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-18 09:49:02.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd._base\u001b[0m:\u001b[36marun\u001b[0m:\u001b[36m231\u001b[0m - \u001b[34m\u001b[1mRunning GapAgent with params: search_results=[SearchResultItem(url=AnyUrl('http://arxiv.org/abs/2406.12031v2'), title='Large Scale Transfer Learning for Tabular Data via Language Modeling', query='using large languagage models for tabular data extraction', pdf_url=AnyUrl('http://arxiv.org/pdf/2406.12031v2'), content='Tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. We release our model, code, and data along with the publication of this paper.', category='science', doi='None', published_date='2024-06-17T18:58:20', engine='arxiv', tags=['cs.LG', 'cs.AI', 'cs.CL'], score=None, extra={'template': 'paper.html', 'authors': ['Josh Gardner', 'Juan C. Perdomo', 'Ludwig Schmidt'], 'journal': None, 'comments': 'NeurIPS 2024 camera-ready updates', 'parsed_url': ['http', 'arxiv.org', '/abs/2406.12031v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [1], 'score': 1.0}, title_augmented='Large Scale Transfer Learning for Tabular Data via Language Modeling - (Published 2024-06-17T18:58:20)', relevancy_score=None), SearchResultItem(url=AnyUrl('http://arxiv.org/abs/2409.13882v2'), title='Tabular Data Generation using Binary Diffusion', query='using large languagage models for tabular data extraction', pdf_url=AnyUrl('http://arxiv.org/pdf/2409.13882v2'), content='Generating synthetic tabular data is critical in machine learning, especially when real data is limited or sensitive. Traditional generative models often face challenges due to the unique characteristics of tabular data, such as mixed data types and varied distributions, and require complex preprocessing or large pretrained models. In this paper, we introduce a novel, lossless binary transformation method that converts any tabular data into fixed-size binary representations, and a corresponding new generative model called Binary Diffusion, specifically designed for binary data. Binary Diffusion leverages the simplicity of XOR operations for noise addition and removal and employs binary cross-entropy loss for training. Our approach eliminates the need for extensive preprocessing, complex noise parameter tuning, and pretraining on large datasets. We evaluate our model on several popular tabular benchmark datasets, demonstrating that Binary Diffusion outperforms existing state-of-the-art models on Travel, Adult Income, and Diabetes datasets while being significantly smaller in size. Code and models are available at: https://github.com/vkinakh/binary-diffusion-tabular', category='science', doi='None', published_date='2024-09-20T20:22:28', engine='arxiv', tags=['cs.LG', 'cs.AI'], score=None, extra={'template': 'paper.html', 'authors': ['Vitaliy Kinakh', 'Slava Voloshynovskiy'], 'journal': None, 'comments': 'Accepted to 3rd Table Representation Learning Workshop @ NeurIPS 2024', 'parsed_url': ['http', 'arxiv.org', '/abs/2409.13882v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [1], 'score': 1.0}, title_augmented='Tabular Data Generation using Binary Diffusion - (Published 2024-09-20T20:22:28)', relevancy_score=None), SearchResultItem(url=AnyUrl('http://arxiv.org/abs/2505.07453v2'), title='How well do LLMs reason over tabular data, really?', query='using large languagage models for tabular data extraction', pdf_url=AnyUrl('http://arxiv.org/pdf/2505.07453v2'), content=\"Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM's realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM's performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.\", category='science', doi='None', published_date='2025-05-12T11:35:28', engine='arxiv', tags=['cs.AI'], score=None, extra={'template': 'paper.html', 'authors': ['Cornelius Wolff', 'Madelon Hulsebos'], 'journal': None, 'comments': '10 pages, 4 figures', 'parsed_url': ['http', 'arxiv.org', '/abs/2505.07453v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [2], 'score': 0.5}, title_augmented='How well do LLMs reason over tabular data, really? - (Published 2025-05-12T11:35:28)', relevancy_score=None), SearchResultItem(url=AnyUrl('http://arxiv.org/abs/2501.00057v2'), title='VisTabNet: Adapting Vision Transformers for Tabular Data', query='using large languagage models for tabular data extraction', pdf_url=AnyUrl('http://arxiv.org/pdf/2501.00057v2'), content=\"Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is challenging to transfer large-scale pre-trained models to downstream tasks defined on small tabular datasets. To address this, we propose VisTabNet -- a cross-modal transfer learning method, which allows for adapting Vision Transformer (ViT) with pre-trained weights to process tabular data. By projecting tabular inputs to patch embeddings acceptable by ViT, we can directly apply a pre-trained Transformer Encoder to tabular inputs. This approach eliminates the conceptual cost of designing a suitable architecture for processing tabular data, while reducing the computational cost of training the model from scratch. Experimental results on multiple small tabular datasets (less than 1k samples) demonstrate VisTabNet's superiority, outperforming both traditional ensemble methods and recent deep learning models. The proposed method goes beyond conventional transfer learning practice and shows that pre-trained image models can be transferred to solve tabular problems, extending the boundaries of transfer learning. We share our example implementation as a GitHub repository available at https://github.com/wwydmanski/VisTabNet.\", category='science', doi='None', published_date='2024-12-28T13:40:46', engine='arxiv', tags=['cs.LG', 'cs.AI', 'cs.CV'], score=None, extra={'template': 'paper.html', 'authors': ['Witold Wydma≈Ñski', 'Ulvi Movsum-zada', 'Jacek Tabor', 'Marek ≈ömieja'], 'journal': None, 'comments': None, 'parsed_url': ['http', 'arxiv.org', '/abs/2501.00057v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [2], 'score': 0.5}, title_augmented='VisTabNet: Adapting Vision Transformers for Tabular Data - (Published 2024-12-28T13:40:46)', relevancy_score=None), SearchResultItem(url=AnyUrl('http://arxiv.org/abs/2507.17259v1'), title='Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs', query='using large languagage models for tabular data extraction', pdf_url=AnyUrl('http://arxiv.org/pdf/2507.17259v1'), content='Large language models (LLMs) are increasingly trained on tabular data, which, unlike unstructured text, often contains personally identifiable information (PII) in a highly structured and explicit format. As a result, privacy risks arise, since sensitive records can be inadvertently retained by the model and exposed through data extraction or membership inference attacks (MIAs). While existing MIA methods primarily target textual content, their efficacy and threat implications may differ when applied to structured data, due to its limited content, diverse data types, unique value distributions, and column-level semantics. In this paper, we present Tab-MIA, a benchmark dataset for evaluating MIAs on tabular data in LLMs and demonstrate how it can be used. Tab-MIA comprises five data collections, each represented in six different encoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation of state-of-the-art MIA methods on LLMs finetuned with tabular data across multiple encoding formats. In the evaluation, we analyze the memorization behavior of pretrained LLMs on structured data derived from Wikipedia tables. Our findings show that LLMs memorize tabular data in ways that vary across encoding formats, making them susceptible to extraction via MIAs. Even when fine-tuned for as few as three epochs, models exhibit high vulnerability, with AUROC scores approaching 90% in most cases. Tab-MIA enables systematic evaluation of these risks and provides a foundation for developing privacy-preserving methods for tabular data in LLMs.', category='science', doi='None', published_date='2025-07-23T06:56:34', engine='arxiv', tags=['cs.CR', 'cs.CL'], score=None, extra={'template': 'paper.html', 'authors': ['Eyal German', 'Sagiv Antebi', 'Daniel Samira', 'Asaf Shabtai', 'Yuval Elovici'], 'journal': None, 'comments': None, 'parsed_url': ['http', 'arxiv.org', '/abs/2507.17259v1', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [3], 'score': 0.3333333333333333}, title_augmented='Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs - (Published 2025-07-23T06:56:34)', relevancy_score=None)] gap='evidence'\u001b[0m\n",
      "\u001b[32m2025-08-18 09:49:02.687\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.agents.gap_analysis.gap_analysis\u001b[0m:\u001b[36mget_response_async\u001b[0m:\u001b[36m206\u001b[0m - \u001b[34m\u001b[1mRunning gap analysis to investigate evidence gap.\u001b[0m\n",
      "\u001b[32m2025-08-18 09:49:07.967\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.agents.gap_analysis.gap_analysis\u001b[0m:\u001b[36mget_response_async\u001b[0m:\u001b[36m210\u001b[0m - \u001b[34m\u001b[1mFetch 5 papers from semantic scholar.\u001b[0m\n",
      "Could not parse formula with MathML\n",
      "Could not parse formula with MathML\n",
      "Could not parse formula with MathML\n",
      "Could not parse formula with MathML\n",
      "\u001b[32m2025-08-18 09:52:55.808\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.agents.gap_analysis.gap_analysis\u001b[0m:\u001b[36mget_response_async\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mCreated Graph with 149 nodes and 144 edges\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# This example analyses evidence gaps\n",
    "evidence_gap_output = await gap_agent.arun(GapInputSchema(search_results=search_output.results, gap=\"evidence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e870732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Overview of Empirical Evidence in Literature\n",
      "\n",
      "The inquiry into the availability of robust empirical evidence in the literature reveals a nuanced landscape. The sources reviewed provide a mixed picture regarding the support for core claims, the sufficiency of theoretical assertions, and the acknowledgment of the need for further empirical validation.\n",
      "\n",
      "### Shortage of Empirical Evidence\n",
      "\n",
      "1. **General Findings**:\n",
      "   - Most sources do not explicitly mention a **shortage of robust empirical evidence** such as experiments, trials, or longitudinal studies to support or challenge core claims made in the literature. For instance, several sections focus on methodologies and model evaluations without addressing broader empirical validation concerns.\n",
      "\n",
      "2. **Specific Instances**:\n",
      "   - Notably, one source indicates a **shortage of robust empirical evidence** regarding the evaluation of multi-table reasoning and free-form question answering. It highlights that existing evaluation metrics fail to provide reliable signals for correctness, suggesting a need for stronger empirical validation and the collection of primary data.\n",
      "\n",
      "3. **Evaluation Metrics**:\n",
      "   - Another source points out that common evaluation metrics, like SacreBleu, do not effectively distinguish between correct and incorrect answers, further emphasizing the need for improved empirical validation methods.\n",
      "\n",
      "### Theoretical Assertions and Empirical Backing\n",
      "\n",
      "1. **Lack of Quantitative or Qualitative Support**:\n",
      "   - The majority of sources do not indicate that **theoretical assertions** are made without sufficient quantitative or qualitative backing. They primarily focus on methodologies and performance evaluations rather than critiquing the theoretical foundations of the claims.\n",
      "\n",
      "2. **Acknowledgment of Gaps**:\n",
      "   - While some sources do not explicitly state the need for more primary data collection, the discussions around the limitations of existing metrics and methodologies imply an **acknowledgment of gaps** in empirical support.\n",
      "\n",
      "### Review Papers and Calls for Data Collection\n",
      "\n",
      "1. **Explicit Notes on Data Collection**:\n",
      "   - There is a notable absence of references to **review papers** or authors explicitly calling for more primary data collection or stronger empirical validation across most sources. The focus tends to remain on specific methodologies or model performances rather than on the overarching need for additional empirical research.\n",
      "\n",
      "2. **Future Research Directions**:\n",
      "   - Some sections suggest future research directions but do not directly link these suggestions to a need for empirical validation or data collection, indicating a potential area for further exploration in the literature.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "In summary, while there are indications of a **shortage of robust empirical evidence** in specific contexts, particularly regarding evaluation metrics, the overall literature does not uniformly acknowledge this issue. Theoretical assertions generally appear to be supported, but there is a lack of explicit calls for more primary data collection or empirical validation in many sources. This highlights an opportunity for future research to address these gaps and strengthen the empirical foundations of the claims made in the literature.\n"
     ]
    }
   ],
   "source": [
    "# To view output\n",
    "print(evidence_gap_output.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e04f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Large Scale Transfer Learning for Tabular Data via Language Modeling\n",
      "section_title:  6 Discussion\n",
      "url:  http://arxiv.org/abs/2406.12031v2\n",
      "locally generated content:  The provided content does not explicitly mention a shortage of robust empirical evidence, nor does it indicate that theoretical assertions are made without sufficient quantitative or qualitative backing. Additionally, there is no mention of review papers or authors explicitly noting the need for more primary data collection or stronger empirical validation. The focus of the section is primarily on the limitations of the TABULA-8B model and suggestions for future research directions rather than on the empirical support for claims made in the literature.\n",
      "--------------------\n",
      "title:  Large Scale Transfer Learning for Tabular Data via Language Modeling\n",
      "section_title:  2 Related Work\n",
      "url:  http://arxiv.org/abs/2406.12031v2\n",
      "locally generated content:  The provided content does not explicitly mention a shortage of robust empirical evidence, such as experiments, trials, longitudinal studies, or real-world data, to support, challenge, or validate the core claims made in the literature. It does not indicate whether theoretical assertions are made without sufficient quantitative or qualitative backing. Additionally, there is no mention of review papers or authors explicitly noting the need for more primary data collection or stronger empirical validation. The focus of the section is primarily on the development and challenges related to foundation models, dataset curation, and methods for tabular prediction without addressing the empirical evidence or validation concerns directly.\n",
      "--------------------\n",
      "title:  Large Scale Transfer Learning for Tabular Data via Language Modeling\n",
      "section_title:  3 TABULA-8B - Model Design and Training\n",
      "url:  http://arxiv.org/abs/2406.12031v2\n",
      "locally generated content:  The provided content does not explicitly mention a shortage of robust empirical evidence, nor does it indicate that theoretical assertions are made without sufficient quantitative or qualitative backing. There is no reference to review papers or authors noting the need for more primary data collection or stronger empirical validation. The focus of the section is primarily on the methodology of training the TABULA-8B model and does not address the broader context of empirical evidence in the literature.\n",
      "--------------------\n",
      "title:  Large Scale Transfer Learning for Tabular Data via Language Modeling\n",
      "section_title:  4 Dataset Construction: Building The Tremendous TabLib Trawl (T4)\n",
      "url:  http://arxiv.org/abs/2406.12031v2\n",
      "locally generated content:  The provided content does not explicitly mention a shortage of robust empirical evidence, such as experiments, trials, longitudinal studies, or real-world data, to support, challenge, or validate core claims made in the literature. It does not indicate whether theoretical assertions are made without sufficient quantitative or qualitative backing. Additionally, there is no mention of review papers or authors explicitly noting the need for more primary data collection or stronger empirical validation. The focus of the section is primarily on the methodology for constructing the T4 dataset from the TabLib corpus, including filtering strategies and unsupervised task selection.\n",
      "--------------------\n",
      "title:  Tabular Data Generation using Binary Diffusion\n",
      "section_title:  5 Results\n",
      "url:  http://arxiv.org/abs/2409.13882v2\n",
      "locally generated content:  The provided content does not indicate a shortage of robust empirical evidence to support the claims made in the literature. It details the evaluation of Binary Diffusion on recognized benchmark datasets, using established metrics such as classification accuracy and mean squared error (MSE). The results are based on systematic experiments involving multiple models and a clear evaluation protocol. However, there is no mention of theoretical assertions lacking quantitative or qualitative backing, nor do the review papers or authors explicitly note a need for more primary data collection or stronger empirical validation. The focus is primarily on the performance results of Binary Diffusion compared to other models.\n",
      "--------------------\n",
      "title:  Tabular Data Generation using Binary Diffusion\n",
      "section_title:  2 Related Work\n",
      "url:  http://arxiv.org/abs/2409.13882v2\n",
      "locally generated content:  The provided content does not explicitly mention a shortage of robust empirical evidence, such as experiments, trials, longitudinal studies, or real-world data, to support, challenge, or validate the core claims made in the literature. It also does not indicate whether theoretical assertions are made without sufficient quantitative or qualitative backing. Furthermore, there is no mention of review papers or authors explicitly noting the need for more primary data collection or stronger empirical validation. The focus of the content is primarily on the description of various data generation methods and their shortcomings, rather than on the empirical validation of claims.\n",
      "--------------------\n",
      "title:  Tabular Data Generation using Binary Diffusion\n",
      "section_title:  3 Data transformation\n",
      "url:  http://arxiv.org/abs/2409.13882v2\n",
      "locally generated content:  The provided content does not address the existence of a shortage of robust empirical evidence, nor does it discuss whether theoretical assertions are made without sufficient quantitative or qualitative backing. Additionally, there is no mention of review papers or authors explicitly noting the need for more primary data collection or stronger empirical validation. The section focuses solely on the methodology of data transformation for the Binary Diffusion model.\n",
      "--------------------\n",
      "title:  Tabular Data Generation using Binary Diffusion\n",
      "section_title:  4 Binary Diffusion\n",
      "url:  http://arxiv.org/abs/2409.13882v2\n",
      "locally generated content:  The provided content does not explicitly mention any shortage of robust empirical evidence, such as experiments, trials, longitudinal studies, or real-world data, to support, challenge, or validate the core claims made in the literature. It also does not indicate whether theoretical assertions are made without sufficient quantitative or qualitative backing. Furthermore, there is no mention of review papers or authors explicitly noting the need for more primary data collection or stronger empirical validation. The section primarily focuses on the methodology of Binary Diffusion without addressing these concerns.\n",
      "--------------------\n",
      "title:  How well do LLMs reason over tabular data, really?\n",
      "section_title:  2 Related Work\n",
      "url:  http://arxiv.org/abs/2505.07453v2\n",
      "locally generated content:  The provided content does not explicitly mention a shortage of robust empirical evidence, nor does it indicate that theoretical assertions are made without sufficient quantitative or qualitative backing. Additionally, there is no mention of review papers or authors explicitly noting the need for more primary data collection or stronger empirical validation. The focus of the section is primarily on various benchmarks and evaluations related to tabular reasoning capabilities of LLMs, rather than on the overall state of empirical evidence in the literature.\n",
      "--------------------\n",
      "title:  How well do LLMs reason over tabular data, really?\n",
      "section_title:  3 The TQA-Bench and Revisions\n",
      "url:  http://arxiv.org/abs/2505.07453v2\n",
      "locally generated content:  The provided content does not address the presence or absence of robust empirical evidence, the sufficiency of theoretical assertions, or whether review papers or authors note the need for more primary data collection or stronger empirical validation. It focuses solely on the methodology related to the TQA-Bench and the assessment of evaluation metrics and tabular reasoning capabilities of LLMs.\n",
      "--------------------\n",
      "title:  How well do LLMs reason over tabular data, really?\n",
      "section_title:  4 Towards Reliable Evaluation of Tabular Reasoning Capabilities\n",
      "url:  http://arxiv.org/abs/2505.07453v2\n",
      "locally generated content:  Yes, there is a shortage of robust empirical evidence to support, challenge, or validate the core claims made in the literature regarding the evaluation of multi-table reasoning and free-form question answering. The section indicates that existing evaluation metrics, such as SacreBleu and BERT-score, fail to provide reliable signals for assessing the correctness of generated answers against ground-truth answers. The authors note that these metrics do not distinguish between correct and incorrect answers effectively, highlighting a need for stronger empirical validation.\n",
      "\n",
      "Additionally, the authors propose the use of LLM-as-a-judge as a more reliable evaluation method, suggesting that current theoretical assertions about evaluation metrics lack sufficient quantitative backing. They also imply the necessity for more primary data collection and stronger empirical validation by demonstrating the limitations of existing metrics and advocating for a new approach. However, the section does not explicitly state that review papers or authors have noted the need for more primary data collection.\n",
      "--------------------\n",
      "title:  How well do LLMs reason over tabular data, really?\n",
      "section_title:  5 On the Realistic Tabular Reasoning Capabilities of LLMs\n",
      "url:  http://arxiv.org/abs/2505.07453v2\n",
      "locally generated content:  The provided content does not indicate a shortage of robust empirical evidence or mention any theoretical assertions made without sufficient backing. It does not explicitly note the need for more primary data collection or stronger empirical validation in the context of the literature discussed. The focus is on examining LLMs' performance and extending benchmarks rather than addressing gaps in empirical evidence or the need for further studies.\n",
      "--------------------\n",
      "title:  How well do LLMs reason over tabular data, really?\n",
      "section_title:  Conclusion\n",
      "url:  http://arxiv.org/abs/2505.07453v2\n",
      "locally generated content:  Yes, there is a shortage of robust empirical evidence to support, challenge, or validate the core claims made in the literature regarding LLMs' tabular reasoning abilities. The studies mentioned often lack reliable evaluations and robustness checks. Theoretical assertions are made without sufficient quantitative or qualitative backing, as indicated by the limitations of common evaluation metrics like SacreBleu. Additionally, the findings highlight the need for further advancements in LLM architectures and training, suggesting that there is an explicit acknowledgment of the necessity for more primary data collection or stronger empirical validation.\n",
      "--------------------\n",
      "title:  VisTabNet: Adapting Vision Transformers for Tabular Data\n",
      "section_title:  2 Related Work\n",
      "url:  http://arxiv.org/abs/2501.00057v2\n",
      "locally generated content:  The provided content does not explicitly mention a shortage of robust empirical evidence, nor does it indicate that theoretical assertions are made without sufficient quantitative or qualitative backing. Additionally, there is no mention of review papers or authors explicitly noting the need for more primary data collection or stronger empirical validation. The focus of the section is primarily on the performance of various models and architectures in relation to tabular data, without addressing the empirical support for the claims made in the literature.\n",
      "--------------------\n",
      "title:  VisTabNet: Adapting Vision Transformers for Tabular Data\n",
      "section_title:  3 VisTabNet model\n",
      "url:  http://arxiv.org/abs/2501.00057v2\n",
      "locally generated content:  The provided content does not explicitly mention any shortage of robust empirical evidence, such as experiments, trials, longitudinal studies, or real-world data, to support, challenge, or validate the core claims made in the literature. It also does not indicate whether theoretical assertions are made without sufficient quantitative or qualitative backing. Furthermore, there is no mention of review papers or authors explicitly noting the need for more primary data collection or stronger empirical validation. The focus of the section is primarily on the methodology and technical details of the VisTabNet model and its transfer learning capabilities.\n",
      "--------------------\n",
      "title:  VisTabNet: Adapting Vision Transformers for Tabular Data\n",
      "section_title:  4 Experiments\n",
      "url:  http://arxiv.org/abs/2501.00057v2\n",
      "locally generated content:  The provided content does not explicitly mention a shortage of robust empirical evidence or the need for more primary data collection or stronger empirical validation. It focuses on the experimental evaluation of VisTabNet, comparing it with various established methods in tabular data classification and discussing its performance, particularly in scenarios with small datasets and few-shot learning. The section details the experimental setup, methodology, and results, indicating that VisTabNet achieves high performance and stability compared to other models. However, there is no indication that the authors note a lack of quantitative or qualitative backing for their claims or that they call for additional empirical validation in the literature.\n",
      "--------------------\n",
      "title:  VisTabNet: Adapting Vision Transformers for Tabular Data\n",
      "section_title:  A Experimental setup\n",
      "url:  http://arxiv.org/abs/2501.00057v2\n",
      "locally generated content:  The provided content does not explicitly address the existence of a shortage of robust empirical evidence, nor does it mention whether theoretical assertions are made without sufficient quantitative or qualitative backing. Additionally, there is no indication that review papers or authors note the need for more primary data collection or stronger empirical validation. The section primarily focuses on the experimental setup, including data splitting, preprocessing, and hyperparameter optimization, without discussing the broader context of empirical evidence in the literature.\n",
      "--------------------\n",
      "title:  VisTabNet: Adapting Vision Transformers for Tabular Data\n",
      "section_title:  B Detailed results\n",
      "url:  http://arxiv.org/abs/2501.00057v2\n",
      "locally generated content:  The provided content does not explicitly mention a shortage of robust empirical evidence, nor does it indicate that theoretical assertions are made without sufficient quantitative or qualitative backing. Additionally, there is no reference to review papers or authors noting the need for more primary data collection or stronger empirical validation. The content primarily focuses on the performance metrics and training phases of the VisTabNet model without addressing the broader context of empirical evidence in the literature.\n",
      "--------------------\n",
      "title:  VisTabNet: Adapting Vision Transformers for Tabular Data\n",
      "section_title:  5 Conclusion\n",
      "url:  http://arxiv.org/abs/2501.00057v2\n",
      "locally generated content:  The provided content does not indicate a shortage of robust empirical evidence or mention any specific lack of quantitative or qualitative backing for theoretical assertions. Additionally, there is no explicit note in the conclusion regarding the need for more primary data collection or stronger empirical validation. The focus of the conclusion is primarily on the introduction of a new approach to cross-modal transfer and suggestions for future work, rather than on the evaluation of existing literature or empirical evidence.\n",
      "--------------------\n",
      "title:  Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs\n",
      "section_title:  5 Results\n",
      "url:  http://arxiv.org/abs/2507.17259v1\n",
      "locally generated content:  The provided content does not indicate a shortage of robust empirical evidence or mention any theoretical assertions made without sufficient backing. It focuses on presenting empirical findings related to the evaluation of MIAs on tabular data in LLMs, highlighting trends influenced by various factors. There is no explicit note regarding the need for more primary data collection or stronger empirical validation in the content provided.\n",
      "--------------------\n",
      "title:  Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs\n",
      "section_title:  2 Related Work\n",
      "url:  http://arxiv.org/abs/2507.17259v1\n",
      "locally generated content:  The provided content does not explicitly mention a shortage of robust empirical evidence, nor does it address whether theoretical assertions are made without sufficient backing. Additionally, there is no indication that review papers or authors note the need for more primary data collection or stronger empirical validation. The section primarily focuses on the capabilities of LLMs and the areas of prior work being reviewed.\n",
      "--------------------\n",
      "title:  Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs\n",
      "section_title:  3 Construction of the Tab-MIA Benchmark\n",
      "url:  http://arxiv.org/abs/2507.17259v1\n",
      "locally generated content:  The provided content does not indicate a shortage of robust empirical evidence or mention any theoretical assertions made without sufficient backing. It does not explicitly note the need for more primary data collection or stronger empirical validation in the context of the Tab-MIA benchmark. The focus is on the construction of the benchmark for systematic evaluation rather than on the existing literature's empirical support or the need for further data.\n",
      "--------------------\n",
      "title:  Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs\n",
      "section_title:  4 Experimental Setup\n",
      "url:  http://arxiv.org/abs/2507.17259v1\n",
      "locally generated content:  The provided content does not explicitly mention a shortage of robust empirical evidence or the need for more primary data collection. It focuses on the experimental setup and methodology used to evaluate the vulnerability of fine-tuned language models to membership inference attacks (MIAs). The section describes the models, training configurations, and the metrics used for analysis, but it does not address whether theoretical assertions are made without sufficient backing or if review papers note the need for stronger empirical validation. Therefore, based on the content provided, there is no indication of these issues being discussed.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# To view sources\n",
    "print_sources(evidence_gap_output.attributed_source_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95662e9",
   "metadata": {},
   "source": [
    "# You can also define your own gap or query to analyse the retrieved corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f16cf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-18 09:53:44.989\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd._base\u001b[0m:\u001b[36marun\u001b[0m:\u001b[36m231\u001b[0m - \u001b[34m\u001b[1mRunning GapAgent with params: search_results=[SearchResultItem(url=AnyUrl('http://arxiv.org/abs/2406.12031v2'), title='Large Scale Transfer Learning for Tabular Data via Language Modeling', query='using large languagage models for tabular data extraction', pdf_url=AnyUrl('http://arxiv.org/pdf/2406.12031v2'), content='Tabular data -- structured, heterogeneous, spreadsheet-style data with rows and columns -- is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TabuLa-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TabuLa-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16x more data. We release our model, code, and data along with the publication of this paper.', category='science', doi='None', published_date='2024-06-17T18:58:20', engine='arxiv', tags=['cs.LG', 'cs.AI', 'cs.CL'], score=None, extra={'template': 'paper.html', 'authors': ['Josh Gardner', 'Juan C. Perdomo', 'Ludwig Schmidt'], 'journal': None, 'comments': 'NeurIPS 2024 camera-ready updates', 'parsed_url': ['http', 'arxiv.org', '/abs/2406.12031v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [1], 'score': 1.0}, title_augmented='Large Scale Transfer Learning for Tabular Data via Language Modeling - (Published 2024-06-17T18:58:20)', relevancy_score=None), SearchResultItem(url=AnyUrl('http://arxiv.org/abs/2409.13882v2'), title='Tabular Data Generation using Binary Diffusion', query='using large languagage models for tabular data extraction', pdf_url=AnyUrl('http://arxiv.org/pdf/2409.13882v2'), content='Generating synthetic tabular data is critical in machine learning, especially when real data is limited or sensitive. Traditional generative models often face challenges due to the unique characteristics of tabular data, such as mixed data types and varied distributions, and require complex preprocessing or large pretrained models. In this paper, we introduce a novel, lossless binary transformation method that converts any tabular data into fixed-size binary representations, and a corresponding new generative model called Binary Diffusion, specifically designed for binary data. Binary Diffusion leverages the simplicity of XOR operations for noise addition and removal and employs binary cross-entropy loss for training. Our approach eliminates the need for extensive preprocessing, complex noise parameter tuning, and pretraining on large datasets. We evaluate our model on several popular tabular benchmark datasets, demonstrating that Binary Diffusion outperforms existing state-of-the-art models on Travel, Adult Income, and Diabetes datasets while being significantly smaller in size. Code and models are available at: https://github.com/vkinakh/binary-diffusion-tabular', category='science', doi='None', published_date='2024-09-20T20:22:28', engine='arxiv', tags=['cs.LG', 'cs.AI'], score=None, extra={'template': 'paper.html', 'authors': ['Vitaliy Kinakh', 'Slava Voloshynovskiy'], 'journal': None, 'comments': 'Accepted to 3rd Table Representation Learning Workshop @ NeurIPS 2024', 'parsed_url': ['http', 'arxiv.org', '/abs/2409.13882v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [1], 'score': 1.0}, title_augmented='Tabular Data Generation using Binary Diffusion - (Published 2024-09-20T20:22:28)', relevancy_score=None), SearchResultItem(url=AnyUrl('http://arxiv.org/abs/2505.07453v2'), title='How well do LLMs reason over tabular data, really?', query='using large languagage models for tabular data extraction', pdf_url=AnyUrl('http://arxiv.org/pdf/2505.07453v2'), content=\"Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM's realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM's performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.\", category='science', doi='None', published_date='2025-05-12T11:35:28', engine='arxiv', tags=['cs.AI'], score=None, extra={'template': 'paper.html', 'authors': ['Cornelius Wolff', 'Madelon Hulsebos'], 'journal': None, 'comments': '10 pages, 4 figures', 'parsed_url': ['http', 'arxiv.org', '/abs/2505.07453v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [2], 'score': 0.5}, title_augmented='How well do LLMs reason over tabular data, really? - (Published 2025-05-12T11:35:28)', relevancy_score=None), SearchResultItem(url=AnyUrl('http://arxiv.org/abs/2501.00057v2'), title='VisTabNet: Adapting Vision Transformers for Tabular Data', query='using large languagage models for tabular data extraction', pdf_url=AnyUrl('http://arxiv.org/pdf/2501.00057v2'), content=\"Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is challenging to transfer large-scale pre-trained models to downstream tasks defined on small tabular datasets. To address this, we propose VisTabNet -- a cross-modal transfer learning method, which allows for adapting Vision Transformer (ViT) with pre-trained weights to process tabular data. By projecting tabular inputs to patch embeddings acceptable by ViT, we can directly apply a pre-trained Transformer Encoder to tabular inputs. This approach eliminates the conceptual cost of designing a suitable architecture for processing tabular data, while reducing the computational cost of training the model from scratch. Experimental results on multiple small tabular datasets (less than 1k samples) demonstrate VisTabNet's superiority, outperforming both traditional ensemble methods and recent deep learning models. The proposed method goes beyond conventional transfer learning practice and shows that pre-trained image models can be transferred to solve tabular problems, extending the boundaries of transfer learning. We share our example implementation as a GitHub repository available at https://github.com/wwydmanski/VisTabNet.\", category='science', doi='None', published_date='2024-12-28T13:40:46', engine='arxiv', tags=['cs.LG', 'cs.AI', 'cs.CV'], score=None, extra={'template': 'paper.html', 'authors': ['Witold Wydma≈Ñski', 'Ulvi Movsum-zada', 'Jacek Tabor', 'Marek ≈ömieja'], 'journal': None, 'comments': None, 'parsed_url': ['http', 'arxiv.org', '/abs/2501.00057v2', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [2], 'score': 0.5}, title_augmented='VisTabNet: Adapting Vision Transformers for Tabular Data - (Published 2024-12-28T13:40:46)', relevancy_score=None), SearchResultItem(url=AnyUrl('http://arxiv.org/abs/2507.17259v1'), title='Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs', query='using large languagage models for tabular data extraction', pdf_url=AnyUrl('http://arxiv.org/pdf/2507.17259v1'), content='Large language models (LLMs) are increasingly trained on tabular data, which, unlike unstructured text, often contains personally identifiable information (PII) in a highly structured and explicit format. As a result, privacy risks arise, since sensitive records can be inadvertently retained by the model and exposed through data extraction or membership inference attacks (MIAs). While existing MIA methods primarily target textual content, their efficacy and threat implications may differ when applied to structured data, due to its limited content, diverse data types, unique value distributions, and column-level semantics. In this paper, we present Tab-MIA, a benchmark dataset for evaluating MIAs on tabular data in LLMs and demonstrate how it can be used. Tab-MIA comprises five data collections, each represented in six different encoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation of state-of-the-art MIA methods on LLMs finetuned with tabular data across multiple encoding formats. In the evaluation, we analyze the memorization behavior of pretrained LLMs on structured data derived from Wikipedia tables. Our findings show that LLMs memorize tabular data in ways that vary across encoding formats, making them susceptible to extraction via MIAs. Even when fine-tuned for as few as three epochs, models exhibit high vulnerability, with AUROC scores approaching 90% in most cases. Tab-MIA enables systematic evaluation of these risks and provides a foundation for developing privacy-preserving methods for tabular data in LLMs.', category='science', doi='None', published_date='2025-07-23T06:56:34', engine='arxiv', tags=['cs.CR', 'cs.CL'], score=None, extra={'template': 'paper.html', 'authors': ['Eyal German', 'Sagiv Antebi', 'Daniel Samira', 'Asaf Shabtai', 'Yuval Elovici'], 'journal': None, 'comments': None, 'parsed_url': ['http', 'arxiv.org', '/abs/2507.17259v1', '', '', ''], 'img_src': '', 'thumbnail': '', 'priority': '', 'engines': ['arxiv'], 'positions': [3], 'score': 0.3333333333333333}, title_augmented='Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs - (Published 2025-07-23T06:56:34)', relevancy_score=None)] gap='Is there a disconnect between theoretical findings and their practical implementation?'\u001b[0m\n",
      "\u001b[32m2025-08-18 09:53:44.990\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.agents.gap_analysis.gap_analysis\u001b[0m:\u001b[36mget_response_async\u001b[0m:\u001b[36m204\u001b[0m - \u001b[34m\u001b[1mYou are running the gap agent with your own defined gap. Please ensure you have described the gap you want to investigate in detail.\u001b[0m\n",
      "\u001b[32m2025-08-18 09:53:45.190\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.agents.gap_analysis.gap_analysis\u001b[0m:\u001b[36mget_response_async\u001b[0m:\u001b[36m210\u001b[0m - \u001b[34m\u001b[1mFetch 5 papers from semantic scholar.\u001b[0m\n",
      "Could not parse formula with MathML\n",
      "Could not parse formula with MathML\n",
      "Could not parse formula with MathML\n",
      "Could not parse formula with MathML\n",
      "\u001b[32m2025-08-18 09:57:42.117\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36makd.agents.gap_analysis.gap_analysis\u001b[0m:\u001b[36mget_response_async\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mCreated Graph with 149 nodes and 144 edges\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "user_defined_gap = \"Is there a disconnect between theoretical findings and their practical implementation?\"\n",
    "user_gap_output = await gap_agent.arun(GapInputSchema(search_results=search_output.results, gap=user_defined_gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Disconnect Between Theoretical Findings and Practical Implementation\n",
      "\n",
      "### Overview\n",
      "There is a notable disconnect between theoretical advancements in various fields, particularly in machine learning and deep learning, and their practical implementation. This gap can be attributed to several factors, including limitations in model performance, evaluation metrics, and the availability of training data.\n",
      "\n",
      "### Key Points of Disconnect\n",
      "\n",
      "1. **Model Limitations**:\n",
      "   - **TABULA-8B**: This model exhibits a restricted context window and high resource requirements, which complicate its practical application. Additionally, biases from training data can hinder its effectiveness in real-world scenarios.\n",
      "   - **Deep Learning vs. Traditional Methods**: While deep learning methods have shown promise, their practical benefits compared to traditional models like gradient-boosted decision trees (GBDTs) are often limited. Models that excel in controlled environments frequently struggle in real-world applications, particularly when faced with out-of-distribution data.\n",
      "\n",
      "2. **Evaluation Challenges**:\n",
      "   - The evaluation of large language models (LLMs) in tabular reasoning reveals significant issues. For instance, the TARGET benchmark highlights problems with free-form text evaluation metrics, which may not accurately reflect model performance in practical settings. The reliance on ground-truth answers in multiple-choice formats raises questions about the validity of these evaluations.\n",
      "   - Common evaluation metrics often lack reliability, and the practical performance of LLMs in tabular reasoning is not as robust as theoretical claims suggest. This discrepancy indicates that while theoretical frameworks exist, they may not align with the complexities of real-world applications.\n",
      "\n",
      "3. **Data Limitations**:\n",
      "   - The lack of large-scale training data for tabular models is a critical bottleneck. This scarcity complicates the practical implementation of theoretical findings, as models trained on limited data may not generalize well to diverse real-world scenarios.\n",
      "\n",
      "4. **Comparative Performance**:\n",
      "   - Some studies indicate that traditional ensemble methods, when properly tuned, still outperform newer deep learning models. This suggests that despite theoretical advancements, established methods may still hold practical advantages.\n",
      "\n",
      "### Conclusion\n",
      "In summary, while theoretical findings in machine learning and deep learning present exciting possibilities, their practical implementation often reveals significant challenges. Issues such as model limitations, evaluation reliability, and data availability contribute to a disconnect that necessitates further research and development to bridge the gap between theory and practice.\n"
     ]
    }
   ],
   "source": [
    "# To view output\n",
    "print(user_gap_output.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c070bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Large Scale Transfer Learning for Tabular Data via Language Modeling\n",
      "section_title:  6 Discussion\n",
      "url:  http://arxiv.org/abs/2406.12031v2\n",
      "locally generated content:  Yes, there appears to be a disconnect between theoretical findings and their practical implementation. The limitations of TABULA-8B, such as its restricted context window, high resource requirements for serving and inference, and the introduction of potential biases from its training data, suggest challenges in applying theoretical advancements in a practical setting. Additionally, while the document discusses promising avenues for improvement and extension, these areas indicate that further research and development are needed to bridge the gap between theory and practical application.\n",
      "--------------------\n",
      "title:  Large Scale Transfer Learning for Tabular Data via Language Modeling\n",
      "section_title:  2 Related Work\n",
      "url:  http://arxiv.org/abs/2406.12031v2\n",
      "locally generated content:  Yes, there appears to be a disconnect between theoretical findings and their practical implementation. While deep learning methods have shown promise in various domains, including tabular prediction, the practical benefits of these approaches relative to traditional methods like gradient-boosted decision trees (GBDTs) seem to be limited. Additionally, models that perform well in-distribution often struggle when evaluated out-of-distribution, indicating a gap between theoretical advancements and their effectiveness in real-world applications. Furthermore, the lack of large-scale training data for tabular models has been identified as a critical bottleneck, which further complicates the practical implementation of these theoretical findings.\n",
      "--------------------\n",
      "title:  Tabular Data Generation using Binary Diffusion\n",
      "section_title:  5 Results\n",
      "url:  http://arxiv.org/abs/2409.13882v2\n",
      "locally generated content:  The provided content does not explicitly mention a disconnect between theoretical findings and their practical implementation. It focuses on the evaluation of Binary Diffusion's performance on benchmark datasets, highlighting its superior performance compared to state-of-the-art models, efficiency, and competitive results in various tasks. There is no indication of any discrepancies or issues between theory and practice in the results discussed.\n",
      "--------------------\n",
      "title:  Tabular Data Generation using Binary Diffusion\n",
      "section_title:  6 Conclusions\n",
      "url:  http://arxiv.org/abs/2409.13882v2\n",
      "locally generated content:  The provided content does not explicitly mention a disconnect between theoretical findings and their practical implementation. It focuses on the proposed method, its advantages, and the performance of the Binary Diffusion model without addressing any potential gaps between theory and practice.\n",
      "--------------------\n",
      "title:  How well do LLMs reason over tabular data, really?\n",
      "section_title:  2 Related Work\n",
      "url:  http://arxiv.org/abs/2505.07453v2\n",
      "locally generated content:  Yes, there appears to be a disconnect between theoretical findings and their practical implementation. The document highlights various benchmarks and evaluations of LLMs in tabular reasoning, noting issues such as the reliability of evaluation metrics and the challenges posed by the format of ground-truth answers. For instance, the TARGET benchmark identifies problems with free-form text evaluation metrics when applied to tabular reasoning, while other works attempt to address these issues but still face challenges related to structured output generation and response parsing. Additionally, the evaluation methods used, such as including ground-truth answers in multiple-choice options, raise questions about their validity and reliability. This suggests that while theoretical frameworks and benchmarks exist, their practical application may not fully align with real-world complexities and requirements.\n",
      "--------------------\n",
      "title:  How well do LLMs reason over tabular data, really?\n",
      "section_title:  Conclusion\n",
      "url:  http://arxiv.org/abs/2505.07453v2\n",
      "locally generated content:  Yes, there is a disconnect between theoretical findings and their practical implementation. The studies suggest that LLMs have reasonable tabular reasoning abilities, but they often lack reliable evaluations and robustness checks. The limitations of common evaluation metrics and the findings of significant deficits in LLMs' tabular reasoning capabilities indicate that while theoretical claims may be optimistic, the practical performance of LLMs in real-world scenarios is not as strong. Additionally, the sensitivity of LLMs to variations in tabular data further highlights this disconnect.\n",
      "--------------------\n",
      "title:  VisTabNet: Adapting Vision Transformers for Tabular Data\n",
      "section_title:  2 Related Work\n",
      "url:  http://arxiv.org/abs/2501.00057v2\n",
      "locally generated content:  The content does suggest a disconnect between theoretical findings and their practical implementation. While various deep learning models have been proposed to improve performance on tabular data and some authors claim these models outperform traditional shallow ensemble methods, other experimental studies indicate that typical ensemble methods, when carefully tuned, still demonstrate superior performance. This discrepancy highlights a gap between the theoretical advancements in deep learning and their practical effectiveness compared to established methods like Gradient Boosting and Random Forests.\n",
      "--------------------\n",
      "title:  VisTabNet: Adapting Vision Transformers for Tabular Data\n",
      "section_title:  5 Conclusion\n",
      "url:  http://arxiv.org/abs/2501.00057v2\n",
      "locally generated content:  The provided content does not explicitly mention a disconnect between theoretical findings and their practical implementation. It discusses the introduction of a cross-modal transfer method and its successful application using the ViT architecture, indicating that the approach has been realized effectively. However, it does suggest that further exploration is needed regarding the applicability of cross-modal transfer to different network architectures, which could imply potential challenges in practical implementation in the future.\n",
      "--------------------\n",
      "title:  Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs\n",
      "section_title:  5 Results\n",
      "url:  http://arxiv.org/abs/2507.17259v1\n",
      "locally generated content:  The provided content does not indicate whether there is a disconnect between theoretical findings and their practical implementation. It focuses on empirical findings related to vulnerabilities in MIAs on tabular data in LLMs, highlighting trends influenced by fine-tuning duration, encoding format, and model architecture, but does not address any potential disconnect.\n",
      "--------------------\n",
      "title:  Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs\n",
      "section_title:  6 Conclusion\n",
      "url:  http://arxiv.org/abs/2507.17259v1\n",
      "locally generated content:  The section does not explicitly mention a disconnect between theoretical findings and their practical implementation. It discusses the results of experiments and the implications of those results regarding the risks of memorization and privacy leakage in LLMs trained on tabular data, as well as the effectiveness of certain encoding strategies in mitigating these risks. However, it does not address any specific disconnect between theory and practice.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# To view sources\n",
    "print_sources(user_gap_output.attributed_source_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e641f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a637a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecb47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beacd755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
