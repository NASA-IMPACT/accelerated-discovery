{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Auto-Update: Sync with latest repository changes\n",
                "# import subprocess\n",
                "# import sys\n",
                "# from pathlib import Path\n",
                "\n",
                "# def update_repository():\n",
                "#     \"\"\"Update the repository and reinstall the package if needed.\"\"\"\n",
                "#     try:\n",
                "#         print(\"ðŸ”„ Checking for repository updates...\")\n",
                "        \n",
                "#         # Check if we're in a git repository\n",
                "#         repo_root = Path.cwd()\n",
                "#         while repo_root != repo_root.parent and not (repo_root / '.git').exists():\n",
                "#             repo_root = repo_root.parent\n",
                "            \n",
                "#         if not (repo_root / '.git').exists():\n",
                "#             print(\"âš ï¸  Not in a git repository, skipping update\")\n",
                "#             return\n",
                "            \n",
                "#         # Fetch latest changes\n",
                "#         result = subprocess.run(['git', 'fetch'], capture_output=True, text=True, cwd=repo_root)\n",
                "#         if result.returncode != 0:\n",
                "#             print(f\"âš ï¸  Git fetch failed: {result.stderr}\")\n",
                "#             return\n",
                "            \n",
                "#         # Check if there are updates\n",
                "#         result = subprocess.run(['git', 'status', '-uno'], capture_output=True, text=True, cwd=repo_root)\n",
                "#         if \"Your branch is behind\" in result.stdout:\n",
                "#             print(\"ðŸ“¥ Updates found, pulling latest changes...\")\n",
                "            \n",
                "#             # Pull latest changes\n",
                "#             pull_result = subprocess.run(['git', 'pull'], capture_output=True, text=True, cwd=repo_root)\n",
                "#             if pull_result.returncode != 0:\n",
                "#                 print(f\"âŒ Git pull failed: {pull_result.stderr}\")\n",
                "#                 return\n",
                "                \n",
                "#             print(\"âœ… Repository updated successfully\")\n",
                "            \n",
                "#             # Check if pyproject.toml or requirements changed\n",
                "#             changed_files = subprocess.run(['git', 'diff', 'HEAD@{1}', '--name-only'], \n",
                "#                                          capture_output=True, text=True, cwd=repo_root)\n",
                "            \n",
                "#             if any(file in changed_files.stdout for file in ['pyproject.toml', 'requirements.txt', 'setup.py']):\n",
                "#                 print(\"ðŸ“¦ Dependencies changed, reinstalling package...\")\n",
                "                \n",
                "#                 # Reinstall in development mode\n",
                "#                 install_result = subprocess.run([sys.executable, '-m', 'pip', 'install', '-e', '.'], \n",
                "#                                               capture_output=True, text=True, cwd=repo_root)\n",
                "                \n",
                "#                 if install_result.returncode == 0:\n",
                "#                     print(\"âœ… Package reinstalled successfully\")\n",
                "#                 else:\n",
                "#                     print(f\"âš ï¸  Package reinstall failed: {install_result.stderr}\")\n",
                "                    \n",
                "#             print(\"ðŸ”„ Please restart kernel if major changes were made\")\n",
                "#         else:\n",
                "#             print(\"âœ… Repository is up to date\")\n",
                "            \n",
                "#     except Exception as e:\n",
                "#         print(f\"âŒ Update failed: {e}\")\n",
                "\n",
                "# # Run update check\n",
                "# update_repository()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Search Agent Testing Notebook\n",
                "\n",
                "A streamlined testing environment for the DeepLitSearchAgent with automatic repository updates.\n",
                "\n",
                "## Quick Start\n",
                "1. **Auto-Update**: The cell above automatically syncs with the latest repository changes\n",
                "2. **Setup**: Run the imports cell below\n",
                "3. **Configure**: Adjust parameters in the configuration section  \n",
                "4. **Test**: Set your query and run the search\n",
                "5. **Analyze**: Review results in the analysis section\n",
                "\n",
                "## What is DeepLitSearchAgent?\n",
                "An advanced multi-agent system for literature search with:\n",
                "- **Iterative refinement** - Improves search quality over multiple rounds\n",
                "- **Quality assessment** - Filters results using relevancy scoring  \n",
                "- **Source validation (ISSN whitelist)** - Ensures high-quality peer-reviewed sources\n",
                "- **Comprehensive synthesis** - Generates research reports from findings\n",
                "\n",
                "---\n",
                "**âš ï¸ Note**: If you see \"Please restart kernel\" after updates, restart your Jupyter kernel to use the latest code."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import asyncio\n",
                "import json\n",
                "from datetime import datetime\n",
                "from pathlib import Path\n",
                "from typing import Dict, List, Any\n",
                "from IPython.display import display, Markdown\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# AKD imports\n",
                "from akd.agents.search.deep_search import DeepLitSearchAgent, DeepLitSearchAgentConfig\n",
                "from akd.agents.search._base import LitSearchAgentInputSchema\n",
                "from akd.tools.search.searxng_search import SearxNGSearchTool, SearxNGSearchToolConfig, SearxNGSearchToolInputSchema\n",
                "\n",
                "print(\"âœ… Setup complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TRIAGE_AGENT_PROMPT = \"\"\"IDENTITY and PURPOSE:\n",
                "You are an expert query triage specialist who determines the optimal path for research requests. Your role is to quickly assess whether a query has sufficient context for immediate research or needs clarification first.\n",
                "\n",
                "DECISION CRITERIA:\n",
                "1. **Needs Clarification If:**\n",
                "   - The query is too vague or broad (e.g., \"Tell me about AI\")\n",
                "   - Key parameters are missing (timeframe, scope, specific aspects)\n",
                "   - Multiple interpretations are possible\n",
                "   - The research goal or intended use is unclear\n",
                "\n",
                "2. **Ready for Instructions If:**\n",
                "   - The query has clear scope and boundaries\n",
                "   - Specific aspects or questions are identified\n",
                "   - The depth/type of research needed is apparent\n",
                "   - Any ambiguity wouldn't significantly impact research quality\n",
                "\n",
                "3. **Direct Research (Rare) If:**\n",
                "   - The query is extremely specific and detailed\n",
                "   - All necessary context is provided\n",
                "   - No clarification could improve the research\n",
                "\n",
                "OUTPUT INSTRUCTIONS:\n",
                "- Make a quick, decisive routing decision\n",
                "- Provide brief reasoning (1-2 sentences)\n",
                "- Err on the side of clarity - better to clarify than to research the wrong thing\n",
                "- Consider the research domain (scientific, technical, historical, etc.)\"\"\"\n",
                "\n",
                "DEEP_RESEARCH_AGENT_PROMPT = \"\"\"IDENTITY and PURPOSE:\n",
                "You are an expert deep research agent with advanced capabilities in scientific literature search, synthesis, and analysis. You perform comprehensive, iterative research to produce high-quality, evidence-based reports.\n",
                "\n",
                "CORE CAPABILITIES:\n",
                "1. **Iterative Search Strategy**\n",
                "   - Start with broad searches to understand the landscape\n",
                "   - Progressively refine queries based on initial findings\n",
                "   - Identify and pursue promising research threads\n",
                "   - Recognize when sufficient depth has been achieved\n",
                "\n",
                "2. **Source Evaluation**\n",
                "   - Prioritize peer-reviewed and authoritative sources\n",
                "   - Assess credibility and potential biases\n",
                "   - Note publication dates and relevance\n",
                "   - Identify consensus vs. controversial findings\n",
                "\n",
                "   - Connect findings across multiple sources\n",
                "   - Identify patterns, trends, and relationships\n",
                "   - Highlight contradictions or conflicting evidence\n",
                "   - Draw evidence-based conclusions\n",
                "\n",
                "4. **Research Quality Assurance**\n",
                "3. **Synthesis and Analysis**\n",
                "   - Maintain scientific rigor throughout\n",
                "   - Provide proper attribution for all claims\n",
                "   - Acknowledge limitations and gaps\n",
                "   - Avoid overgeneralization or speculation\n",
                "\n",
                "RESEARCH PROCESS:\n",
                "1. Parse and understand the detailed research instructions\n",
                "2. Plan initial search strategy and keywords\n",
                "3. Execute searches and evaluate results\n",
                "4. Identify knowledge gaps and refine approach\n",
                "5. Iterate until quality threshold is met\n",
                "6. Synthesize findings into comprehensive report\n",
                "\n",
                "OUTPUT REQUIREMENTS:\n",
                "- Well-structured research report with clear sections\n",
                "- Executive summary of key findings\n",
                "- Detailed evidence with proper citations\n",
                "- Identification of gaps or areas for future research\n",
                "- Objective presentation of conflicting viewpoints\n",
                "- Tables, comparisons, or visualizations where helpful\n",
                "\n",
                "QUALITY STANDARDS:\n",
                "- Comprehensive coverage of the topic\n",
                "- Balanced representation of different perspectives\n",
                "- Clear distinction between evidence and interpretation\n",
                "- Appropriate depth for the intended use\n",
                "- Professional, academic writing style\"\"\"\n",
                "\n",
                "CLARIFYING_AGENT_PROMPT = \"\"\"IDENTITY and PURPOSE:\n",
                "You are an expert research assistant who helps users clarify their research requests to ensure comprehensive and accurate results.\n",
                "\n",
                "If the user hasn't specifically asked for research (unlikely), ask them what research they would like you to do.\n",
                "\n",
                "GUIDELINES:\n",
                "1. **Be concise while gathering all necessary information** \n",
                "   - Ask 2â€“3 clarifying questions to gather more context for research\n",
                "   - Make sure to gather all the information needed to carry out the research task in a concise, well-structured manner\n",
                "   - Use bullet points or numbered lists if appropriate for clarity\n",
                "   - Don't ask for unnecessary information, or information that the user has already provided\n",
                "\n",
                "2. **Maintain a Friendly and Professional Tone**\n",
                "   - For example, instead of saying \"I need a bit more detail on Y,\" say, \"Could you share more detail on Y?\"\n",
                "   - Be encouraging and show genuine interest in helping with the research\n",
                "\n",
                "3. **Focus on Research-Relevant Clarifications**\n",
                "   - Ask about scope, depth, time period, specific aspects of interest\n",
                "   - Clarify any ambiguous terms or concepts\n",
                "   - Understand the intended use or application of the research\n",
                "\n",
                "OUTPUT INSTRUCTIONS:\n",
                "- Return 2-3 focused clarifying questions\n",
                "- Each question should help narrow down or better define the research scope\n",
                "- Questions should be clear and easy to answer\"\"\"\n",
                "\n",
                "RESEARCH_INSTRUCTION_AGENT_PROMPT = \"\"\"IDENTITY and PURPOSE:\n",
                "You are an expert research instruction designer who transforms user queries and clarifications into detailed, actionable research briefs for deep research execution.\n",
                "\n",
                "Based on the following guidelines, take the users query (and any clarifications), and rewrite it into detailed research instructions. OUTPUT ONLY THE RESEARCH INSTRUCTIONS, NOTHING ELSE.\n",
                "\n",
                "GUIDELINES:\n",
                "1. **Maximize Specificity and Detail**\n",
                "   - Include all known user preferences and explicitly list key attributes or dimensions to consider\n",
                "   - It is of utmost importance that all details from the user are included in the expanded prompt\n",
                "   - Be explicit about depth, breadth, and type of analysis required\n",
                "\n",
                "2. **Fill in Unstated But Necessary Dimensions as Open-Ended**\n",
                "   - If certain attributes are essential for meaningful output but the user has not provided them, explicitly state that they are open-ended or default to \"no specific constraint\"\n",
                "   - Guide the research to explore these dimensions comprehensively\n",
                "\n",
                "3. **Avoid Unwarranted Assumptions**\n",
                "   - If the user has not provided a particular detail, do not invent one\n",
                "   - Instead, state the lack of specification and guide the deep research model to treat it as flexible or accept all possible options\n",
                "\n",
                "4. **Use the First Person**\n",
                "   - Phrase the request from the perspective of the user\n",
                "   - Example: \"I need research on...\" rather than \"The user needs...\"\n",
                "\n",
                "5. **Structure and Formatting Requirements**\n",
                "   - Explicitly request appropriate headers and formatting for clarity\n",
                "   - If the research would benefit from tables, comparisons, or structured data, explicitly request them\n",
                "   - Examples of when to request tables:\n",
                "     - Comparing multiple options, methodologies, or approaches\n",
                "     - Summarizing key findings across multiple sources\n",
                "     - Presenting timeline or chronological information\n",
                "     - Showing statistical data or numerical comparisons\n",
                "\n",
                "6. **Source Requirements**\n",
                "   - Specify preference for peer-reviewed sources, primary research, or authoritative publications\n",
                "   - Request proper citations and attribution for all claims\n",
                "   - If domain-specific sources are important, mention them explicitly\n",
                "\n",
                "7. **Language and Style**\n",
                "   - Maintain scientific rigor and objectivity\n",
                "   - Request evidence-based conclusions\n",
                "   - Ask for identification of conflicting viewpoints or contradictory evidence\n",
                "\n",
                "8. **Expected Deliverables**\n",
                "   - Be clear about what constitutes a complete research output\n",
                "   - Specify if synthesis, analysis, or recommendations are needed\n",
                "   - Request identification of gaps or areas needing further research\n",
                "\n",
                "IMPORTANT: Ensure the instructions are comprehensive yet focused on the user's actual needs\"\"\"\n",
                "\n",
                "QUERY_SYSTEM_PROMPT = \"\"\"IDENTITY and PURPOSE:\n",
                "You are an expert scientific search engine query generator with a deep understanding of which queries will maximize the number of relevant results for science.\n",
                "\n",
                "INTERNAL ASSISTANT STEPS:\n",
                "- Analyze the given instruction to identify key concepts and aspects that need to be researched.\n",
                "- For each aspect, craft a search query using appropriate search operators and syntax.\n",
                "- Ensure queries cover different angles of the topic (technical, practical, comparative, etc.).\n",
                "\n",
                "OUTPUT INSTRUCTIONS:\n",
                "- Return exactly the requested number of queries.\n",
                "- Format each query like a search engine query, not a natural language question.\n",
                "- Each query should be a concise string of keywords and operators.\"\"\"\n",
                "\n",
                "\n",
                "# Follow-up query generation focused on gap-closing and diversification\n",
                "FOLLOWUP_QUERY_SYSTEM_PROMPT = \"\"\"IDENTITY and PURPOSE:\n",
                "You are an expert follow-up query generator. Given the original queries that were already tried and a synthesized content summary of what they retrieved, propose new search queries that:\n",
                "- Close gaps, surface missing perspectives, and reduce redundancy\n",
                "- Target higher-evidence sources (peer-reviewed, meta-analyses), and recent works when appropriate\n",
                "- Explore alternative methods, datasets, benchmarks, and negative/contradictory evidence\n",
                "\n",
                "INPUTS YOU WILL RECEIVE:\n",
                "- original_queries: the list of queries already used\n",
                "- content: a compact summary of what was found (titles, brief snippets, instructions context)\n",
                "- num_queries: the exact number of follow-up queries to return\n",
                "\n",
                "GUIDELINES:\n",
                "1. Maximize novelty and coverage relative to original_queries and the provided content.\n",
                "2. Prefer queries that increase evidence quality, methodological rigor, and recency when relevant.\n",
                "3. Propose targeted queries (concise keyword/operator strings), not natural language questions.\n",
                "4. Consider synonyms, controlled vocabulary (e.g., MeSH terms for biomed), and domain-specific operators.\n",
                "5. Include disconfirming or boundary-case queries where useful (e.g., failure modes, limitations, critiques).\n",
                "6. Avoid near-duplicates of original_queries and avoid repeating obviously saturated angles.\n",
                "\n",
                "OUTPUT INSTRUCTIONS:\n",
                "- Return exactly num_queries follow-up queries.\n",
                "- Each query must be a concise search-engine-style string of keywords and operators.\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Configuration\n",
                "\n",
                "**Edit these parameters to customize the search behavior:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SEARCH CONFIGURATION ===\n",
                "USER_QUERY = \"Find research papers on studies that use climate and hydrological modeling, LiDAR-derived snowpack data, and precipitation.\"\n",
                "\n",
                "# Core Search Parameters\n",
                "MAX_RESEARCH_ITERATIONS = 1       # Number of search refinement cycles (1-10)\n",
                "QUALITY_THRESHOLD = 0.7            # Stop when this quality is reached (0.0-1.0)\n",
                "MIN_RELEVANCY_SCORE = 0.3          # Minimum score to include results (0.0-1.0)\n",
                "\n",
                "# Advanced Features  \n",
                "USE_SEMANTIC_SCHOLAR = True        # Include academic papers from Semantic Scholar\n",
                "SOURCE_VALIDATION = True           # Enable source validation (ISSN-based whitelist) -- leads to 75 - 85% filtering of links\n",
                "ENABLE_FULL_CONTENT_SCRAPING = True # Fetch full content for high-scoring results\n",
                "FULL_CONTENT_THRESHOLD = 0.7       # Score threshold for full content fetch\n",
                "\n",
                "# SearxNG Configuration (used by both Deep Search and direct SearxNG test)\n",
                "SEARXNG_ENGINES = [\"crossref\", \"arxiv\", \"google_scholar\", \"semantic_scholar\"]  # Search engines to use\n",
                "SEARXNG_MAX_RESULTS = 50           # Maximum results to fetch\n",
                "SEARXNG_MAX_PAGES = 5              # Maximum pages to search\n",
                "SEARXNG_RESULTS_PER_PAGE = 10      # Results per page\n",
                "SEARXNG_SCORE_CUTOFF = 0.25        # Minimum score threshold\n",
                "\n",
                "# Debug and Performance\n",
                "DEBUG_MODE = True                  # Enable detailed logging\n",
                "MAX_RESULTS_TO_DISPLAY = 1000      # Limit display (None = show all)\n",
                "\n",
                "print(\"ðŸ“Š Configuration loaded:\")\n",
                "print(f\"   Query: {USER_QUERY[:50]}{'...' if len(USER_QUERY) > 50 else ''}\")\n",
                "print(f\"   Max Iterations: {MAX_RESEARCH_ITERATIONS}\")\n",
                "print(f\"   Quality Threshold: {QUALITY_THRESHOLD}\")\n",
                "print(f\"   Source Validation: {SOURCE_VALIDATION}\")\n",
                "print(f\"   SearxNG Engines: {SEARXNG_ENGINES}\")\n",
                "print(f\"   SearxNG Max Results: {SEARXNG_MAX_RESULTS}\")\n",
                "print(f\"   Debug Mode: {DEBUG_MODE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Run Deep Search"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Deep Search Test\n",
                "async def run_deep_search(user_query):\n",
                "    print(\"ðŸš€ Initializing DeepLitSearchAgent...\")\n",
                "    searxng_config = SearxNGSearchToolConfig(\n",
                "        max_results=SEARXNG_MAX_RESULTS,\n",
                "        engines=SEARXNG_ENGINES,\n",
                "        max_pages=SEARXNG_MAX_PAGES,\n",
                "        results_per_page=SEARXNG_RESULTS_PER_PAGE,\n",
                "        score_cutoff=SEARXNG_SCORE_CUTOFF,\n",
                "        debug=DEBUG_MODE\n",
                "    )\n",
                "    search_tool = SearxNGSearchTool(config=searxng_config)\n",
                "    # Create configuration\n",
                "    config = DeepLitSearchAgentConfig(\n",
                "        max_research_iterations=MAX_RESEARCH_ITERATIONS,\n",
                "        quality_threshold=QUALITY_THRESHOLD,\n",
                "        min_relevancy_score=MIN_RELEVANCY_SCORE,\n",
                "        use_semantic_scholar=USE_SEMANTIC_SCHOLAR,\n",
                "        search_tool=search_tool,\n",
                "        source_validation=SOURCE_VALIDATION,\n",
                "        enable_full_content_scraping=ENABLE_FULL_CONTENT_SCRAPING,\n",
                "        full_content_threshold=FULL_CONTENT_THRESHOLD,\n",
                "        enable_streaming=False,\n",
                "        debug=DEBUG_MODE\n",
                "    )\n",
                "\n",
                "    # Initialize and run\n",
                "    agent = DeepLitSearchAgent(config=config, search_tool=search_tool, debug=DEBUG_MODE)\n",
                "    agent_input = LitSearchAgentInputSchema(query=user_query, category=\"science\")\n",
                "\n",
                "    print(\"ðŸ”Ž Running deep search...\")\n",
                "    output = await agent.arun(agent_input)\n",
                "\n",
                "    # Results summary\n",
                "    num_results = len(output.results)\n",
                "    iterations = getattr(output, \"iterations_performed\", 1)\n",
                "    has_report = output.results and output.results[0].get(\"url\") == \"deep-search://report\"\n",
                "\n",
                "    print(f\"âœ… Search complete!\")\n",
                "    print(f\"   ðŸ“Š Total results: {num_results}\")\n",
                "    print(f\"   ðŸ”„ Iterations: {iterations}\")\n",
                "    print(f\"   ðŸ“‹ Research report: {'Yes' if has_report else 'No'}\")\n",
                "\n",
                "    # Save results - create directory first\n",
                "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
                "    results_dir = Path(\"notebooks\")\n",
                "    results_dir.mkdir(exist_ok=True)  # Create directory if it doesn't exist\n",
                "    results_file = results_dir / f\"deep_search_results_{timestamp}.json\"\n",
                "\n",
                "    with open(results_file, 'w') as f:\n",
                "        json.dump({\n",
                "            \"query\": user_query,\n",
                "            \"category\": output.category,\n",
                "            \"iterations_performed\": iterations,\n",
                "            \"results\": output.results\n",
                "        }, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "    print(f\"ðŸ’¾ Results saved to: {results_file}\")\n",
                "    return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "USER_QUERY = \"Find research papers on basin-scale water budgets, focusing on the role of groundwater as a stable and important source for streamflow.\"\n",
                "\n",
                "output = await run_deep_search(\n",
                "    USER_QUERY\n",
                ")\n",
                "print(output)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Results Synthesis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display Research Report (if generated)\n",
                "if 'output' in globals() and output.results:\n",
                "    # Check for research report\n",
                "    report = None\n",
                "    if output.results[0].get(\"url\") == \"deep-research://report\":\n",
                "        report = output.results[0]\n",
                "    \n",
                "    if report:\n",
                "        print(\"ðŸ“‹ RESEARCH SYNTHESIS REPORT\")\n",
                "        print(\"=\" * 80)\n",
                "        content = report.get(\"content\", \"No content available\")\n",
                "        print(content)\n",
                "        print(\"=\" * 80)\n",
                "    else:\n",
                "        print(\"ðŸ“‹ No research synthesis report generated\")\n",
                "        \n",
                "else:\n",
                "    print(\"âš ï¸  No results available. Run the search cell first!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## References for Synthesis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display Search Results Summary\n",
                "if 'output' in globals() and output.results:\n",
                "    search_results = [r for r in output.results if r.get(\"url\") != \"deep-research://report\"]\n",
                "    \n",
                "    if search_results:\n",
                "        print(f\"ðŸ” SEARCH RESULTS SUMMARY ({len(search_results)} papers)\")\n",
                "        print(\"=\" * 80)\n",
                "        \n",
                "        # Calculate quality metrics\n",
                "        scores = [r.get(\"relevancy_score\") for r in search_results if isinstance(r.get(\"relevancy_score\"), (int, float))]\n",
                "        if scores:\n",
                "            avg_score = sum(scores) / len(scores)\n",
                "            print(f\"ðŸ“ˆ Quality Metrics: Avg={avg_score:.2f}, Min={min(scores):.2f}, Max={max(scores):.2f}\")\n",
                "            print(\"-\" * 40)\n",
                "        \n",
                "        # Display results (limited by MAX_RESULTS_TO_DISPLAY)\n",
                "        display_count = len(search_results) if MAX_RESULTS_TO_DISPLAY is None else min(MAX_RESULTS_TO_DISPLAY, len(search_results))\n",
                "        \n",
                "        for i, result in enumerate(search_results[:display_count]):\n",
                "            print(f\"\\nðŸ“„ [{i+1}] {result.get('title', 'Untitled')}\")\n",
                "            \n",
                "            # Show relevancy score\n",
                "            score = result.get('relevancy_score')\n",
                "            if score is not None:\n",
                "                print(f\"    ðŸ“Š Relevancy: {score:.2f}\")\n",
                "            \n",
                "            # Show author if available\n",
                "            author = result.get('author')\n",
                "            if author:\n",
                "                print(f\"    ðŸ‘¤ Author: {author}\")\n",
                "                \n",
                "            # Show URL\n",
                "            url = result.get('url', '')\n",
                "            if url:\n",
                "                print(f\"    ðŸ”— {url}\")\n",
                "            \n",
                "            # Show summary if available\n",
                "            summary = result.get('summary', '')\n",
                "            if summary:\n",
                "                summary_preview = summary[:200] + \"...\" if len(summary) > 200 else summary\n",
                "                print(f\"    ðŸ“ Summary: {summary_preview}\")\n",
                "            \n",
                "            print(\"-\" * 40)\n",
                "            \n",
                "        if MAX_RESULTS_TO_DISPLAY and len(search_results) > MAX_RESULTS_TO_DISPLAY:\n",
                "            print(f\"\\n... and {len(search_results) - MAX_RESULTS_TO_DISPLAY} more results\")\n",
                "            \n",
                "    else:\n",
                "        print(\"ðŸ“‹ No search results found\")\n",
                "        \n",
                "else:\n",
                "    print(\"âš ï¸  No results available. Run the search cell first!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5. SearxNG Direct Testing\n",
                "\n",
                "Test the underlying SearxNG search tool with the same configuration and Query used by Deep Search."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "searxng_config = SearxNGSearchToolConfig(\n",
                "        max_results=SEARXNG_MAX_RESULTS,\n",
                "        engines=SEARXNG_ENGINES,\n",
                "        max_pages=SEARXNG_MAX_PAGES,\n",
                "        results_per_page=SEARXNG_RESULTS_PER_PAGE,\n",
                "        score_cutoff=SEARXNG_SCORE_CUTOFF,\n",
                "        debug=DEBUG_MODE\n",
                ")\n",
                "search_tool = SearxNGSearchTool(config=searxng_config)\n",
                "searxng_input = SearxNGSearchToolInputSchema(queries=[USER_QUERY], category=\"science\", max_results=SEARXNG_MAX_RESULTS)\n",
                "\n",
                "searxng_output = await search_tool.arun(searxng_input)\n",
                "\n",
                "print(f\"âœ… Found {len(searxng_output.results)} results\")\n",
                "\n",
                "# Display results\n",
                "for i, result in enumerate(searxng_output.results, 1):\n",
                "    print(f\"\\n{i}. {result.title}\")\n",
                "    print(f\"   {result.url}\")\n",
                "    if hasattr(result, 'content') and result.content:\n",
                "        content_preview = result.content[:150] + \"...\" if len(result.content) > 150 else result.content\n",
                "        print(f\"   {content_preview}\")\n",
                "    print(\"-\" * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
