name: Integration Branch Sync
on:
  pull_request:
    types: [opened, synchronize, reopened]
    branches: [develop]

jobs:
  test-pr-branch:
    name: Run Tests on PR Branch üß™
    runs-on: ubuntu-latest
    environment: integration
    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref }}
          lfs: "true"
          submodules: "recursive"

      - name: Set up python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install requirements
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"
        working-directory: ${{ vars.CDK_WORKING_DIRECTORY }}

      - name: Run tests with coverage
        id: test
        env:
          PYTHONUNBUFFERED: 1
          FORCE_COLOR: 1
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          SEARXNG_BASE_URL: ${{ vars.SEARXNG_BASE_URL}}
        run: |
          echo "Checking environment setup..."
          if [ -z "$OPENAI_API_KEY" ]; then
            echo "‚ùå OPENAI_API_KEY is not set"
          else
            echo "‚úÖ OPENAI_API_KEY is set (length: ${#OPENAI_API_KEY})"
            echo "Key starts with: ${OPENAI_API_KEY:0:7}..." # Show first 7 chars safely
          fi

          echo "Running tests..."
          echo "Using SearxNG instance: $SEARXNG_BASE_URL"
          set +e  # Don't exit on command failure

          # Run tests with real-time output and capture for parsing
          python -m pytest tests/ -v --cov --cov-report=xml --cov-report=term 2>&1 | tee test_output.log
          test_exit_code=${PIPESTATUS[0]}

          echo ""
          echo "üìä Test Results Summary:"
          echo "Exit code: $test_exit_code"

          # Extract test results from pytest output
          if [ -f test_output.log ]; then
            # Debug: Show what we're parsing
            echo "=== Debug: Searching for test results in output ==="
            echo "Last 10 lines of test output:"
            tail -10 test_output.log
            echo ""
            echo "Lines containing 'failed', 'passed', or 'warnings':"
            grep -E "(failed|passed|warnings)" test_output.log | tail -5 || echo "No matching lines found"
            echo "Lines containing '====' (pytest separators):"
            grep -E "=+" test_output.log | tail -3 || echo "No separator lines found"
            echo "=============================================="

            # Strategy A: Look for pytest summary line
            # Format: ============ 6 failed, 98 passed, 87 warnings in 200.97s =============
            summary_line=$(grep -E "^=+ [0-9]+ [a-z]+, [0-9]+ [a-z]+" test_output.log | tail -1)

            # Debug: Show what Strategy A found
            echo "Strategy A debug:"
            echo "  Looking for pattern: '^=+ [0-9]+ [a-z]+, [0-9]+ [a-z]+'"
            if [ -n "$summary_line" ]; then
              echo "  Found summary line: '$summary_line'"
            else
              echo "  No summary line found"
              # Alternative pattern matching
              summary_line=$(grep -E "=.*[0-9]+ failed.*[0-9]+ passed" test_output.log | tail -1)
              if [ -n "$summary_line" ]; then
                echo "  Found with alternative pattern: '$summary_line'"
              fi
            fi

            if [ -n "$summary_line" ]; then
              echo "Strategy A: Found summary line: $summary_line"
              # Extract from summary line
              failed_count=$(echo "$summary_line" | grep -oE "[0-9]+ failed" | grep -oE "[0-9]+" || echo "0")
              passed_count=$(echo "$summary_line" | grep -oE "[0-9]+ passed" | grep -oE "[0-9]+" || echo "0")
              warnings_count=$(echo "$summary_line" | grep -oE "[0-9]+ warnings" | grep -oE "[0-9]+" || echo "0")
              parsing_strategy="summary_line"
            else
              echo "Strategy B: No summary line found, counting individual test results..."
              # Count individual FAILED/PASSED lines (without space requirement)
              failed_count=$(grep -c "^FAILED" test_output.log || echo "0")
              passed_count=$(grep -c "^PASSED" test_output.log || echo "0")
              warnings_count=$(grep -c "warnings summary" test_output.log || echo "0")
              parsing_strategy="individual_counts"

              echo "Strategy B debug:"
              echo "  FAILED pattern '^FAILED' matches: $failed_count"
              echo "  PASSED pattern '^PASSED' matches: $passed_count"

              # Strategy C: Try short test summary info section
              if [ "$failed_count" = "0" ] && [ "$passed_count" = "0" ]; then
                echo "Strategy C: Trying short test summary info section..."
                summary_info=$(grep -A 10 "short test summary info" test_output.log || echo "")
                if [ -n "$summary_info" ]; then
                  failed_count=$(echo "$summary_info" | grep -c "FAILED" || echo "0")
                  passed_count=$(echo "$summary_info" | grep -c "PASSED" || echo "0")
                  parsing_strategy="summary_info_section"
                  echo "  Strategy C found: $failed_count failed, $passed_count passed"
                fi
              fi
            fi

            # Extract coverage percentage from coverage report
            coverage_percent=$(grep -E "TOTAL.*[0-9]+%" test_output.log | awk '{print $(NF)}' | sed 's/%//' || echo "0")

            echo "Extracted metrics (using $parsing_strategy):"
            echo "- Passed tests: $passed_count"
            echo "- Failed tests: $failed_count"
            echo "- Warnings: $warnings_count"
            echo "- Coverage: $coverage_percent%"

            # Validate variables before setting outputs
            [ -z "$passed_count" ] && passed_count="0"
            [ -z "$failed_count" ] && failed_count="0"
            [ -z "$warnings_count" ] && warnings_count="0"
            [ -z "$coverage_percent" ] && coverage_percent="0"
            [ -z "$parsing_strategy" ] && parsing_strategy="unknown"

            # Set outputs for GitHub Actions
            echo "passed_tests=$passed_count" >> $GITHUB_OUTPUT
            echo "failed_tests=$failed_count" >> $GITHUB_OUTPUT
            echo "warnings_count=$warnings_count" >> $GITHUB_OUTPUT
            echo "coverage_percent=$coverage_percent" >> $GITHUB_OUTPUT
            echo "parsing_strategy=$parsing_strategy" >> $GITHUB_OUTPUT
          fi

          # Check for coverage files
          if [ -f coverage.xml ]; then
            echo "‚úÖ Coverage XML generated"
          fi

          if [ -f htmlcov/index.html ]; then
            echo "‚úÖ HTML coverage report generated"
          fi

          # Set output for later use
          if [ $test_exit_code -eq 0 ]; then
            echo "test_result=success" >> $GITHUB_OUTPUT
          else
            echo "test_result=failure" >> $GITHUB_OUTPUT
          fi

          echo "test_exit_code=$test_exit_code" >> $GITHUB_OUTPUT

          # Always exit successfully to show coverage table
          exit 0
        working-directory: ${{ vars.CDK_WORKING_DIRECTORY }}

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: pr-branch-coverage-report
          path: ${{ vars.CDK_WORKING_DIRECTORY }}/coverage.xml

      - name: Comment PR with test results
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const testResult = '${{ steps.test.outputs.test_result }}';
            const exitCode = '${{ steps.test.outputs.test_exit_code }}';
            const passedTests = '${{ steps.test.outputs.passed_tests }}' || '0';
            const failedTests = '${{ steps.test.outputs.failed_tests }}' || '0';
            const warningsCount = '${{ steps.test.outputs.warnings_count }}' || '0';
            const coveragePercent = '${{ steps.test.outputs.coverage_percent }}' || '0';
            const parsingStrategy = '${{ steps.test.outputs.parsing_strategy }}' || 'unknown';

            const outcome = testResult === 'success' ? '‚úÖ Tests passed' : `‚ùå Tests failed (exit code: ${exitCode})`;

            // Always show test results section
            let testSummary = `\n\n## üìä Test Results\n- **Passed**: ${passedTests}\n- **Failed**: ${failedTests}\n- **Warnings**: ${warningsCount}\n- **Coverage**: ${coveragePercent}%`;

            // Add parsing context if results are all zeros
            if (passedTests === '0' && failedTests === '0') {
              testSummary += `\n\n‚ö†Ô∏è _Note: Test counts are 0, which may indicate parsing issues or early test failure. Check the workflow logs for details._`;
              if (parsingStrategy !== 'unknown') {
                testSummary += `\n_Parsing strategy used: ${parsingStrategy}_`;
              }
            }

            github.rest.issues.createComment({
              issue_number: ${{ github.event.pull_request.number }},
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `${outcome}${testSummary}\n\nBranch: \`${{ github.head_ref }}\`\nPR: #${{ github.event.pull_request.number }}\nCommit: ${{ github.sha }}\n\nüìã Full coverage report and logs are available in the [workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}).`
            })

  sync-integration:
    name: Sync Integration Branch
    needs: [test-pr-branch]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout integration branch
        uses: actions/checkout@v4
        with:
          ref: integration
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: Configure Git
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

      - name: Merge PR branch into integration
        run: |
          git fetch origin ${{ github.head_ref }}
          git merge origin/${{ github.head_ref }} --no-ff -m "Auto-merge PR #${{ github.event.pull_request.number }} (${{ github.head_ref }}) into integration for testing"

      - name: Push integration branch
        run: git push origin integration

      - name: Create issue on merge conflict
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'Integration branch merge conflict',
              body: `Automatic merge of PR #${{ github.event.pull_request.number }} (${{ github.head_ref }}) into integration failed.\n\nPlease resolve conflicts manually:\n\`\`\`bash\ngit checkout integration\ngit merge origin/${{ github.head_ref }}\n# resolve conflicts\ngit push origin integration\n\`\`\``,
              labels: ['integration', 'conflict']
            })
