{
    "input": "how is hashing a one way process (not reversible)?",
    "output": "Hashing is a one-way process because it uses a mathematical algorithm to transform input data of any size into a fixed-size string of characters, known as a hash value or digest. This process is designed to be irreversible, meaning it is computationally infeasible to recreate the original input data from the hash value. The algorithm used for hashing is typically a one-way function, which means it is easy to compute the hash value from the input data, but it is extremely difficult to compute the original input data from the hash value.\n\nThe irreversibility of hashing is due to the loss of information that occurs during the hashing process. When input data is hashed, the algorithm discards some of the information in the data and only retains a summary of the data's characteristics. As a result, multiple input data sets can produce the same hash value, a phenomenon known as a collision. However, it is still computationally infeasible to determine the original input data from the hash value, making hashing a secure and reliable method for data integrity and authentication. This property of hashing is what makes it a fundamental component of many cryptographic and security protocols.",
    "topic": "You can think of it like a shadow of a object. If you only see the shadow you cannot have the original object, but if you have the original object you always can create the exact shadow.\n\nThis is the reason you just save the shadow of a password in the database and compare the shadows.",
    "cat": [
        "https://www.reddit.com/r/explainlikeimfive/comments/u0lumm/eli5_how_is_hashing_a_one_way_process_not/"
    ],
    "atoms": [
        {
            "id": "a0",
            "text": "Hashing is designed to be irreversible...",
            "original": "Here are the independent content units broken down from the given text:",
            "label": null,
            "contexts": [
                "c_a0_0",
                "c_a0_1",
                "c_a0_2"
            ]
        },
        {
            "id": "a1",
            "text": "Hashing is a one-way process",
            "original": "Hashing is a one-way process",
            "label": null,
            "contexts": [
                "c_a1_0",
                "c_a1_1",
                "c_a1_2"
            ]
        },
        {
            "id": "a2",
            "text": "Hashing uses a mathematical algorithm to transform input data of any size into a fixed-size string of characters",
            "original": "Hashing uses a mathematical algorithm to transform input data of any size into a fixed-size string of characters",
            "label": null,
            "contexts": [
                "c_a2_0",
                "c_a2_1",
                "c_a2_2"
            ]
        },
        {
            "id": "a3",
            "text": "The fixed-size string of characters is known as a hash value or digest",
            "original": "The fixed-size string of characters is known as a hash value or digest",
            "label": null,
            "contexts": [
                "c_a3_0",
                "c_a3_1",
                "c_a3_2"
            ]
        },
        {
            "id": "a4",
            "text": "The hashing process is designed to be irreversible",
            "original": "The process of hashing is designed to be irreversible",
            "label": null,
            "contexts": [
                "c_a4_0",
                "c_a4_1",
                "c_a4_2"
            ]
        },
        {
            "id": "a5",
            "text": "It is computationally infeasible to recreate the original input data from the hash value",
            "original": "It is computationally infeasible to recreate the original input data from the hash value",
            "label": null,
            "contexts": [
                "c_a5_0",
                "c_a5_1",
                "c_a5_2"
            ]
        },
        {
            "id": "a6",
            "text": "The hashing algorithm used is typically a one-way function",
            "original": "The algorithm used for hashing is typically a one-way function",
            "label": null,
            "contexts": [
                "c_a6_0",
                "c_a6_1",
                "c_a6_2"
            ]
        },
        {
            "id": "a7",
            "text": "The hash value is easy to compute from the input data",
            "original": "It is easy to compute the hash value from the input data",
            "label": null,
            "contexts": [
                "c_a7_0",
                "c_a7_1",
                "c_a7_2"
            ]
        },
        {
            "id": "a8",
            "text": "It is extremely difficult to compute the original input data from the hash value",
            "original": "It is extremely difficult to compute the original input data from the hash value",
            "label": null,
            "contexts": [
                "c_a8_0",
                "c_a8_1",
                "c_a8_2"
            ]
        },
        {
            "id": "a9",
            "text": "The irreversibility of hashing is due to the loss of information that occurs during the hashing process",
            "original": "The irreversibility of hashing is due to the loss of information that occurs during the hashing process",
            "label": null,
            "contexts": [
                "c_a9_0",
                "c_a9_1",
                "c_a9_2"
            ]
        },
        {
            "id": "a10",
            "text": "When input data is hashed, the hashing algorithm discards some of the information in the data",
            "original": "When input data is hashed, the algorithm discards some of the information in the data",
            "label": null,
            "contexts": [
                "c_a10_0",
                "c_a10_1",
                "c_a10_2"
            ]
        },
        {
            "id": "a11",
            "text": "The hashing algorithm only retains a summary of the input data's characteristics",
            "original": "The algorithm only retains a summary of the data's characteristics",
            "label": null,
            "contexts": [
                "c_a11_0",
                "c_a11_1"
            ]
        },
        {
            "id": "a12",
            "text": "Multiple input data sets can produce the same hash value",
            "original": "Multiple input data sets can produce the same hash value",
            "label": null,
            "contexts": [
                "c_a12_0",
                "c_a12_1",
                "c_a12_2"
            ]
        },
        {
            "id": "a13",
            "text": "The phenomenon of multiple input data sets producing the same hash value is known as a collision",
            "original": "The phenomenon of multiple input data sets producing the same hash value is known as a collision",
            "label": null,
            "contexts": [
                "c_a13_0",
                "c_a13_1"
            ]
        },
        {
            "id": "a14",
            "text": "Determining the original input data from the hash value is still computationally infeasible.",
            "original": "It is still computationally infeasible to determine the original input data from the hash value",
            "label": null,
            "contexts": [
                "c_a14_0",
                "c_a14_1",
                "c_a14_2"
            ]
        },
        {
            "id": "a15",
            "text": "Hashing is a secure and reliable method for data integrity and authentication",
            "original": "Hashing is a secure and reliable method for data integrity and authentication",
            "label": null,
            "contexts": [
                "c_a15_0",
                "c_a15_1",
                "c_a15_2"
            ]
        },
        {
            "id": "a16",
            "text": "Hashing's ability to ensure data integrity and authentication is what makes it a fundamental component of many cryptographic and security protocols.",
            "original": "The property of hashing that makes it a fundamental component of many cryptographic and security protocols is its ability to ensure data integrity and authentication",
            "label": null,
            "contexts": [
                "c_a16_0",
                "c_a16_1",
                "c_a16_2"
            ]
        },
        {
            "id": "a17",
            "text": "Hashing is a fundamental component of many cryptographic and security protocols",
            "original": "Hashing is a fundamental component of many cryptographic and security protocols",
            "label": null,
            "contexts": [
                "c_a17_0",
                "c_a17_1",
                "c_a17_2"
            ]
        }
    ],
    "contexts": [
        {
            "id": "c_a0_0",
            "title": "Brute-force attack",
            "text": "In cryptography, a brute-force attack consists of an attacker submitting many passwords or passphrases with the hope of eventually guessing correctly. The attacker systematically checks all possible  passwords and passphrases until the correct one is found. Alternatively, the attacker can attempt to guess the key which is typically created from the password using a key derivation function. This is known as an exhaustive key search. This approach doesn't depend on intellectual tactics; rather, it relies on making several attempts.\nA brute-force attack is a cryptanalytic attack that can, in theory, be used to attempt to decrypt any encrypted data (except for data encrypted in an information-theoretically secure manner).  Such an attack might be used when it is not possible to take advantage of other weaknesses in an encryption system (if any exist) that would make the task easier. When password-guessing, this method is very fast when used to check all short passwords, but for longer passwords other methods such as the dictionary attack are used because a brute-force search takes too long. Longer passwords, passphrases and keys have more possible values, making them exponentially more difficult to crack than shorter ones due to diversity of characters.\nBrute-force attacks can be made less effective by obfuscating the data to be encoded making it more difficult for an attacker to recognize when the code has been cracked or by making the attacker do more work to test each guess. One of the measures of the strength of an encryption system is how long it would theoretically take an attacker to mount a successful brute-force attack against it. Brute-force attacks are an application of brute-force search, the general problem-solving technique of enumerating all candidates and checking each one. The word 'hammering' is sometimes used to describe a brute-force attack, with 'anti-hammering' for countermeasures. == Basic concept ==\nBrute-force attacks work by calculating every possible combination that could make up a password and testing it to see if it is the correct password. As the password's length increases, the amount of time, on average, to find the correct password increases exponentially. == Theoretical limits ==\nThe resources required for a brute-force attack grow exponentially with increasing key size, not linearly. Although U.S. export regulations historically restricted key lengths to 56-bit symmetric keys (e.g. Data Encryption Standard), these restrictions are no longer in place, so modern symmetric algorithms typically use computationally stronger 128- to 256-bit keys. There is a physical argument that a 128-bit symmetric key is computationally secure against brute-force attack. The Landauer limit implied by the laws of physics sets a lower limit on the energy required to perform a computation of kT  \u00b7  ln 2 per bit erased in a computation, where T is the temperature of the computing device in kelvins, k is the Boltzmann constant, and the natural logarithm of 2 is about 0.693 (0.6931471805599453). No irreversible computing device can use less energy than this, even in principle.  Thus, in order to simply flip through the possible values for a 128-bit symmetric key (ignoring doing the actual computing to check it) would, theoretically, require 2128 \u2212 1 bit flips on a conventional processor.  If it is assumed that the calculation occurs near room temperature (\u2248300 K), the Von Neumann-Landauer Limit can be applied to estimate the energy required as \u22481018 joules, which is equivalent to consuming 30 gigawatts of power for one year . This is equal to 30\u00d7109 W\u00d7365\u00d724\u00d73600 s = 9.46\u00d71017 J or 262.7 TWh (about 0.1% of the yearly world energy production). The full actual computation \u2013 checking each key to see if a solution has been found \u2013 would consume many times this amount. Furthermore, this is simply the energy requirement for cycling through the key space; the actual time it takes to flip each bit is not considered, which is c",
            "link": "https://en.wikipedia.org/wiki/Brute-force_attack",
            "snippet": "In cryptography, a brute-force attack consists of an attacker submitting many passwords or passphrases with the hope of eventually guessing correctly. The attacker systematically checks all possible  passwords and passphrases until the correct one is found. Alternatively, the attacker can attempt to guess the key which is typically created from the password using a key derivation function. This is known as an exhaustive key search. This approach doesn't depend on intellectual tactics; rather, it relies on making several attempts.\nA brute-force attack is a cryptanalytic attack that can, in theory, be used to attempt to decrypt any encrypted data (except for data encrypted in an information-theoretically secure manner).  Such an attack might be used when it is not possible to take advantage of other weaknesses in an encryption system (if any exist) that would make the task easier.\nWhen password-guessing, this method is very fast when used to check all short passwords, but for longer passwords other methods such as the dictionary attack are used because a brute-force search takes too long. Longer passwords, passphrases and keys have more possible values, making them exponentially more difficult to crack than shorter ones due to diversity of characters.\nBrute-force attacks can be made less effective by obfuscating the data to be encoded making it more difficult for an attacker to recognize when the code has been cracked or by making the attacker do more work to test each guess. One of the measures of the strength of an encryption system is how long it would theoretically take an attacker to mount a successful brute-force attack against it.\nBrute-force attacks are an application of brute-force search, the general problem-solving technique of enumerating all candidates and checking each one. The word 'hammering' is sometimes used to describe a brute-force attack, with 'anti-hammering' for countermeasures."
        },
        {
            "id": "c_a0_1",
            "title": "Cryptocurrency",
            "text": "A cryptocurrency, crypto-currency, or crypto is a digital currency designed to work through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it. \nIndividual coin ownership records are stored in a digital ledger or \"blockchain,\" which is a computerized database using strong cryptography to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership. Despite its name, which has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdicitons, including classification as commodities, securities, and currencies. Cryptocurrencies are generally viewed as a distinct asset class in practice. Some crypto schemes use validators to maintain the cryptocurrency. The first cryptocurrency was bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion. == History == In 1983, American cryptographer David Chaum conceived of a type of cryptographic electronic money called ecash. Later, in 1995, he implemented it through Digicash, an early form of cryptographic electronic payments. Digicash required user software in order to withdraw notes from a bank and designate specific encrypted keys before they could be sent to a recipient. This allowed the digital currency to be untraceable by a third party.\nIn 1996, the National Security Agency published a paper entitled How to Make a Mint: The Cryptography of Anonymous Electronic Cash, describing a cryptocurrency system. The paper was first published in an MIT mailing list (October 1996) and later (April 1997) in The American Law Review. In 1998, Wei Dai described \"b-money,\" an anonymous, distributed electronic cash system. Shortly thereafter, Nick Szabo described bit gold. Like bitcoin and other cryptocurrencies that would follow it, bit gold (not to be confused with the later gold-based exchange BitGold) was described as an electronic currency system that required users to complete a proof of work function with solutions being cryptographically put together and published.\nIn January 2009, bitcoin was created by pseudonymous developer Satoshi Nakamoto. It used SHA-256, a cryptographic hash function, in its proof-of-work scheme. In April 2011, Namecoin was created as an attempt at forming a decentralized DNS. In October 2011, Litecoin was released, which used scrypt as its hash function instead of SHA-256. Peercoin, created in August 2012, used a hybrid of proof-of-work and proof-of-stake. Cryptocurrency has undergone several periods of growth and retraction, including several bubbles and market crashes, such as in 2011, 2013\u20132014/15, 2017\u20132018, and 2021\u20132023.\nOn 6 August 2014, the UK announced its Treasury had commissioned a study of cryptocurrencies and what role, if any, they could play in the UK economy. The study was also to report on whether regulation should be considered. Its final report was published in 2018, and it issued a consultation on cryptoassets and stablecoins in January 2021.\nIn June 2021, El Salvador became the first country to accept bitcoin as legal tender, after the Legislative Assembly had voted 62\u201322 to pass a bill submitted by President Nayib Bukele classifying the cryptocurrency as such.\nIn August 2021, Cuba followed with Resolution 215 to recognize and regulate cryptocurrencies such as bitcoin. In September 2021, the government of China, the single largest market for cryptocurrency, declared all cryptocurrency transactions illegal. This completed a crackdown on cryptocurrency that had previously banned the operation of intermediaries and miners within China.\nOn 15 September 2022, the world's second largest cryptocurrency",
            "link": "https://en.wikipedia.org/wiki/Cryptocurrency",
            "snippet": "A cryptocurrency, crypto-currency, or crypto is a digital currency designed to work through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it. \nIndividual coin ownership records are stored in a digital ledger or \"blockchain,\" which is a computerized database using strong cryptography to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership. Despite its name, which has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdicitons, including classification as commodities, securities, and currencies. Cryptocurrencies are generally viewed as a distinct asset class in practice. Some crypto schemes use validators to maintain the cryptocurrency.\nThe first cryptocurrency was bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion."
        },
        {
            "id": "c_a0_2",
            "title": "History of bitcoin",
            "text": "Bitcoin is a cryptocurrency, a digital asset that uses cryptography to control its creation and management rather than relying on central authorities. Originally designed as a medium of exchange, Bitcoin is now primarily regarded as a store of value. The history of bitcoin started with its invention and implementation by Satoshi Nakamoto, who integrated many existing ideas from the cryptography community. Over the course of bitcoin's history, it has undergone rapid growth to become a significant store of value both on- and offline. From the mid-2010s, some businesses began accepting bitcoin in addition to traditional currencies.\n\n\n== Background ==\nPrior to the release of bitcoin, there were a number of digital cash technologies, starting with the issuer-based ecash protocols of David Chaum and Stefan Brands. The idea that solutions to computational puzzles could have some value was first proposed by cryptographers Cynthia Dwork and Moni Naor in 1992. === 31 October 1996 NSA paper ===\n12 years prior to creating Bitcoin the NSA published the white paper HOW TO MAKE A MINT: THE CRYPTOGRAPHY OF ANONYMOUS ELECTRONIC CASH \n\n\n=== Adam Back ===\nThe idea was independently rediscovered by Adam Back who developed hashcash, a proof-of-work scheme for spam control in 1997. The first proposals for distributed digital scarcity-based cryptocurrencies were Wei Dai's b-money and Nick Szabo's bit gold. Hal Finney developed reusable proof of work (RPOW) using hashcash as its proof of work algorithm.\nIn the bit gold proposal which proposed a collectible market-based mechanism for inflation control, Nick Szabo also investigated some additional aspects including a Byzantine fault-tolerant agreement protocol based on quorum addresses to store and transfer the chained proof-of-work solutions, which was vulnerable to Sybil attacks, though. == Creation ==\nOn the 18th of August 2008, the domain name bitcoin.org was registered. Later that year, on 31 October, a link to a paper authored by Satoshi Nakamoto titled Bitcoin: A Peer-to-Peer Electronic Cash System was posted to a cryptography mailing list. This paper detailed methods of using a peer-to-peer network to generate what was described as \"a system for electronic transactions without relying on trust\". On 3 January 2009, the bitcoin network came into existence with Satoshi Nakamoto mining the genesis block of bitcoin (block number 0), which had a reward of 50 bitcoins. Embedded in the genesis block was the text: The Times 03/Jan/2009 Chancellor on brink of second bailout for banks\nThe text refers to a headline in The Times published on 3 January 2009. This note has been interpreted as both a timestamp of the genesis date and a derisive comment on the instability caused by fractional-reserve banking.:\u200a18\u200a\nThe first open source bitcoin client was released on 9 January 2009, hosted at SourceForge.\nOne of the first supporters, adopters, contributors to bitcoin and receiver of the first bitcoin transaction was programmer Hal Finney. Finney downloaded the bitcoin software the day it was released, and received 10 bitcoins from Nakamoto in the world's first bitcoin transaction on 12 January 2009 (block 170). Other early supporters were Wei Dai, creator of bitcoin predecessor b-money, and Nick Szabo, creator of bitcoin predecessor bit gold. One of the first miners included James Howells, who subsequently lost thousands of Bitcoin to a landfill in Newport. In the early days, Nakamoto is estimated to have mined 1 million bitcoins. Before disappearing from any involvement in bitcoin, Nakamoto in a sense handed over the reins to developer Gavin Andresen, who then became the bitcoin lead developer at the Bitcoin Foundation, the 'anarchic' bitcoin community's closest thing to an official public face. === Satoshi Nakamoto ===\n\n\"Satoshi Nakamoto\" is presumed to be a pseudonym for the person or people who designed the original bitcoin protocol in 2007 then released the whitepaper in 2008 and finally launched the",
            "link": "https://en.wikipedia.org/wiki/History_of_bitcoin",
            "snippet": "Bitcoin is a cryptocurrency, a digital asset that uses cryptography to control its creation and management rather than relying on central authorities. Originally designed as a medium of exchange, Bitcoin is now primarily regarded as a store of value. The history of bitcoin started with its invention and implementation by Satoshi Nakamoto, who integrated many existing ideas from the cryptography community. Over the course of bitcoin's history, it has undergone rapid growth to become a significant store of value both on- and offline. From the mid-2010s, some businesses began accepting bitcoin in addition to traditional currencies."
        },
        {
            "id": "c_a1_0",
            "title": "Cryptographic hash function",
            "text": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application: the probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox). Cryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions. == Properties ==\nMost cryptographic hash functions are designed to take a string of any length as input and produce a fixed-length hash value.\nA cryptographic hash function must be able to withstand all known types of cryptanalytic attack. In theoretical cryptography, the security level of a cryptographic hash function has been defined using the following properties: Pre-image resistance\nGiven a hash value h, it should be difficult to find any message m such that h = hash(m). This concept is related to that of a one-way function. Functions that lack this property are vulnerable to preimage attacks.\nSecond pre-image resistance\nGiven an input m1, it should be difficult to find a different input m2 such that hash(m1) = hash(m2). This property is sometimes referred to as weak collision resistance. Functions that lack this property are vulnerable to second-preimage attacks.\nCollision resistance\nIt should be difficult to find two different messages m1 and m2 such that hash(m1) = hash(m2). Such a pair is called a cryptographic hash collision. This property is sometimes referred to as strong collision resistance. It requires",
            "link": "https://en.wikipedia.org/wiki/Cryptographic_hash_function",
            "snippet": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application:\n\nthe probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox).\nCryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions.\n\n"
        },
        {
            "id": "c_a1_1",
            "title": "Hash function",
            "text": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing. Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527 Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords. == Overview ==\nIn a hash table, a hash function takes a key as an input, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed-length, like an integer, or variable-length, like a name.  In some cases, the key is the datum itself.  The output is a hash code used to index a hash table holding the data or records, or pointers to them.\nA hash function may be considered to perform three functions: Convert variable-length keys into fixed-length (usually machine-word-length or less) values, by folding them by words or other units using a parity-preserving operator like ADD or XOR,\nScramble the bits of the key so that the resulting values are uniformly distributed over the keyspace, and\nMap the key values into ones less than or equal to the size of the table. A good hash function satisfies two basic properties: it should be very fast to compute, and it should minimize duplication of output values (collisions).  Hash functions rely on generating favorable probability distributions for their effectiveness, reducing access time to nearly constant.  High table loading factors, pathological key sets, and poorly designed hash functions can result in access times approaching linear in the number of items in the table. Hash functions can be designed to give the best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists, or systematic probing of the table to find an empty slot. == Hash tables ==\n\nHash functions are used in conjunction with hash tables to store and retrieve data items or data records. The hash function translates the key associated with each datum or record into a hash code, which is used to index the hash table. When an item is to be added to the table, the hash code may index an empty slot (also called a bucket), in which case the item is added t",
            "link": "https://en.wikipedia.org/wiki/Hash_function",
            "snippet": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing.\nHash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527\u200a\nHash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords."
        },
        {
            "id": "c_a1_2",
            "title": "Hash table",
            "text": "In computer science, a hash table is a data structure that implements an associative array, also called a dictionary or simply map; an associative array is an abstract data type that maps keys to values. A hash table uses a hash function to compute an index, also called a hash code, into an array of buckets or slots, from which the desired value can be found. During lookup, the key is hashed and the resulting hash indicates where the corresponding value is stored. A map implemented by a hash table is called a hash map.\nMost hash table designs employ an imperfect hash function. Hash collisions, where the hash function generates the same index for more than one key, therefore typically must be accommodated in some way. In a well-dimensioned hash table, the average time complexity for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key\u2013value pairs, at amortized constant average cost per operation.\nHashing is an example of a space-time tradeoff. If memory is infinite, the entire key can be used directly as an index to locate its value with a single memory access. On the other hand, if infinite time is available, values can be stored without regard for their keys, and a binary search or linear search can be used to retrieve the element.:\u200a458\u200a\nIn many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets. == History ==\nThe idea of hashing arose independently in different places. In January 1953, Hans Peter Luhn wrote an internal IBM memorandum that used hashing with chaining.  The first example of open addressing was proposed by A. D. Linh, building on Luhn's memorandum.:\u200a547\u200a Around the same time, Gene Amdahl, Elaine M. McGraw, Nathaniel Rochester, and Arthur Samuel of IBM Research implemented hashing for the IBM 701 assembler.:\u200a124\u200a Open addressing with linear probing is credited to Amdahl, although Andrey Ershov independently had the same idea.:\u200a124\u2013125\u200a The term \"open addressing\" was coined by W. Wesley Peterson in his article which discusses the problem of search in large files.:\u200a15 The first published work on hashing with chaining is credited to Arnold Dumey, who discussed the idea of using remainder modulo a prime as a hash function.:\u200a15\u200a The word \"hashing\" was first published in an article by Robert Morris.:\u200a126\u200a A theoretical analysis of linear probing was submitted originally by Konheim and Weiss.:\u200a15 == Overview ==\nAn associative array stores a set of (key, value) pairs and allows insertion, deletion, and lookup (search), with the constraint of unique keys. In the hash table implementation of associative arrays, an array \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n of length \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n is partially filled with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n elements, where \n  \n    \n      \n        m\n        \u2265\n        n\n      \n    \n    {\\displaystyle m\\geq n}\n  \n. A value \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n gets stored at an index location \n  \n    \n      \n        A\n        [\n        h\n        (\n        x\n        )\n        ]\n      \n    \n    {\\displaystyle A[h(x)]}\n  \n, where \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n is a hash function, and \n  \n    \n      \n        h\n        (\n        x\n        )\n        <\n        m {\\displaystyle h(x)<m}\n  \n.:\u200a2\u200a Under reasonable assumptions, hash tables have better time complexity bounds on search, delete, and insert operations in comparison to self-balancing binary search trees.:\u200a1\u200a\nHash tables are also commonly used to implement sets, by omitting the stored value for each key and merely tracking whether the key is present.:\u200a1",
            "link": "https://en.wikipedia.org/wiki/Hash_table",
            "snippet": "In computer science, a hash table is a data structure that implements an associative array, also called a dictionary or simply map; an associative array is an abstract data type that maps keys to values. A hash table uses a hash function to compute an index, also called a hash code, into an array of buckets or slots, from which the desired value can be found. During lookup, the key is hashed and the resulting hash indicates where the corresponding value is stored. A map implemented by a hash table is called a hash map.\nMost hash table designs employ an imperfect hash function. Hash collisions, where the hash function generates the same index for more than one key, therefore typically must be accommodated in some way.\nIn a well-dimensioned hash table, the average time complexity for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key\u2013value pairs, at amortized constant average cost per operation.\nHashing is an example of a space-time tradeoff. If memory is infinite, the entire key can be used directly as an index to locate its value with a single memory access. On the other hand, if infinite time is available, values can be stored without regard for their keys, and a binary search or linear search can be used to retrieve the element.:\u200a458\u200a\nIn many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets."
        },
        {
            "id": "c_a2_0",
            "title": "SHA-3",
            "text": "SHA-3 (Secure Hash Algorithm 3) is the latest member of the Secure Hash Algorithm family of standards, released by NIST on August 5, 2015. Although part of the same series of standards, SHA-3 is internally different from the MD5-like structure of SHA-1 and SHA-2.\nSHA-3 is a subset of the broader cryptographic primitive family Keccak ( or ), designed by Guido Bertoni, Joan Daemen, Micha\u00ebl Peeters, and Gilles Van Assche, building upon RadioGat\u00fan. Keccak's authors have proposed additional uses for the function, not (yet) standardized by NIST, including a stream cipher, an authenticated encryption system, a \"tree\" hashing scheme for faster hashing on certain architectures, and AEAD ciphers Keyak and Ketje. Keccak is based on a novel approach called sponge construction. Sponge construction is based on a wide random function or random permutation, and allows inputting (\"absorbing\" in sponge terminology) any amount of data, and outputting (\"squeezing\") any amount of data, while acting as a pseudorandom function with regard to all previous inputs. This leads to great flexibility.\nAs of 2022, NIST does not plan to withdraw SHA-2 or remove it from the revised Secure Hash Standard. The purpose of SHA-3 is that it can be directly substituted for SHA-2 in current applications if necessary, and to significantly improve the robustness of NIST's overall hash algorithm toolkit.\nFor small message sizes, the creators of the Keccak algorithms and the SHA-3 functions suggest using the faster function KangarooTwelve with adjusted parameters and a new tree hashing mode without extra overhead. == History ==\nThe Keccak algorithm is the work of Guido Bertoni, Joan Daemen (who also co-designed the Rijndael cipher with Vincent Rijmen), Micha\u00ebl Peeters, and Gilles Van Assche. It is based on earlier hash function designs PANAMA and RadioGat\u00fan. PANAMA was designed by Daemen and Craig Clapp in 1998. RadioGat\u00fan, a successor of PANAMA, was designed by Daemen, Peeters, and Van Assche, and was presented at the NIST Hash Workshop in 2006. The reference implementation source code was dedicated to public domain via CC0 waiver.\nIn 2006, NIST started to organize the NIST hash function competition to create a new hash standard, SHA-3. SHA-3 is not meant to replace SHA-2, as no significant attack on SHA-2 has been publicly demonstrated . Because of the successful attacks on MD5, SHA-0 and SHA-1,\nNIST perceived a need for an alternative, dissimilar cryptographic hash, which became SHA-3. After a setup period, admissions were to be submitted by the end of 2008. Keccak was accepted as one of the 51 candidates. In July 2009, 14 algorithms were selected for the second round. Keccak advanced to the last round in December 2010.\nDuring the competition, entrants were permitted to \"tweak\" their algorithms to address issues that were discovered. Changes that have been made to Keccak are: The number of rounds was increased from 12 + \u2113 to 12 + 2\u2113 to be more conservative about security.\nThe message padding was changed from a more complex scheme to the simple 10*1 pattern described below.\nThe rate r was increased to the security limit, rather than rounding down to the nearest power of 2.\nOn October 2, 2012, Keccak was selected as the winner of the competition.\nIn 2014, the NIST published a draft FIPS 202 \"SHA-3 Standard: Permutation-Based Hash and Extendable-Output Functions\". FIPS 202 was approved on August 5, 2015.\nOn August 5, 2015, NIST announced that SHA-3 had become a hashing standard. === Weakening controversy ===\nIn early 2013 NIST announced they would select different values for the \"capacity\", the overall strength vs. speed parameter, for the SHA-3 standard, compared to the submission. The changes caused some turmoil.\nThe hash function competition called for hash functions at least as secure as the SHA-2 instances. It means that a d-bit output should have d/2-bit resistance to collision attacks and d-bit resistance to preimage attacks, the maximum achievable for d bits",
            "link": "https://en.wikipedia.org/wiki/SHA-3",
            "snippet": "SHA-3 (Secure Hash Algorithm 3) is the latest member of the Secure Hash Algorithm family of standards, released by NIST on August 5, 2015. Although part of the same series of standards, SHA-3 is internally different from the MD5-like structure of SHA-1 and SHA-2.\nSHA-3 is a subset of the broader cryptographic primitive family Keccak ( or ), designed by Guido Bertoni, Joan Daemen, Micha\u00ebl Peeters, and Gilles Van Assche, building upon RadioGat\u00fan. Keccak's authors have proposed additional uses for the function, not (yet) standardized by NIST, including a stream cipher, an authenticated encryption system, a \"tree\" hashing scheme for faster hashing on certain architectures, and AEAD ciphers Keyak and Ketje.\nKeccak is based on a novel approach called sponge construction. Sponge construction is based on a wide random function or random permutation, and allows inputting (\"absorbing\" in sponge terminology) any amount of data, and outputting (\"squeezing\") any amount of data, while acting as a pseudorandom function with regard to all previous inputs. This leads to great flexibility.\nAs of 2022, NIST does not plan to withdraw SHA-2 or remove it from the revised Secure Hash Standard. The purpose of SHA-3 is that it can be directly substituted for SHA-2 in current applications if necessary, and to significantly improve the robustness of NIST's overall hash algorithm toolkit.\nFor small message sizes, the creators of the Keccak algorithms and the SHA-3 functions suggest using the faster function KangarooTwelve with adjusted parameters and a new tree hashing mode without extra overhead.\n\n"
        },
        {
            "id": "c_a2_1",
            "title": "List of algorithms",
            "text": "An algorithm is fundamentally a set of rules or defined procedures that is typically designed and used to solve a specific problem or a broad set of problems. \nBroadly, algorithms define process(es), sets of rules, or methodologies that are to be followed in calculations, data processing, data mining, pattern recognition, automated reasoning or other problem-solving operations. With the increasing automation of services, more and more decisions are being made by algorithms. Some general examples are; risk assessments, anticipatory policing, and pattern recognition technology.\nThe following is a list of well-known algorithms along with one-line descriptions for each.\n\n\n== Automated planning ==\n\n\n== Combinatorial algorithms == === General combinatorial algorithms ===\nBrent's algorithm: finds a cycle in function value iterations using only two iterators\nFloyd's cycle-finding algorithm: finds a cycle in function value iterations\nGale\u2013Shapley algorithm: solves the stable marriage problem\nPseudorandom number generators (uniformly distributed\u2014see also List of pseudorandom number generators for other PRNGs with varying degrees of convergence and varying statistical quality):\nACORN generator\nBlum Blum Shub\nLagged Fibonacci generator\nLinear congruential generator\nMersenne Twister\n\n\n=== Graph algorithms === Coloring algorithm: Graph coloring algorithm.\nHopcroft\u2013Karp algorithm: convert a bipartite graph to a maximum cardinality matching\nHungarian algorithm: algorithm for finding a perfect matching\nPr\u00fcfer coding: conversion between a labeled tree and its Pr\u00fcfer sequence\nTarjan's off-line lowest common ancestors algorithm: computes lowest common ancestors for pairs of nodes in a tree\nTopological sort: finds linear order of nodes (e.g. jobs) based on their dependencies.\n\n\n==== Graph drawing ====\n\nForce-based algorithms (also known as force-directed algorithms or spring-based algorithm)\nSpectral layout\n\n\n==== Network theory ==== Network analysis\nLink analysis\nGirvan\u2013Newman algorithm: detect communities in complex systems\nWeb link analysis\nHyperlink-Induced Topic Search (HITS) (also known as Hubs and authorities)\nPageRank\nTrustRank\nFlow networks\nDinic's algorithm: is a strongly polynomial algorithm for computing the maximum flow in a flow network.\nEdmonds\u2013Karp algorithm: implementation of Ford\u2013Fulkerson\nFord\u2013Fulkerson algorithm: computes the maximum flow in a graph\nKarger's algorithm: a Monte Carlo method to compute the minimum cut of a connected graph\nPush\u2013relabel algorithm: computes a maximum flow in a graph ==== Routing for graphs ====\nEdmonds' algorithm (also known as Chu\u2013Liu/Edmonds' algorithm): find maximum or minimum branchings\nEuclidean minimum spanning tree: algorithms for computing the minimum spanning tree of a set of points in the plane\nLongest path problem: find a simple path of maximum length in a given graph\nMinimum spanning tree\nBor\u016fvka's algorithm\nKruskal's algorithm\nPrim's algorithm\nReverse-delete algorithm\nNonblocking minimal spanning switch say, for a telephone exchange\nShortest path problem\nBellman\u2013Ford algorithm: computes shortest paths in a weighted graph (where some of the edge weights may be negative)\nDijkstra's algorithm: computes shortest paths in a graph with non-negative edge weights\nFloyd\u2013Warshall algorithm: solves the all pairs shortest path problem in a weighted, directed graph\nJohnson's algorithm: all pairs shortest path algorithm in sparse weighted directed graph\nTransitive closure problem: find the transitive closure of a given binary relation Traveling salesman problem\nChristofides algorithm\nNearest neighbour algorithm\nWarnsdorff's rule: a heuristic method for solving the Knight's tour problem ==== Graph search ====\n\nA*: special case of best-first search that uses heuristics to improve speed\nB*: a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals)\nBacktracking: abandons partial solutions when they are found n",
            "link": "https://en.wikipedia.org/wiki/List_of_algorithms",
            "snippet": "An algorithm is fundamentally a set of rules or defined procedures that is typically designed and used to solve a specific problem or a broad set of problems. \nBroadly, algorithms define process(es), sets of rules, or methodologies that are to be followed in calculations, data processing, data mining, pattern recognition, automated reasoning or other problem-solving operations. With the increasing automation of services, more and more decisions are being made by algorithms. Some general examples are; risk assessments, anticipatory policing, and pattern recognition technology.\nThe following is a list of well-known algorithms along with one-line descriptions for each."
        },
        {
            "id": "c_a2_2",
            "title": "Glossary of computer science",
            "text": "This glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.\n\n\n== A ==\n\nabstract data type (ADT)\nA mathematical model for data types in which a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This contrasts with data structures, which are concrete representations of data from the point of view of an implementer rather than a user.\n\nabstract method\nOne with only a signature and no implementation body. It is often used to specify that a subclass must provide an implementation of the method. Abstract methods are used to specify interfaces in some computer languages. abstraction\n1.  In software engineering and computer science, the process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest; it is also very similar in nature to the process of generalization.\n2.  The result of this process: an abstract concept-object created by keeping common features or attributes to various concrete objects or systems of study.\n\nagent architecture\nA blueprint for software agents and intelligent control systems depicting the arrangement of components. The architectures implemented by intelligent agents are referred to as cognitive architectures. agent-based model (ABM)\nA class of computational models for simulating the actions and interactions of autonomous agents (both individual or collective entities such as organizations or groups) with a view to assessing their effects on the system as a whole. It combines elements of game theory, complex systems, emergence, computational sociology, multi-agent systems, and evolutionary programming. Monte Carlo methods are used to introduce randomness.\n\naggregate function\nIn database management, a function in which the values of multiple rows are grouped together to form a single value of more significant meaning or measurement, such as a sum, count, or max. agile software development\nAn approach to software development under which requirements and solutions evolve through the collaborative effort of self-organizing and cross-functional teams and their customer(s)/end user(s). It advocates adaptive planning, evolutionary development, early delivery, and continual improvement, and it encourages rapid and flexible response to change.\n\nalgorithm\nAn unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing, and automated reasoning tasks. They are ubiquitous in computing technologies.\n\nalgorithm design\nA method or mathematical process for problem-solving and for engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern. algorithmic efficiency\nA property of an algorithm which relates to the number of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.\n\nAmerican Standard Code for Information Interchange (ASCII)\nA character encoding standard for electronic communications. ASCII codes represent text in computers, telecommunications equipment, and other devices. Most modern character-encoding schemes are based on ASCII, although they support many additional characters.\n\napplication programming interface (",
            "link": "https://en.wikipedia.org/wiki/Glossary_of_computer_science",
            "snippet": "This glossary of computer science is a list of definitions of terms and concepts used in computer science, its sub-disciplines, and related fields, including terms relevant to software, data science, and computer programming.\n\n"
        },
        {
            "id": "c_a3_0",
            "title": "Hash function",
            "text": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing. Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527 Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords. == Overview ==\nIn a hash table, a hash function takes a key as an input, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed-length, like an integer, or variable-length, like a name.  In some cases, the key is the datum itself.  The output is a hash code used to index a hash table holding the data or records, or pointers to them.\nA hash function may be considered to perform three functions: Convert variable-length keys into fixed-length (usually machine-word-length or less) values, by folding them by words or other units using a parity-preserving operator like ADD or XOR,\nScramble the bits of the key so that the resulting values are uniformly distributed over the keyspace, and\nMap the key values into ones less than or equal to the size of the table. A good hash function satisfies two basic properties: it should be very fast to compute, and it should minimize duplication of output values (collisions).  Hash functions rely on generating favorable probability distributions for their effectiveness, reducing access time to nearly constant.  High table loading factors, pathological key sets, and poorly designed hash functions can result in access times approaching linear in the number of items in the table. Hash functions can be designed to give the best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists, or systematic probing of the table to find an empty slot. == Hash tables ==\n\nHash functions are used in conjunction with hash tables to store and retrieve data items or data records. The hash function translates the key associated with each datum or record into a hash code, which is used to index the hash table. When an item is to be added to the table, the hash code may index an empty slot (also called a bucket), in which case the item is added t",
            "link": "https://en.wikipedia.org/wiki/Hash_function",
            "snippet": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing.\nHash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527\u200a\nHash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords."
        },
        {
            "id": "c_a3_1",
            "title": "Bcrypt",
            "text": "bcrypt is a password-hashing function designed by Niels Provos and David Mazi\u00e8res, based on the Blowfish cipher and presented at USENIX in 1999. Besides incorporating a salt to protect against rainbow table attacks, bcrypt is an adaptive function: over time, the iteration count can be increased to make it slower, so it remains resistant to brute-force search attacks even with increasing computation power.\nThe bcrypt function is the default password hash algorithm for OpenBSD, and was the default for some Linux distributions such as SUSE Linux.\nThere are implementations of bcrypt in C, C++, C#, Embarcadero Delphi, Elixir, Go, Java, JavaScript, Perl, PHP, Ruby, Python, Rust, Zig and other languages. == Background ==\nBlowfish is notable among block ciphers for its expensive key setup phase.  It starts off with subkeys in a standard state, then uses this state to perform a block encryption using part of the key, and uses the result of that encryption (which is more accurate at hashing) to replace some of the subkeys.  Then it uses this modified state to encrypt another part of the key, and uses the result to replace more of the subkeys.  It proceeds in this fashion, using a progressively modified state to hash the key and replace bits of state, until all subkeys have been set. Provos and Mazi\u00e8res took advantage of this, and took it further.  They developed a new key setup algorithm for Blowfish, dubbing the resulting cipher \"Eksblowfish\" (\"expensive key schedule Blowfish\").  The key setup begins with a modified form of the standard Blowfish key setup, in which both the salt and password are used to set all subkeys.  There are then a number of rounds in which the standard Blowfish keying algorithm is applied, using alternatively the salt and the password as the key, each round starting with the subkey state from the previous round.  In theory, this is no stronger than the standard Blowfish key schedule, but the number of rekeying rounds is configurable; this process can therefore be made arbitrarily slow, which helps deter brute-force attacks upon the hash or salt. == Description ==\nThe input to the bcrypt function is the password string (up to 72 bytes), a numeric cost, and a 16-byte (128-bit) salt value.  The salt is typically a random value.  The bcrypt function uses these inputs to compute a 24-byte (192-bit) hash.  The final output of the bcrypt function is a string of the form:\n\n$2<a/b/x/y>$[cost]$[22 character salt][31 character hash]\n\nFor example, with input password abc123xyz, cost 12, and a random salt, the output of bcrypt is the string\n\n$2a$12$R9h/cIPz0gi.URNNX3kh2OPST9/PgBkqquzi.Ss7KIUgO2t0jWMUW\n\\__/\\/ \\____________________/\\_____________________________/\nAlg Cost      Salt                        Hash\n\nWhere: $2a$: The hash algorithm identifier (bcrypt)\n12: Input cost (212 i.e. 4096 rounds)\nR9h/cIPz0gi.URNNX3kh2O: A base-64 encoding of the input salt\nPST9/PgBkqquzi.Ss7KIUgO2t0jWMUW: A base-64 encoding of the first 23 bytes of the computed 24 byte hash\nThe base-64 encoding in bcrypt uses the table ./ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789, which differs from RFC 4648 Base64 encoding.\n\n\n== Versioning history ==\n $2$ (1999) \nThe original bcrypt specification defined a prefix of $2$. This follows the Modular Crypt Format format used when storing passwords in the OpenBSD password file:\n\n$1$: MD5-based crypt ('md5crypt')\n$2$: Blowfish-based crypt ('bcrypt')\n$sha1$: SHA-1-based crypt ('sha1crypt')\n$5$: SHA-256-based crypt ('sha256crypt')\n$6$: SHA-512-based crypt ('sha512crypt')\n $2a$ \nThe original specification did not define how to handle non-ASCII character, nor how to handle a null terminator. The specification was revised to specify that when hashing strings: the string must be UTF-8 encoded\nthe null terminator must be included\nWith this change, the version was changed to $2a$\n $2x$, $2y$ (June 2011) \nIn June 2011, a bug was discovered in crypt_blowfish, a PHP implementation of bcrypt. It was",
            "link": "https://en.wikipedia.org/wiki/Bcrypt",
            "snippet": "bcrypt is a password-hashing function designed by Niels Provos and David Mazi\u00e8res, based on the Blowfish cipher and presented at USENIX in 1999. Besides incorporating a salt to protect against rainbow table attacks, bcrypt is an adaptive function: over time, the iteration count can be increased to make it slower, so it remains resistant to brute-force search attacks even with increasing computation power.\nThe bcrypt function is the default password hash algorithm for OpenBSD, and was the default for some Linux distributions such as SUSE Linux.\nThere are implementations of bcrypt in C, C++, C#, Embarcadero Delphi, Elixir, Go, Java, JavaScript, Perl, PHP, Ruby, Python, Rust, Zig and other languages.\n\n"
        },
        {
            "id": "c_a3_2",
            "title": "Product key",
            "text": "A product key, also known as a software key, serial key or activation key, is a specific software-based key for a computer program. It certifies that the copy of the program is original.\nProduct keys consist of a series of numbers and/or letters. This sequence is typically entered by the user during the installation of computer software, and is then passed to a verification function in the program. This function manipulates the key sequence according to a mathematical algorithm and attempts to match the results to a set of valid solutions. == Effectiveness ==\nStandard key generation, where product keys are generated mathematically, is not completely effective in stopping copyright infringement of software, as these keys can be distributed. In addition, with improved communication from the rise of the Internet, more sophisticated attacks on keys such as cracks (removing the need for a key) and product key generators have become common. Because of this, software publishers use additional product activation methods to verify that keys are both valid and uncompromised. One method assigns a product key based on a unique feature of the purchaser's computer hardware, which cannot be as easily duplicated since it depends on the user's hardware. Another method involves requiring one-time or periodical validation of the product key with an internet server (for games with an online component, this is done whenever the user signs in). The server can deactivate unmodified client software presenting invalid or compromised keys. Modified clients may bypass these checks, but the server can still deny those clients information or communication. == Examples ==\n\n\n=== Windows 95 retail key ===\nWindows 95 retail product keys take the form XXX-XXXXXXX. To determine whether the key is valid, Windows 95 performs the following checks:\n\nThe first 3 characters must not be equal to 333, 444, 555, 666, 777, 888 or 999.\nThe last 7 characters must all be numbers from 0-8.\nThe sum of the last 7 numbers must be divisible by 7 with no remainder.\nThe fourth character is unchecked.\nIf all checks pass, the product key is valid. Consequently a product key of 000-0000000 would be considered valid under these conditions.\n\n\n=== Windows 95 OEM key ===\nWindows 95 OEM keys take the form XXXXX-OEM-XXXXXXX-XXXXX.\n\nThe first 3 characters must be a number between 0-366.\nThe next 2 characters must be a number between 04-93.\nThe next 3 characters must be OEM.\nThe sum of the next 7 numbers must be divisible by 7 with no remainder.\nThe rest of the characters are unchecked. === Windows XP retail key ===\nWindows XP uses an installation ID, product ID, and a product key for activation. \n\n\n==== Installation ID ====\nThe installation ID is a 50 digit decimal string that is divided into 5 groups of six digits each with 2 digits at the end, which takes the form of XXXXXX-XXXXXX-XXXXXX-XXXXXX-XXXXXX-XXXXXX-XXXXXX-XXXXXX-XX. The installation ID is regenerated every time msoobe.exe is ran. \n\n\n===== Check digits =====\nThe right most digit in each group of the installation ID is a check digit.\n\nEach check digit is calculated by adding the other five digits in its group.\nThen adding the digits in that group in the even positions a second time.\nThen dividing the sum of them by 7.\nThe remainder is the value of the check digit in its group.\n\n\n===== Decoding =====\nRemoving the check digits results in a 41-digit decimal encoded 136 bit multi precision integer, which is stored in little endian byte order as a byte array. ===== Decryption =====\nThe lower 16 bytes of the Installation ID are encrypted, whereas the most significant byte is kept in plaintext. The cryptographic algorithm used to encrypt the Installation ID is a proprietary four-round Feistel cipher. Since the block of input bytes passed to a Feistel cipher is divided into two blocks of equal size, this class of ciphers is typically applied to input blocks consisting of an even number of bytes in this case the lower 16 of the 17",
            "link": "https://en.wikipedia.org/wiki/Product_key",
            "snippet": "A product key, also known as a software key, serial key or activation key, is a specific software-based key for a computer program. It certifies that the copy of the program is original.\nProduct keys consist of a series of numbers and/or letters. This sequence is typically entered by the user during the installation of computer software, and is then passed to a verification function in the program. This function manipulates the key sequence according to a mathematical algorithm and attempts to match the results to a set of valid solutions.\n\n"
        },
        {
            "id": "c_a4_0",
            "title": "Brute-force attack",
            "text": "In cryptography, a brute-force attack consists of an attacker submitting many passwords or passphrases with the hope of eventually guessing correctly. The attacker systematically checks all possible  passwords and passphrases until the correct one is found. Alternatively, the attacker can attempt to guess the key which is typically created from the password using a key derivation function. This is known as an exhaustive key search. This approach doesn't depend on intellectual tactics; rather, it relies on making several attempts.\nA brute-force attack is a cryptanalytic attack that can, in theory, be used to attempt to decrypt any encrypted data (except for data encrypted in an information-theoretically secure manner).  Such an attack might be used when it is not possible to take advantage of other weaknesses in an encryption system (if any exist) that would make the task easier. When password-guessing, this method is very fast when used to check all short passwords, but for longer passwords other methods such as the dictionary attack are used because a brute-force search takes too long. Longer passwords, passphrases and keys have more possible values, making them exponentially more difficult to crack than shorter ones due to diversity of characters.\nBrute-force attacks can be made less effective by obfuscating the data to be encoded making it more difficult for an attacker to recognize when the code has been cracked or by making the attacker do more work to test each guess. One of the measures of the strength of an encryption system is how long it would theoretically take an attacker to mount a successful brute-force attack against it. Brute-force attacks are an application of brute-force search, the general problem-solving technique of enumerating all candidates and checking each one. The word 'hammering' is sometimes used to describe a brute-force attack, with 'anti-hammering' for countermeasures. == Basic concept ==\nBrute-force attacks work by calculating every possible combination that could make up a password and testing it to see if it is the correct password. As the password's length increases, the amount of time, on average, to find the correct password increases exponentially. == Theoretical limits ==\nThe resources required for a brute-force attack grow exponentially with increasing key size, not linearly. Although U.S. export regulations historically restricted key lengths to 56-bit symmetric keys (e.g. Data Encryption Standard), these restrictions are no longer in place, so modern symmetric algorithms typically use computationally stronger 128- to 256-bit keys. There is a physical argument that a 128-bit symmetric key is computationally secure against brute-force attack. The Landauer limit implied by the laws of physics sets a lower limit on the energy required to perform a computation of kT  \u00b7  ln 2 per bit erased in a computation, where T is the temperature of the computing device in kelvins, k is the Boltzmann constant, and the natural logarithm of 2 is about 0.693 (0.6931471805599453). No irreversible computing device can use less energy than this, even in principle.  Thus, in order to simply flip through the possible values for a 128-bit symmetric key (ignoring doing the actual computing to check it) would, theoretically, require 2128 \u2212 1 bit flips on a conventional processor.  If it is assumed that the calculation occurs near room temperature (\u2248300 K), the Von Neumann-Landauer Limit can be applied to estimate the energy required as \u22481018 joules, which is equivalent to consuming 30 gigawatts of power for one year . This is equal to 30\u00d7109 W\u00d7365\u00d724\u00d73600 s = 9.46\u00d71017 J or 262.7 TWh (about 0.1% of the yearly world energy production). The full actual computation \u2013 checking each key to see if a solution has been found \u2013 would consume many times this amount. Furthermore, this is simply the energy requirement for cycling through the key space; the actual time it takes to flip each bit is not considered, which is c",
            "link": "https://en.wikipedia.org/wiki/Brute-force_attack",
            "snippet": "In cryptography, a brute-force attack consists of an attacker submitting many passwords or passphrases with the hope of eventually guessing correctly. The attacker systematically checks all possible  passwords and passphrases until the correct one is found. Alternatively, the attacker can attempt to guess the key which is typically created from the password using a key derivation function. This is known as an exhaustive key search. This approach doesn't depend on intellectual tactics; rather, it relies on making several attempts.\nA brute-force attack is a cryptanalytic attack that can, in theory, be used to attempt to decrypt any encrypted data (except for data encrypted in an information-theoretically secure manner).  Such an attack might be used when it is not possible to take advantage of other weaknesses in an encryption system (if any exist) that would make the task easier.\nWhen password-guessing, this method is very fast when used to check all short passwords, but for longer passwords other methods such as the dictionary attack are used because a brute-force search takes too long. Longer passwords, passphrases and keys have more possible values, making them exponentially more difficult to crack than shorter ones due to diversity of characters.\nBrute-force attacks can be made less effective by obfuscating the data to be encoded making it more difficult for an attacker to recognize when the code has been cracked or by making the attacker do more work to test each guess. One of the measures of the strength of an encryption system is how long it would theoretically take an attacker to mount a successful brute-force attack against it.\nBrute-force attacks are an application of brute-force search, the general problem-solving technique of enumerating all candidates and checking each one. The word 'hammering' is sometimes used to describe a brute-force attack, with 'anti-hammering' for countermeasures."
        },
        {
            "id": "c_a4_1",
            "title": "Cryptocurrency",
            "text": "A cryptocurrency, crypto-currency, or crypto is a digital currency designed to work through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it. \nIndividual coin ownership records are stored in a digital ledger or \"blockchain,\" which is a computerized database using strong cryptography to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership. Despite its name, which has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdicitons, including classification as commodities, securities, and currencies. Cryptocurrencies are generally viewed as a distinct asset class in practice. Some crypto schemes use validators to maintain the cryptocurrency. The first cryptocurrency was bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion. == History == In 1983, American cryptographer David Chaum conceived of a type of cryptographic electronic money called ecash. Later, in 1995, he implemented it through Digicash, an early form of cryptographic electronic payments. Digicash required user software in order to withdraw notes from a bank and designate specific encrypted keys before they could be sent to a recipient. This allowed the digital currency to be untraceable by a third party.\nIn 1996, the National Security Agency published a paper entitled How to Make a Mint: The Cryptography of Anonymous Electronic Cash, describing a cryptocurrency system. The paper was first published in an MIT mailing list (October 1996) and later (April 1997) in The American Law Review. In 1998, Wei Dai described \"b-money,\" an anonymous, distributed electronic cash system. Shortly thereafter, Nick Szabo described bit gold. Like bitcoin and other cryptocurrencies that would follow it, bit gold (not to be confused with the later gold-based exchange BitGold) was described as an electronic currency system that required users to complete a proof of work function with solutions being cryptographically put together and published.\nIn January 2009, bitcoin was created by pseudonymous developer Satoshi Nakamoto. It used SHA-256, a cryptographic hash function, in its proof-of-work scheme. In April 2011, Namecoin was created as an attempt at forming a decentralized DNS. In October 2011, Litecoin was released, which used scrypt as its hash function instead of SHA-256. Peercoin, created in August 2012, used a hybrid of proof-of-work and proof-of-stake. Cryptocurrency has undergone several periods of growth and retraction, including several bubbles and market crashes, such as in 2011, 2013\u20132014/15, 2017\u20132018, and 2021\u20132023.\nOn 6 August 2014, the UK announced its Treasury had commissioned a study of cryptocurrencies and what role, if any, they could play in the UK economy. The study was also to report on whether regulation should be considered. Its final report was published in 2018, and it issued a consultation on cryptoassets and stablecoins in January 2021.\nIn June 2021, El Salvador became the first country to accept bitcoin as legal tender, after the Legislative Assembly had voted 62\u201322 to pass a bill submitted by President Nayib Bukele classifying the cryptocurrency as such.\nIn August 2021, Cuba followed with Resolution 215 to recognize and regulate cryptocurrencies such as bitcoin. In September 2021, the government of China, the single largest market for cryptocurrency, declared all cryptocurrency transactions illegal. This completed a crackdown on cryptocurrency that had previously banned the operation of intermediaries and miners within China.\nOn 15 September 2022, the world's second largest cryptocurrency",
            "link": "https://en.wikipedia.org/wiki/Cryptocurrency",
            "snippet": "A cryptocurrency, crypto-currency, or crypto is a digital currency designed to work through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it. \nIndividual coin ownership records are stored in a digital ledger or \"blockchain,\" which is a computerized database using strong cryptography to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership. Despite its name, which has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdicitons, including classification as commodities, securities, and currencies. Cryptocurrencies are generally viewed as a distinct asset class in practice. Some crypto schemes use validators to maintain the cryptocurrency.\nThe first cryptocurrency was bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion."
        },
        {
            "id": "c_a4_2",
            "title": "Strong cryptography",
            "text": "Strong cryptography or cryptographically strong are general terms used to designate the cryptographic algorithms that, when used correctly, provide a very high (usually insurmountable) level  of protection against any eavesdropper, including the government agencies. There is no precise definition of the boundary line between the strong cryptography and (breakable) weak cryptography, as this border constantly shifts due to improvements in hardware and cryptanalysis techniques. These improvements eventually place the capabilities once available only to the NSA within the reach of a skilled individual, so in practice there are only two levels of cryptographic security, \"cryptography that will stop your kid sister from reading your files, and cryptography that will stop major governments from reading your files\" (Bruce Schneier). The strong cryptography algorithms have high security strength, for practical purposes usually defined as a number of bits in the key. For example, the United States government, when dealing with export control of encryption, considered as of 1999 any implementation of the symmetric encryption algorithm with the key length above 56 bits or its public key equivalent to be strong and thus potentially a subject to the export licensing. To be strong, an algorithm needs to have a sufficiently long key and be free of known mathematical weaknesses, as exploitation of these effectively reduces the key size. At the beginning of the 21st century, the typical security strength of the strong symmetrical encryption algorithms is 128 bits (slightly lower values still can be strong, but usually there is little technical gain in using smaller key sizes). Demonstrating the resistance of any cryptographic scheme to attack is a complex matter, requiring extensive testing and reviews, preferably in a public forum. Good algorithms and protocols are required (similarly, good materials are required to construct a strong building), but good system design and implementation is needed as well: \"it is possible to build a cryptographically weak system using strong algorithms and protocols\" (just like the use of good materials in construction does not guarantee a solid structure). Many real-life systems turn out to be weak when the strong cryptography is not used properly, for example, random nonces are reused A successful attack might not even involve algorithm at all, for example, if the key is generated from a password, guessing a weak password is easy and does not depend on the strength of the cryptographic primitives . A user can become the weakest link in the overall picture, for example, by sharing passwords and hardware tokens with the colleagues. == Background == The level of expense required for strong cryptography originally restricted its use to the government and military agencies, until the middle of the 20th century the process of encryption required a lot of human labor and errors (preventing the decryption) were very common, so only a small share of written information could have been encrypted. US government, in particular, was able to keep a monopoly on the development and use of cryptography in the US into the 1960s. In the 1970, the increased availability of powerful computers and unclassified research breakthroughs (Data Encryption Standard, the Diffie-Hellman and RSA algorithms) made strong cryptography available for civilian use. Mid-1990s saw the worldwide proliferation of knowledge and tools for strong cryptography. By the 21st century the technical limitations were gone, although the majority of the communication were still unencrypted . At the same the cost of building and running systems with strong cryptography became roughly the same as the one for the weak cryptography. The use of computers changed the process of cryptanalysis, famously with Bletchley Park's Colossus. But just as the development of digital computers and electronics helped in cryptanalysis, it also made possible much more complex cipher",
            "link": "https://en.wikipedia.org/wiki/Strong_cryptography",
            "snippet": "Strong cryptography or cryptographically strong are general terms used to designate the cryptographic algorithms that, when used correctly, provide a very high (usually insurmountable) level  of protection against any eavesdropper, including the government agencies. There is no precise definition of the boundary line between the strong cryptography and (breakable) weak cryptography, as this border constantly shifts due to improvements in hardware and cryptanalysis techniques. These improvements eventually place the capabilities once available only to the NSA within the reach of a skilled individual, so in practice there are only two levels of cryptographic security, \"cryptography that will stop your kid sister from reading your files, and cryptography that will stop major governments from reading your files\" (Bruce Schneier).\nThe strong cryptography algorithms have high security strength, for practical purposes usually defined as a number of bits in the key. For example, the United States government, when dealing with export control of encryption, considered as of 1999 any implementation of the symmetric encryption algorithm with the key length above 56 bits or its public key equivalent to be strong and thus potentially a subject to the export licensing. To be strong, an algorithm needs to have a sufficiently long key and be free of known mathematical weaknesses, as exploitation of these effectively reduces the key size. At the beginning of the 21st century, the typical security strength of the strong symmetrical encryption algorithms is 128 bits (slightly lower values still can be strong, but usually there is little technical gain in using smaller key sizes).\nDemonstrating the resistance of any cryptographic scheme to attack is a complex matter, requiring extensive testing and reviews, preferably in a public forum. Good algorithms and protocols are required (similarly, good materials are required to construct a strong building), but good system design and implementation is needed as well: \"it is possible to build a cryptographically weak system using strong algorithms and protocols\" (just like the use of good materials in construction does not guarantee a solid structure). Many real-life systems turn out to be weak when the strong cryptography is not used properly, for example, random nonces are reused A successful attack might not even involve algorithm at all, for example, if the key is generated from a password, guessing a weak password is easy and does not depend on the strength of the cryptographic primitives. A user can become the weakest link in the overall picture, for example, by sharing passwords and hardware tokens with the colleagues.\n\n"
        },
        {
            "id": "c_a5_0",
            "title": "Tokenization (data security)",
            "text": "Tokenization, when applied to data security, is the process of substituting a sensitive data element with a non-sensitive equivalent, referred to as a token, that has no intrinsic or exploitable meaning or value. The token is a reference (i.e. identifier) that maps back to the sensitive data through a tokenization system. The mapping from original data to a token uses methods that render tokens infeasible to reverse in the absence of the tokenization system, for example using tokens created from random numbers. A one-way cryptographic function is used to convert the original data into tokens, making it difficult to recreate the original data without obtaining entry to the tokenization system's resources. To deliver such services, the system maintains a vault database of tokens that are connected to the corresponding sensitive data. Protecting the system vault is vital to the system, and improved processes must be put in place to offer database integrity and physical security. The tokenization system must be secured and validated using security best practices applicable to sensitive data protection,  secure storage, audit, authentication and authorization. The tokenization system provides data processing applications with the authority and interfaces to request tokens, or detokenize back to sensitive data. \nThe security and risk reduction benefits of tokenization require that the tokenization system is logically isolated and segmented from data processing systems and applications that previously processed or stored sensitive data replaced by tokens. Only the tokenization system can tokenize data to create tokens, or detokenize back to redeem sensitive data under strict security controls. The token generation method must be proven to have the property that there is no feasible means through direct attack, cryptanalysis, side channel analysis, token mapping table exposure or brute force techniques to reverse tokens back to live data. Replacing live data with tokens in systems is intended to minimize exposure of sensitive data to those applications, stores, people and processes, reducing risk of compromise or accidental exposure and unauthorized access to sensitive data. Applications can operate using tokens instead of live data, with the exception of a small number of trusted applications explicitly permitted to detokenize when strictly necessary for an approved business purpose. Tokenization systems may be operated in-house within a secure isolated segment of the data center, or as a service from a secure service provider. Tokenization may be used to safeguard sensitive data involving, for example, bank accounts, financial statements, medical records, criminal records, driver's licenses, loan applications, stock trades, voter registrations, and other types of personally identifiable information (PII). Tokenization is often used in credit card processing. The PCI Council defines tokenization as \"a process by which the primary account number (PAN) is replaced with a surrogate value called a token. A PAN may be linked to a reference number through the tokenization process. In this case, the merchant simply has to retain the token and a reliable third party controls the relationship and holds the PAN. The token may be created independently of the PAN, or the PAN can be used as part of the data input to the tokenization technique. The communication between the merchant and the third-party supplier must be secure to prevent an attacker from intercepting to gain the PAN and the token. De-tokenization is the reverse process of redeeming a token for its associated PAN value. The security of an individual token relies predominantly on the infeasibility of determining the original PAN knowing only the surrogate value\". The choice of tokenization as an alternative to other techniques such as encryption will depend on varying regulatory requirements, interpretation, and acceptance by respective auditing or assessment entities. This is in add",
            "link": "https://en.wikipedia.org/wiki/Tokenization_(data_security)",
            "snippet": "Tokenization, when applied to data security, is the process of substituting a sensitive data element with a non-sensitive equivalent, referred to as a token, that has no intrinsic or exploitable meaning or value. The token is a reference (i.e. identifier) that maps back to the sensitive data through a tokenization system. The mapping from original data to a token uses methods that render tokens infeasible to reverse in the absence of the tokenization system, for example using tokens created from random numbers. A one-way cryptographic function is used to convert the original data into tokens, making it difficult to recreate the original data without obtaining entry to the tokenization system's resources. To deliver such services, the system maintains a vault database of tokens that are connected to the corresponding sensitive data. Protecting the system vault is vital to the system, and improved processes must be put in place to offer database integrity and physical security.\nThe tokenization system must be secured and validated using security best practices applicable to sensitive data protection,  secure storage, audit, authentication and authorization. The tokenization system provides data processing applications with the authority and interfaces to request tokens, or detokenize back to sensitive data. \nThe security and risk reduction benefits of tokenization require that the tokenization system is logically isolated and segmented from data processing systems and applications that previously processed or stored sensitive data replaced by tokens. Only the tokenization system can tokenize data to create tokens, or detokenize back to redeem sensitive data under strict security controls. The token generation method must be proven to have the property that there is no feasible means through direct attack, cryptanalysis, side channel analysis, token mapping table exposure or brute force techniques to reverse tokens back to live data.\nReplacing live data with tokens in systems is intended to minimize exposure of sensitive data to those applications, stores, people and processes, reducing risk of compromise or accidental exposure and unauthorized access to sensitive data. Applications can operate using tokens instead of live data, with the exception of a small number of trusted applications explicitly permitted to detokenize when strictly necessary for an approved business purpose. Tokenization systems may be operated in-house within a secure isolated segment of the data center, or as a service from a secure service provider.\nTokenization may be used to safeguard sensitive data involving, for example, bank accounts, financial statements, medical records, criminal records, driver's licenses, loan applications, stock trades, voter registrations, and other types of personally identifiable information (PII). Tokenization is often used in credit card processing. The PCI Council defines tokenization as \"a process by which the primary account number (PAN) is replaced with a surrogate value called a token. A PAN may be linked to a reference number through the tokenization process. In this case, the merchant simply has to retain the token and a reliable third party controls the relationship and holds the PAN. The token may be created independently of the PAN, or the PAN can be used as part of the data input to the tokenization technique. The communication between the merchant and the third-party supplier must be secure to prevent an attacker from intercepting to gain the PAN and the token.\nDe-tokenization is the reverse process of redeeming a token for its associated PAN value. The security of an individual token relies predominantly on the infeasibility of determining the original PAN knowing only the surrogate value\". The choice of tokenization as an alternative to other techniques such as encryption will depend on varying regulatory requirements, interpretation, and acceptance by respective auditing or assessment entities. This is in addition to any technical, architectural or operational constraint that tokenization imposes in practical use.\n\n"
        },
        {
            "id": "c_a5_1",
            "title": "Rainbow table",
            "text": "A rainbow table is a precomputed table for caching the outputs of a cryptographic hash function, usually for cracking password hashes. Passwords are typically stored not in plain text form, but as hash values. If such a database of hashed passwords falls into the hands of attackers, they can use a precomputed rainbow table to recover the plaintext passwords. A common defense against this attack is to compute the hashes using a key derivation function that adds a \"salt\" to each password before hashing it, with different passwords receiving different salts, which are stored in plain text along with the hash.\nRainbow tables are a practical example of a space\u2013time tradeoff: they use less computer processing time and more storage than a brute-force attack which calculates a hash on every attempt, but more processing time and less storage than a simple table that stores the hash of every possible password. Rainbow tables were invented by Philippe Oechslin as an application of an earlier, simpler algorithm by Martin Hellman. == Background ==\nFor user authentication, passwords are stored either as plaintext or hashes. Since passwords stored as plaintext are easily stolen if database access is compromised, databases typically store hashes instead. Thus, no one \u2013 including the authentication system \u2013 can learn a password merely by looking at the value stored in the database.\nWhen a user enters a password for authentication, a hash is computed for it and then compared to the stored hash for that user. Authentication fails if the two hashes do not match; moreover, authentication would equally fail if a hashed value were entered as a password, since the authentication system would hash it a second time.\nTo learn a password from a hash is to find a string which, when input into the hash function, creates that same hash. This is the same as inverting the hash function. Though brute-force attacks (e.g. dictionary attacks) may be used to try to invert a hash function, they can become infeasible when the set of possible passwords is large enough. An alternative to brute-force is to use precomputed hash chain tables. Rainbow tables are a special kind of such table that overcome certain technical difficulties. == Etymology ==\n\nThe term rainbow tables was first used in Oechslin's initial paper. The term refers to the way different reduction functions are used to increase the success rate of the attack. The original method by Hellman uses many small tables with a different reduction function each. Rainbow tables are much bigger and use a different reduction function in each column. When colors are used to represent the reduction functions, a rainbow appears in the rainbow table. \nFigure 2 of Oechslin's paper contains a black-and-white graphic that illustrates how these sections are related. For his presentation at the Crypto 2003 conference, Oechslin added color to the graphic in order to make the rainbow association more clear. The enhanced graphic that was presented at the conference is shown in the illustration.\n\n\n== Precomputed hash chains == Given a password hash function H and a finite set of passwords P, the goal is to precompute a data structure that, given any output h of the hash function, can either locate an element p in P such that H(p) = h, or determine that there is no such p in P. The simplest way to do this is compute H(p) for all p in P, but then storing the table requires \u0398(|P|n) bits of space, where |P| is the size of the set P and n is the size of an output of H, which is prohibitive for large |P|. Hash chains are a technique for decreasing this space requirement. The idea is to define a reduction function R that maps hash values back into values in P. Note, however, that the reduction function is not actually an inverse of the hash function, but rather a different function with a swapped domain and codomain of the hash function. By alternating the hash function with the reduction function, chains of alternating passwords",
            "link": "https://en.wikipedia.org/wiki/Rainbow_table",
            "snippet": "A rainbow table is a precomputed table for caching the outputs of a cryptographic hash function, usually for cracking password hashes. Passwords are typically stored not in plain text form, but as hash values. If such a database of hashed passwords falls into the hands of attackers, they can use a precomputed rainbow table to recover the plaintext passwords. A common defense against this attack is to compute the hashes using a key derivation function that adds a \"salt\" to each password before hashing it, with different passwords receiving different salts, which are stored in plain text along with the hash.\nRainbow tables are a practical example of a space\u2013time tradeoff: they use less computer processing time and more storage than a brute-force attack which calculates a hash on every attempt, but more processing time and less storage than a simple table that stores the hash of every possible password.\nRainbow tables were invented by Philippe Oechslin as an application of an earlier, simpler algorithm by Martin Hellman."
        },
        {
            "id": "c_a5_2",
            "title": "Private biometrics",
            "text": "Private biometrics is a form of encrypted biometrics, also called privacy-preserving biometric authentication methods, in which the biometric payload is a one-way, homomorphically encrypted feature vector that is 0.05% the size of the original biometric template and can be searched with full accuracy, speed and privacy. The feature vector's homomorphic encryption allows search and match to be conducted in polynomial time on an encrypted dataset and the search result is returned as an encrypted match. One or more computing devices may use an encrypted feature vector to verify an individual person (1:1 verify) or identify an individual in a datastore (1:many identify) without storing, sending or receiving plaintext biometric data within or between computing devices or any other entity .  The purpose of private biometrics is to allow a person to be identified or authenticated while guaranteeing individual privacy and fundamental human rights by only operating on biometric data in the encrypted space. Some private biometrics including fingerprint authentication methods, face authentication methods, and identity-matching algorithms according to bodily features. Private biometrics are constantly evolving based on the changing nature of privacy needs, identity theft, and biotechnology. == Background ==\nBiometric security strengthens user authentication but, until recently, also implied important risks to personal privacy.  Indeed, while compromised passwords can be easily replaced and are not personally identifiable information(PII), biometric data is considered highly sensitive due to its personal nature, unique association with users, and the fact that compromised biometrics (biometric templates) cannot be revoked or replaced. Private biometrics have been developed to address this challenge. Private Biometrics provide the necessary biometric authentication while simultaneously minimizing user's privacy exposure through the use of one-way, fully homomorphic encryption. The Biometric Open Protocol Standard, IEEE 2410-2018, was updated in 2018 to include private biometrics and stated that the one-way fully homomorphic encrypted feature vectors, \u201c...bring a new level of consumer privacy assurance by keeping biometric data encrypted both at rest and in transit.\u201d  The Biometric Open Protocol Standard (BOPS III) also noted a key benefit of private biometrics was the new standard allowed for simplification of the API since the biometric payload was always one-way encrypted and therefore had no need for key management. == Fully homomorphic cryptosystems for biometrics ==\nHistorically, biometric matching techniques have been unable to operate in the encrypted space and have required the biometric to be visible (unencrypted) at specific points during search and match operations. This decrypt requirement made large-scale search across encrypted biometrics (\u201c1:many identify\u201d) infeasible due to both significant overhead issues (e.g. complex key management and significant data storage and processing requirements) and the substantial risk that the biometrics were vulnerable to loss when processed in plaintext within the application or operating system (see FIDO Alliance, for example).\nBiometric security vendors complying with data privacy laws and regulations (including Apple FaceID, Samsung, Google) therefore focused their efforts on the simpler 1:1 verify problem and were unable to overcome the large computational demands required for linear scan to solve the 1:many identify problem. Today, private biometric cryptosystems overcome these limitations and risks through the use of one-way, fully homomorphic encryption. This form of encryption allows computations to be carried out on ciphertext, allows the match to be conducted on an encrypted dataset without decrypting the reference biometric, and returns an encrypted match result. Matching in the encrypted space offers the highest levels of accuracy, speed and privacy and eliminates the risks",
            "link": "https://en.wikipedia.org/wiki/Private_biometrics",
            "snippet": "Private biometrics is a form of encrypted biometrics, also called privacy-preserving biometric authentication methods, in which the biometric payload is a one-way, homomorphically encrypted feature vector that is 0.05% the size of the original biometric template and can be searched with full accuracy, speed and privacy. The feature vector's homomorphic encryption allows search and match to be conducted in polynomial time on an encrypted dataset and the search result is returned as an encrypted match. One or more computing devices may use an encrypted feature vector to verify an individual person (1:1 verify) or identify an individual in a datastore (1:many identify) without storing, sending or receiving plaintext biometric data within or between computing devices or any other entity.  The purpose of private biometrics is to allow a person to be identified or authenticated while guaranteeing individual privacy and fundamental human rights by only operating on biometric data in the encrypted space. Some private biometrics including fingerprint authentication methods, face authentication methods, and identity-matching algorithms according to bodily features. Private biometrics are constantly evolving based on the changing nature of privacy needs, identity theft, and biotechnology.   \n\n"
        },
        {
            "id": "c_a6_0",
            "title": "Hash function",
            "text": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing. Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527 Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords. == Overview ==\nIn a hash table, a hash function takes a key as an input, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed-length, like an integer, or variable-length, like a name.  In some cases, the key is the datum itself.  The output is a hash code used to index a hash table holding the data or records, or pointers to them.\nA hash function may be considered to perform three functions: Convert variable-length keys into fixed-length (usually machine-word-length or less) values, by folding them by words or other units using a parity-preserving operator like ADD or XOR,\nScramble the bits of the key so that the resulting values are uniformly distributed over the keyspace, and\nMap the key values into ones less than or equal to the size of the table. A good hash function satisfies two basic properties: it should be very fast to compute, and it should minimize duplication of output values (collisions).  Hash functions rely on generating favorable probability distributions for their effectiveness, reducing access time to nearly constant.  High table loading factors, pathological key sets, and poorly designed hash functions can result in access times approaching linear in the number of items in the table. Hash functions can be designed to give the best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists, or systematic probing of the table to find an empty slot. == Hash tables ==\n\nHash functions are used in conjunction with hash tables to store and retrieve data items or data records. The hash function translates the key associated with each datum or record into a hash code, which is used to index the hash table. When an item is to be added to the table, the hash code may index an empty slot (also called a bucket), in which case the item is added t",
            "link": "https://en.wikipedia.org/wiki/Hash_function",
            "snippet": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing.\nHash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527\u200a\nHash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords."
        },
        {
            "id": "c_a6_1",
            "title": "Cryptographic hash function",
            "text": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application: the probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox). Cryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions. == Properties ==\nMost cryptographic hash functions are designed to take a string of any length as input and produce a fixed-length hash value.\nA cryptographic hash function must be able to withstand all known types of cryptanalytic attack. In theoretical cryptography, the security level of a cryptographic hash function has been defined using the following properties: Pre-image resistance\nGiven a hash value h, it should be difficult to find any message m such that h = hash(m). This concept is related to that of a one-way function. Functions that lack this property are vulnerable to preimage attacks.\nSecond pre-image resistance\nGiven an input m1, it should be difficult to find a different input m2 such that hash(m1) = hash(m2). This property is sometimes referred to as weak collision resistance. Functions that lack this property are vulnerable to second-preimage attacks.\nCollision resistance\nIt should be difficult to find two different messages m1 and m2 such that hash(m1) = hash(m2). Such a pair is called a cryptographic hash collision. This property is sometimes referred to as strong collision resistance. It requires",
            "link": "https://en.wikipedia.org/wiki/Cryptographic_hash_function",
            "snippet": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application:\n\nthe probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox).\nCryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions.\n\n"
        },
        {
            "id": "c_a6_2",
            "title": "MD2 (hash function)",
            "text": "The MD2 Message-Digest Algorithm is a cryptographic hash function developed by Ronald Rivest in 1989. The algorithm is optimized for 8-bit computers. MD2 is specified in  IETF RFC 1319. The \"MD\" in MD2 stands for \"Message Digest\".\nEven though MD2 is not yet fully compromised, the IETF retired MD2 to \"historic\" status in 2011, citing \"signs of weakness\". It is deprecated in favor of SHA-256 and other strong hashing algorithms.\nNevertheless, as of 2014, it remained in use in public key infrastructures as part of certificates generated with MD2 and RSA. == Description ==\nThe 128-bit hash value of any message is formed by padding it to a multiple of the block length (128 bits or 16 bytes) and adding a 16-byte checksum to it. For the actual calculation, a 48-byte auxiliary block and a 256-byte S-table are used. The constants were generated by shuffling the integers 0 through 255 using a variant of Durstenfeld's algorithm with a pseudorandom number generator based on decimal digits of \u03c0 (pi) (see nothing up my sleeve number).  The algorithm runs through a loop where it permutes each byte in the auxiliary block 18 times for every 16 input bytes processed.  Once all of the blocks of the (lengthened) message have been processed, the first partial block of the auxiliary block becomes the hash value of the message.\nThe S-table values in hex are: { 0x29, 0x2E, 0x43, 0xC9, 0xA2, 0xD8, 0x7C, 0x01, 0x3D, 0x36, 0x54, 0xA1, 0xEC, 0xF0, 0x06, 0x13, \n  0x62, 0xA7, 0x05, 0xF3, 0xC0, 0xC7, 0x73, 0x8C, 0x98, 0x93, 0x2B, 0xD9, 0xBC, 0x4C, 0x82, 0xCA, \n  0x1E, 0x9B, 0x57, 0x3C, 0xFD, 0xD4, 0xE0, 0x16, 0x67, 0x42, 0x6F, 0x18, 0x8A, 0x17, 0xE5, 0x12, \n  0xBE, 0x4E, 0xC4, 0xD6, 0xDA, 0x9E, 0xDE, 0x49, 0xA0, 0xFB, 0xF5, 0x8E, 0xBB, 0x2F, 0xEE, 0x7A, \n  0xA9, 0x68, 0x79, 0x91, 0x15, 0xB2, 0x07, 0x3F, 0x94, 0xC2, 0x10, 0x89, 0x0B, 0x22, 0x5F, 0x21,\n  0x80, 0x7F, 0x5D, 0x9A, 0x5A, 0x90, 0x32, 0x27, 0x35, 0x3E, 0xCC, 0xE7, 0xBF, 0xF7, 0x97, 0x03, \n  0xFF, 0x19, 0x30, 0xB3, 0x48, 0xA5, 0xB5, 0xD1, 0xD7, 0x5E, 0x92, 0x2A, 0xAC, 0x56, 0xAA, 0xC6, \n  0x4F, 0xB8, 0x38, 0xD2, 0x96, 0xA4, 0x7D, 0xB6, 0x76, 0xFC, 0x6B, 0xE2, 0x9C, 0x74, 0x04, 0xF1, \n  0x45, 0x9D, 0x70, 0x59, 0x64, 0x71, 0x87, 0x20, 0x86, 0x5B, 0xCF, 0x65, 0xE6, 0x2D, 0xA8, 0x02, \n  0x1B, 0x60, 0x25, 0xAD, 0xAE, 0xB0, 0xB9, 0xF6, 0x1C, 0x46, 0x61, 0x69, 0x34, 0x40, 0x7E, 0x0F, 0x55, 0x47, 0xA3, 0x23, 0xDD, 0x51, 0xAF, 0x3A, 0xC3, 0x5C, 0xF9, 0xCE, 0xBA, 0xC5, 0xEA, 0x26, \n  0x2C, 0x53, 0x0D, 0x6E, 0x85, 0x28, 0x84, 0x09, 0xD3, 0xDF, 0xCD, 0xF4, 0x41, 0x81, 0x4D, 0x52, \n  0x6A, 0xDC, 0x37, 0xC8, 0x6C, 0xC1, 0xAB, 0xFA, 0x24, 0xE1, 0x7B, 0x08, 0x0C, 0xBD, 0xB1, 0x4A, \n  0x78, 0x88, 0x95, 0x8B, 0xE3, 0x63, 0xE8, 0x6D, 0xE9, 0xCB, 0xD5, 0xFE, 0x3B, 0x00, 0x1D, 0x39, \n  0xF2, 0xEF, 0xB7, 0x0E, 0x66, 0x58, 0xD0, 0xE4, 0xA6, 0x77, 0x72, 0xF8, 0xEB, 0x75, 0x4B, 0x0A, \n  0x31, 0x44, 0x50, 0xB4, 0x8F, 0xED, 0x1F, 0x1A, 0xDB, 0x99, 0x8D, 0x33, 0x9F, 0x11, 0x83, 0x14 } == MD2 hashes ==\nThe 128-bit (16-byte) MD2 hashes (also termed message digests) are typically represented as 32-digit hexadecimal numbers. The following demonstrates a 43-byte ASCII input and the corresponding MD2 hash:\n\n MD2(\"The quick brown fox jumps over the lazy dog\") = \n 03d85a0d629d2c442e987525319fc471\n\nAs the result of the avalanche effect in MD2, even a small change in the input message will (with overwhelming probability) result in a completely different hash. For example, changing the letter d to c in the message results in:\n\n MD2(\"The quick brown fox jumps over the lazy cog\") = \n 6b890c9292668cdbbfda00a4ebf31f05\n\nThe hash of the zero-length string is:\n\n MD2(\"\") = \n 8350e5a3e24c153df2275c9f80692773 == Security ==\nRogier and Chauvaud presented in 1995 collisions of MD2's compression function, although they were unable to extend the attack to the full MD2. The described collisions was published in 1997.\nIn 2004, MD2 was shown to be vulnerable to a preimage attack with time complexity equivalent to 2104 applications of the compres",
            "link": "https://en.wikipedia.org/wiki/MD2_(hash_function)",
            "snippet": "The MD2 Message-Digest Algorithm is a cryptographic hash function developed by Ronald Rivest in 1989. The algorithm is optimized for 8-bit computers. MD2 is specified in  IETF RFC 1319. The \"MD\" in MD2 stands for \"Message Digest\".\nEven though MD2 is not yet fully compromised, the IETF retired MD2 to \"historic\" status in 2011, citing \"signs of weakness\". It is deprecated in favor of SHA-256 and other strong hashing algorithms.\nNevertheless, as of 2014, it remained in use in public key infrastructures as part of certificates generated with MD2 and RSA.\n\n"
        },
        {
            "id": "c_a7_0",
            "title": "Cryptographic hash function",
            "text": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application: the probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox). Cryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions. == Properties ==\nMost cryptographic hash functions are designed to take a string of any length as input and produce a fixed-length hash value.\nA cryptographic hash function must be able to withstand all known types of cryptanalytic attack. In theoretical cryptography, the security level of a cryptographic hash function has been defined using the following properties: Pre-image resistance\nGiven a hash value h, it should be difficult to find any message m such that h = hash(m). This concept is related to that of a one-way function. Functions that lack this property are vulnerable to preimage attacks.\nSecond pre-image resistance\nGiven an input m1, it should be difficult to find a different input m2 such that hash(m1) = hash(m2). This property is sometimes referred to as weak collision resistance. Functions that lack this property are vulnerable to second-preimage attacks.\nCollision resistance\nIt should be difficult to find two different messages m1 and m2 such that hash(m1) = hash(m2). Such a pair is called a cryptographic hash collision. This property is sometimes referred to as strong collision resistance. It requires",
            "link": "https://en.wikipedia.org/wiki/Cryptographic_hash_function",
            "snippet": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application:\n\nthe probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox).\nCryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions.\n\n"
        },
        {
            "id": "c_a7_1",
            "title": "HMAC",
            "text": "In cryptography, an HMAC (sometimes expanded as either keyed-hash message authentication code or hash-based message authentication code) is a specific type of message authentication code (MAC) involving a cryptographic hash function and a secret cryptographic key. As with any MAC, it may be used to simultaneously verify both the data integrity and authenticity of a message. An HMAC is a type of keyed hash function that can also be used in a key derivation scheme or a key stretching scheme.\nHMAC can provide authentication using a shared secret instead of using digital signatures with asymmetric cryptography. It trades off the need for a complex public key infrastructure by delegating the key exchange to the communicating parties, who are responsible for establishing and using a trusted channel to agree on the key prior to communication. == Details ==\nAny cryptographic hash function, such as SHA-2 or SHA-3, may be used in the calculation of an HMAC; the resulting MAC algorithm is termed HMAC-x, where x is the hash function used (e.g. HMAC-SHA256 or HMAC-SHA3-512). The cryptographic strength of the HMAC depends upon the cryptographic strength of the underlying hash function, the size of its hash output, and the size and quality of the key.\nHMAC uses two passes of hash computation. Before either pass, the secret key is used to derive two keys \u2013 inner and outer. Next, the first pass of the hash algorithm produces an internal hash derived from the message and the inner key. The second pass produces the final HMAC code derived from the inner hash result and the outer key. Thus the algorithm provides better immunity against length extension attacks. An iterative hash function (one that uses the Merkle\u2013Damg\u00e5rd construction) breaks up a message into blocks of a fixed size and iterates over them with a compression function. For example, SHA-256 operates on 512-bit blocks. The size of the output of HMAC is the same as that of the underlying hash function (e.g., 256 and 512 bits in the case of SHA-256 and SHA3-512, respectively), although it can be truncated if desired.\nHMAC does not encrypt the message. Instead, the message (encrypted or not) must be sent alongside the HMAC hash. Parties with the secret key will hash the message again themselves, and if it is authentic, the received and computed hashes will match. The definition and analysis of the HMAC construction was first published in 1996 in a paper by Mihir Bellare, Ran Canetti, and Hugo Krawczyk, and they also wrote RFC 2104 in 1997.:\u200a\u00a72\u200a The 1996 paper also defined a nested variant called NMAC (Nested MAC). FIPS PUB 198 generalizes and standardizes the use of HMACs. HMAC is used within the IPsec, SSH and TLS protocols and for JSON Web Tokens. == Definition ==\nThis definition is taken from RFC 2104: HMAC\n                \u2061\n                (\n                K\n                ,\n                m\n                )\n              \n              \n                \n                =\n                H\n                \u2061\n                \n                  \n                    (\n                  \n                \n                \n                  \n                    (\n                  \n                \n                \n                  K\n                  \u2032\n                \n                \u2295\n                o\n                p\n                a\n                d\n                \n                  \n                    )\n                  \n                \n                \u2225\n                H\n                \u2061\n                \n                  \n                    (\n                  \n                \n                \n                  (\n                  \n                    \n                      K \u2032\n                    \n                    \u2295\n                    i\n                    p\n                    a\n                    d\n                  \n                  )",
            "link": "https://en.wikipedia.org/wiki/HMAC",
            "snippet": "In cryptography, an HMAC (sometimes expanded as either keyed-hash message authentication code or hash-based message authentication code) is a specific type of message authentication code (MAC) involving a cryptographic hash function and a secret cryptographic key. As with any MAC, it may be used to simultaneously verify both the data integrity and authenticity of a message. An HMAC is a type of keyed hash function that can also be used in a key derivation scheme or a key stretching scheme.\nHMAC can provide authentication using a shared secret instead of using digital signatures with asymmetric cryptography. It trades off the need for a complex public key infrastructure by delegating the key exchange to the communicating parties, who are responsible for establishing and using a trusted channel to agree on the key prior to communication.\n\n"
        },
        {
            "id": "c_a7_2",
            "title": "Salt (cryptography)",
            "text": "In cryptography, a salt is random data fed as an additional input to a one-way function that hashes data, a password or passphrase. Salting helps defend against attacks that use precomputed tables (e.g. rainbow tables), by vastly growing the size of table needed for a successful attack. It also helps protect passwords that occur multiple times in a database, as a new salt is used for each password instance. Additionally, salting does not place any burden on users. \nTypically, a unique salt is randomly generated for each password. The salt and the password (or its version after key stretching) are concatenated and fed to a cryptographic hash function, and the output hash value is then stored with the salt in a database. The salt does not need to be encrypted, because knowing the salt would not help the attacker.\nSalting is broadly used in cybersecurity, from Unix system credentials to Internet security.\nSalts are related to cryptographic nonces. == Example ==\nWithout a salt, identical passwords will map to identical hash values, which could make it easier for a hacker to guess the passwords from their hash value.\n\nInstead, a salt is generated and appended to each password, which causes the resultant hash to output different values for the same original password. The salt and hash are then stored in the database. To later test if a password a user enters is correct, the same process can be performed on it (appending that user's salt to the password and calculating the resultant hash): if the result does not match the stored hash, it could not have been the correct password that was entered.\nIn practice, a salt is usually generated using a Cryptographically Secure PseudoRandom Number Generator. CSPRNGs are designed to produce unpredictable random numbers which can be alphanumeric. While generally discouraged due to lower security, some systems use timestamps or simple counters as a source of salt. Sometimes, a salt may be generated by combining a random value with additional information, such as a timestamp or user-specific data, to ensure uniqueness across different systems or time periods.\n\n\n== Common mistakes == === Salt re-use ===\nUsing the same salt for all passwords is dangerous because a precomputed table which simply accounts for the salt will render the salt useless. \nGeneration of precomputed tables for databases with unique salts for every password is not viable because of the computational cost of doing so. But, if a common salt is used for all the entries, creating such a table (that accounts for the salt) then becomes a viable and possibly successful attack.\nBecause salt re-use can cause users with the same password to have the same hash, cracking a single hash can result in other passwords being compromised too. === Salt length ===\nIf a salt is too short, an attacker may precompute a table of every possible salt appended to every likely password. Using a long salt ensures such a table would be prohibitively large.  16 bytes (128 bits) or more is generally sufficient to provide a large enough space of possible values, minimizing the risk of collisions (i.e., two different passwords ending up with the same salt). == Benefits ==\nTo understand the difference between cracking a single password and a set of them, consider a file with users and their hashed passwords. Say the file is unsalted. Then an attacker could pick a string, call it attempt[0], and then compute hash(attempt[0]). A user whose hash stored in the file is hash(attempt[0]) may or may not have password attempt[0]. However, even if attempt[0] is not the user's actual password, it will be accepted as if it were, because the system can only check passwords by computing the hash of the password entered and comparing it to the hash stored in the file.  Thus, each match cracks a user password, and the chance of a match rises with the number of passwords in the file. In contrast, if salts are used, the attacker would have to compute hash(attempt[0] || s",
            "link": "https://en.wikipedia.org/wiki/Salt_(cryptography)",
            "snippet": "In cryptography, a salt is random data fed as an additional input to a one-way function that hashes data, a password or passphrase. Salting helps defend against attacks that use precomputed tables (e.g. rainbow tables), by vastly growing the size of table needed for a successful attack. It also helps protect passwords that occur multiple times in a database, as a new salt is used for each password instance. Additionally, salting does not place any burden on users. \nTypically, a unique salt is randomly generated for each password. The salt and the password (or its version after key stretching) are concatenated and fed to a cryptographic hash function, and the output hash value is then stored with the salt in a database. The salt does not need to be encrypted, because knowing the salt would not help the attacker.\nSalting is broadly used in cybersecurity, from Unix system credentials to Internet security.\nSalts are related to cryptographic nonces.\n\n"
        },
        {
            "id": "c_a8_0",
            "title": "Salt (cryptography)",
            "text": "In cryptography, a salt is random data fed as an additional input to a one-way function that hashes data, a password or passphrase. Salting helps defend against attacks that use precomputed tables (e.g. rainbow tables), by vastly growing the size of table needed for a successful attack. It also helps protect passwords that occur multiple times in a database, as a new salt is used for each password instance. Additionally, salting does not place any burden on users. \nTypically, a unique salt is randomly generated for each password. The salt and the password (or its version after key stretching) are concatenated and fed to a cryptographic hash function, and the output hash value is then stored with the salt in a database. The salt does not need to be encrypted, because knowing the salt would not help the attacker.\nSalting is broadly used in cybersecurity, from Unix system credentials to Internet security.\nSalts are related to cryptographic nonces. == Example ==\nWithout a salt, identical passwords will map to identical hash values, which could make it easier for a hacker to guess the passwords from their hash value.\n\nInstead, a salt is generated and appended to each password, which causes the resultant hash to output different values for the same original password. The salt and hash are then stored in the database. To later test if a password a user enters is correct, the same process can be performed on it (appending that user's salt to the password and calculating the resultant hash): if the result does not match the stored hash, it could not have been the correct password that was entered.\nIn practice, a salt is usually generated using a Cryptographically Secure PseudoRandom Number Generator. CSPRNGs are designed to produce unpredictable random numbers which can be alphanumeric. While generally discouraged due to lower security, some systems use timestamps or simple counters as a source of salt. Sometimes, a salt may be generated by combining a random value with additional information, such as a timestamp or user-specific data, to ensure uniqueness across different systems or time periods.\n\n\n== Common mistakes == === Salt re-use ===\nUsing the same salt for all passwords is dangerous because a precomputed table which simply accounts for the salt will render the salt useless. \nGeneration of precomputed tables for databases with unique salts for every password is not viable because of the computational cost of doing so. But, if a common salt is used for all the entries, creating such a table (that accounts for the salt) then becomes a viable and possibly successful attack.\nBecause salt re-use can cause users with the same password to have the same hash, cracking a single hash can result in other passwords being compromised too. === Salt length ===\nIf a salt is too short, an attacker may precompute a table of every possible salt appended to every likely password. Using a long salt ensures such a table would be prohibitively large.  16 bytes (128 bits) or more is generally sufficient to provide a large enough space of possible values, minimizing the risk of collisions (i.e., two different passwords ending up with the same salt). == Benefits ==\nTo understand the difference between cracking a single password and a set of them, consider a file with users and their hashed passwords. Say the file is unsalted. Then an attacker could pick a string, call it attempt[0], and then compute hash(attempt[0]). A user whose hash stored in the file is hash(attempt[0]) may or may not have password attempt[0]. However, even if attempt[0] is not the user's actual password, it will be accepted as if it were, because the system can only check passwords by computing the hash of the password entered and comparing it to the hash stored in the file.  Thus, each match cracks a user password, and the chance of a match rises with the number of passwords in the file. In contrast, if salts are used, the attacker would have to compute hash(attempt[0] || s",
            "link": "https://en.wikipedia.org/wiki/Salt_(cryptography)",
            "snippet": "In cryptography, a salt is random data fed as an additional input to a one-way function that hashes data, a password or passphrase. Salting helps defend against attacks that use precomputed tables (e.g. rainbow tables), by vastly growing the size of table needed for a successful attack. It also helps protect passwords that occur multiple times in a database, as a new salt is used for each password instance. Additionally, salting does not place any burden on users. \nTypically, a unique salt is randomly generated for each password. The salt and the password (or its version after key stretching) are concatenated and fed to a cryptographic hash function, and the output hash value is then stored with the salt in a database. The salt does not need to be encrypted, because knowing the salt would not help the attacker.\nSalting is broadly used in cybersecurity, from Unix system credentials to Internet security.\nSalts are related to cryptographic nonces.\n\n"
        },
        {
            "id": "c_a8_1",
            "title": "Tokenization (data security)",
            "text": "Tokenization, when applied to data security, is the process of substituting a sensitive data element with a non-sensitive equivalent, referred to as a token, that has no intrinsic or exploitable meaning or value. The token is a reference (i.e. identifier) that maps back to the sensitive data through a tokenization system. The mapping from original data to a token uses methods that render tokens infeasible to reverse in the absence of the tokenization system, for example using tokens created from random numbers. A one-way cryptographic function is used to convert the original data into tokens, making it difficult to recreate the original data without obtaining entry to the tokenization system's resources. To deliver such services, the system maintains a vault database of tokens that are connected to the corresponding sensitive data. Protecting the system vault is vital to the system, and improved processes must be put in place to offer database integrity and physical security. The tokenization system must be secured and validated using security best practices applicable to sensitive data protection,  secure storage, audit, authentication and authorization. The tokenization system provides data processing applications with the authority and interfaces to request tokens, or detokenize back to sensitive data. \nThe security and risk reduction benefits of tokenization require that the tokenization system is logically isolated and segmented from data processing systems and applications that previously processed or stored sensitive data replaced by tokens. Only the tokenization system can tokenize data to create tokens, or detokenize back to redeem sensitive data under strict security controls. The token generation method must be proven to have the property that there is no feasible means through direct attack, cryptanalysis, side channel analysis, token mapping table exposure or brute force techniques to reverse tokens back to live data. Replacing live data with tokens in systems is intended to minimize exposure of sensitive data to those applications, stores, people and processes, reducing risk of compromise or accidental exposure and unauthorized access to sensitive data. Applications can operate using tokens instead of live data, with the exception of a small number of trusted applications explicitly permitted to detokenize when strictly necessary for an approved business purpose. Tokenization systems may be operated in-house within a secure isolated segment of the data center, or as a service from a secure service provider. Tokenization may be used to safeguard sensitive data involving, for example, bank accounts, financial statements, medical records, criminal records, driver's licenses, loan applications, stock trades, voter registrations, and other types of personally identifiable information (PII). Tokenization is often used in credit card processing. The PCI Council defines tokenization as \"a process by which the primary account number (PAN) is replaced with a surrogate value called a token. A PAN may be linked to a reference number through the tokenization process. In this case, the merchant simply has to retain the token and a reliable third party controls the relationship and holds the PAN. The token may be created independently of the PAN, or the PAN can be used as part of the data input to the tokenization technique. The communication between the merchant and the third-party supplier must be secure to prevent an attacker from intercepting to gain the PAN and the token. De-tokenization is the reverse process of redeeming a token for its associated PAN value. The security of an individual token relies predominantly on the infeasibility of determining the original PAN knowing only the surrogate value\". The choice of tokenization as an alternative to other techniques such as encryption will depend on varying regulatory requirements, interpretation, and acceptance by respective auditing or assessment entities. This is in add",
            "link": "https://en.wikipedia.org/wiki/Tokenization_(data_security)",
            "snippet": "Tokenization, when applied to data security, is the process of substituting a sensitive data element with a non-sensitive equivalent, referred to as a token, that has no intrinsic or exploitable meaning or value. The token is a reference (i.e. identifier) that maps back to the sensitive data through a tokenization system. The mapping from original data to a token uses methods that render tokens infeasible to reverse in the absence of the tokenization system, for example using tokens created from random numbers. A one-way cryptographic function is used to convert the original data into tokens, making it difficult to recreate the original data without obtaining entry to the tokenization system's resources. To deliver such services, the system maintains a vault database of tokens that are connected to the corresponding sensitive data. Protecting the system vault is vital to the system, and improved processes must be put in place to offer database integrity and physical security.\nThe tokenization system must be secured and validated using security best practices applicable to sensitive data protection,  secure storage, audit, authentication and authorization. The tokenization system provides data processing applications with the authority and interfaces to request tokens, or detokenize back to sensitive data. \nThe security and risk reduction benefits of tokenization require that the tokenization system is logically isolated and segmented from data processing systems and applications that previously processed or stored sensitive data replaced by tokens. Only the tokenization system can tokenize data to create tokens, or detokenize back to redeem sensitive data under strict security controls. The token generation method must be proven to have the property that there is no feasible means through direct attack, cryptanalysis, side channel analysis, token mapping table exposure or brute force techniques to reverse tokens back to live data.\nReplacing live data with tokens in systems is intended to minimize exposure of sensitive data to those applications, stores, people and processes, reducing risk of compromise or accidental exposure and unauthorized access to sensitive data. Applications can operate using tokens instead of live data, with the exception of a small number of trusted applications explicitly permitted to detokenize when strictly necessary for an approved business purpose. Tokenization systems may be operated in-house within a secure isolated segment of the data center, or as a service from a secure service provider.\nTokenization may be used to safeguard sensitive data involving, for example, bank accounts, financial statements, medical records, criminal records, driver's licenses, loan applications, stock trades, voter registrations, and other types of personally identifiable information (PII). Tokenization is often used in credit card processing. The PCI Council defines tokenization as \"a process by which the primary account number (PAN) is replaced with a surrogate value called a token. A PAN may be linked to a reference number through the tokenization process. In this case, the merchant simply has to retain the token and a reliable third party controls the relationship and holds the PAN. The token may be created independently of the PAN, or the PAN can be used as part of the data input to the tokenization technique. The communication between the merchant and the third-party supplier must be secure to prevent an attacker from intercepting to gain the PAN and the token.\nDe-tokenization is the reverse process of redeeming a token for its associated PAN value. The security of an individual token relies predominantly on the infeasibility of determining the original PAN knowing only the surrogate value\". The choice of tokenization as an alternative to other techniques such as encryption will depend on varying regulatory requirements, interpretation, and acceptance by respective auditing or assessment entities. This is in addition to any technical, architectural or operational constraint that tokenization imposes in practical use.\n\n"
        },
        {
            "id": "c_a8_2",
            "title": "Error detection and correction",
            "text": "In information theory and coding theory with applications in computer science and telecommunications, error detection and correction (EDAC) or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. Many communication channels are subject to channel noise, and thus errors may be introduced during transmission from the source to a receiver. Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases.\n\n\n== Definitions ==\nError detection is the detection of errors caused by noise or other impairments during transmission from the transmitter to the receiver.\nError correction is the detection of errors and reconstruction of the original, error-free data. == History == In classical antiquity, copyists of the Hebrew Bible were paid for their work according to the number of stichs (lines of verse). As the prose books of the Bible were hardly ever written in stichs, the copyists, in order to estimate the amount of work, had to count the letters. This also helped ensure accuracy in the transmission of the text with the production of subsequent copies. Between the 7th and 10th centuries CE a group of Jewish scribes formalized and expanded this to create the Numerical Masorah to ensure accurate reproduction of the sacred text. It included counts of the number of words in a line, section, book and groups of books, noting the middle stich of a book, word use statistics, and commentary. Standards became such that a deviation in even a single letter in a Torah scroll was considered unacceptable . The effectiveness of their error correction method was verified by the accuracy of copying through the centuries demonstrated by discovery of the Dead Sea Scrolls in 1947\u20131956, dating from c.\u2009150 BCE-75 CE. The modern development of error correction codes is credited to Richard Hamming in 1947. A description of Hamming's code appeared in Claude Shannon's A Mathematical Theory of Communication and was quickly generalized by Marcel J. E. Golay. == Principles == All error-detection and correction schemes add some redundancy (i.e., some extra data) to a message, which receivers can use to check consistency of the delivered message and to recover data that has been determined to be corrupted. Error detection and correction schemes can be either systematic or non-systematic. In a systematic scheme, the transmitter sends the original (error-free) data and attaches a fixed number of check bits (or parity data), which are derived from the data bits by some encoding algorithm. If error detection is required, a receiver can simply apply the same algorithm to the received data bits and compare its output with the received check bits; if the values do not match, an error has occurred at some point during the transmission. If error correction is required, a receiver can apply the decoding algorithm to the received data bits and the received check bits to recover the original error-free data . In a system that uses a non-systematic code, the original message is transformed into an encoded message carrying the same information and that has at least as many bits as the original message. Good error control performance requires the scheme to be selected based on the characteristics of the communication channel. Common channel models include memoryless models where errors occur randomly and with a certain probability, and dynamic models where errors occur primarily in bursts. Consequently, error-detecting and -correcting codes can be generally distinguished between random-error-detecting/correcting and burst-error-detecting/correcting. Some codes can also be suitable for a mixture of random errors and burst errors.\nIf the channel characteristics cannot be determined, or are highly variable, an error-detection scheme may be combined with a system for retransmissions of erroneous data. This is known as automatic repeat request (ARQ), an",
            "link": "https://en.wikipedia.org/wiki/Error_detection_and_correction",
            "snippet": "In information theory and coding theory with applications in computer science and telecommunications, error detection and correction (EDAC) or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. Many communication channels are subject to channel noise, and thus errors may be introduced during transmission from the source to a receiver. Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases.\n\n"
        },
        {
            "id": "c_a9_0",
            "title": "Cryptocurrency",
            "text": "A cryptocurrency, crypto-currency, or crypto is a digital currency designed to work through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it. \nIndividual coin ownership records are stored in a digital ledger or \"blockchain,\" which is a computerized database using strong cryptography to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership. Despite its name, which has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdicitons, including classification as commodities, securities, and currencies. Cryptocurrencies are generally viewed as a distinct asset class in practice. Some crypto schemes use validators to maintain the cryptocurrency. The first cryptocurrency was bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion. == History == In 1983, American cryptographer David Chaum conceived of a type of cryptographic electronic money called ecash. Later, in 1995, he implemented it through Digicash, an early form of cryptographic electronic payments. Digicash required user software in order to withdraw notes from a bank and designate specific encrypted keys before they could be sent to a recipient. This allowed the digital currency to be untraceable by a third party.\nIn 1996, the National Security Agency published a paper entitled How to Make a Mint: The Cryptography of Anonymous Electronic Cash, describing a cryptocurrency system. The paper was first published in an MIT mailing list (October 1996) and later (April 1997) in The American Law Review. In 1998, Wei Dai described \"b-money,\" an anonymous, distributed electronic cash system. Shortly thereafter, Nick Szabo described bit gold. Like bitcoin and other cryptocurrencies that would follow it, bit gold (not to be confused with the later gold-based exchange BitGold) was described as an electronic currency system that required users to complete a proof of work function with solutions being cryptographically put together and published.\nIn January 2009, bitcoin was created by pseudonymous developer Satoshi Nakamoto. It used SHA-256, a cryptographic hash function, in its proof-of-work scheme. In April 2011, Namecoin was created as an attempt at forming a decentralized DNS. In October 2011, Litecoin was released, which used scrypt as its hash function instead of SHA-256. Peercoin, created in August 2012, used a hybrid of proof-of-work and proof-of-stake. Cryptocurrency has undergone several periods of growth and retraction, including several bubbles and market crashes, such as in 2011, 2013\u20132014/15, 2017\u20132018, and 2021\u20132023.\nOn 6 August 2014, the UK announced its Treasury had commissioned a study of cryptocurrencies and what role, if any, they could play in the UK economy. The study was also to report on whether regulation should be considered. Its final report was published in 2018, and it issued a consultation on cryptoassets and stablecoins in January 2021.\nIn June 2021, El Salvador became the first country to accept bitcoin as legal tender, after the Legislative Assembly had voted 62\u201322 to pass a bill submitted by President Nayib Bukele classifying the cryptocurrency as such.\nIn August 2021, Cuba followed with Resolution 215 to recognize and regulate cryptocurrencies such as bitcoin. In September 2021, the government of China, the single largest market for cryptocurrency, declared all cryptocurrency transactions illegal. This completed a crackdown on cryptocurrency that had previously banned the operation of intermediaries and miners within China.\nOn 15 September 2022, the world's second largest cryptocurrency",
            "link": "https://en.wikipedia.org/wiki/Cryptocurrency",
            "snippet": "A cryptocurrency, crypto-currency, or crypto is a digital currency designed to work through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it. \nIndividual coin ownership records are stored in a digital ledger or \"blockchain,\" which is a computerized database using strong cryptography to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership. Despite its name, which has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdicitons, including classification as commodities, securities, and currencies. Cryptocurrencies are generally viewed as a distinct asset class in practice. Some crypto schemes use validators to maintain the cryptocurrency.\nThe first cryptocurrency was bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion."
        },
        {
            "id": "c_a9_1",
            "title": "History of cannabis in Italy",
            "text": "The cultivation of cannabis in Italy has a long history dating back to Roman times, when it was primarily used to produce hemp ropes, although pollen records from core samples show that Cannabaceae plants were present in the Italian peninsula since at least the Late Pleistocene, while the earliest evidence of their use dates back to the Bronze Age. For a long time after the fall of Rome in the 5th century A.D., the cultivation of hemp, although present in several Italian regions, mostly consisted in small-scale productions aimed at satisfying the local needs for fabrics and ropes. Known as canapa in Italian, the historical ubiquity of hemp is reflected in the different variations of the name given to the plant in the various regions, including canape, c\u00e0neva, canava, and canva (or canav\u00f2n for female plants) in northern Italy; canapuccia and canapone in the Po Valley; c\u00e0nnavo in Naples; c\u00e0nnavu in Calabria; cannavusa and c\u00e0nnavu in Sicily; c\u00e0nnau and cagnu in Sardinia. The mass cultivation of industrial cannabis for the production of hemp fiber in Italy really took off during the period of the Maritime Republics and the Age of Sail, due to its strategic importance for the naval industry. In particular, two main economic models were implemented between the 15th and 19th centuries for the cultivation of hemp, and their primary differences essentially derived from the diverse relationships between landowners and hemp producers. The Venetian model was based on a state monopoly system, by which the farmers had to sell the harvested hemp to the Arsenal at an imposed price, in order to ensure preferential, regular, and advantageous supplies of the raw material for the navy, as a matter of national security. Such system was particularly developed in the southern part of the province of Padua, which was under the direct control of the administrators of the Arsenal . Conversely, the Emilian model, which was typical of the provinces of Bologna and Ferrara, was strongly export-oriented and it was based on the mezzadria farming system by which, for instance, Bolognese landowners could relegate most of the production costs and risks to the farmers, while also keeping for themselves the largest share of the profits. From the 18th century onwards, hemp production in Italy established itself as one of the most important industries at an international level, with the most productive areas being located in Emilia-Romagna, Campania, and Piedmont. The well renowned and flourishing Italian hemp sector continued well after the unification of the country in 1861, only to experience a sudden decline during the second half of the 20th century, with the introduction of synthetic fibers and the start of the war on drugs, and only recently it is slowly experiencing a resurgence. == Prehistory == The family of Cannabaceae includes the two genera of cannabis and humulus, with the former believed to be native exclusively to Asia, while the latter also to Europe. In particular, while humulus naturally dispersed from Asia to Europe without human agency, cannabis is commonly thought to have been spread by humans once its multiple uses, most importantly those involving its fiber, had been discovered and developed by the various cultures. At present, cannabis sativa is classified as an archaeophyte alien species for Italy, with sporadic wild occurrences being attributed to escaped plants, which are non-naturalized and distinct from the wild oriental varieties. According to Greek historian and geographer Herodotus, the Scythians brought hemp from Central Asia to Europe during their migrations around 1500 B.C., while the Teutons were a major factor in the spread of the cultivation of hemp throughout Europe. Another proposed theory is that hemp may have been introduced into the continent by the earliest incursions of the Aryans into Thrace and Western Europe, although no evidence of its presence was found in the lake dwellings of Switzerland and northern Italy. Sti",
            "link": "https://en.wikipedia.org/wiki/History_of_cannabis_in_Italy",
            "snippet": "The cultivation of cannabis in Italy has a long history dating back to Roman times, when it was primarily used to produce hemp ropes, although pollen records from core samples show that Cannabaceae plants were present in the Italian peninsula since at least the Late Pleistocene, while the earliest evidence of their use dates back to the Bronze Age. For a long time after the fall of Rome in the 5th century A.D., the cultivation of hemp, although present in several Italian regions, mostly consisted in small-scale productions aimed at satisfying the local needs for fabrics and ropes. Known as canapa in Italian, the historical ubiquity of hemp is reflected in the different variations of the name given to the plant in the various regions, including canape, c\u00e0neva, canava, and canva (or canav\u00f2n for female plants) in northern Italy; canapuccia and canapone in the Po Valley; c\u00e0nnavo in Naples; c\u00e0nnavu in Calabria; cannavusa and c\u00e0nnavu in Sicily; c\u00e0nnau and cagnu in Sardinia.\nThe mass cultivation of industrial cannabis for the production of hemp fiber in Italy really took off during the period of the Maritime Republics and the Age of Sail, due to its strategic importance for the naval industry. In particular, two main economic models were implemented between the 15th and 19th centuries for the cultivation of hemp, and their primary differences essentially derived from the diverse relationships between landowners and hemp producers. The Venetian model was based on a state monopoly system, by which the farmers had to sell the harvested hemp to the Arsenal at an imposed price, in order to ensure preferential, regular, and advantageous supplies of the raw material for the navy, as a matter of national security. Such system was particularly developed in the southern part of the province of Padua, which was under the direct control of the administrators of the Arsenal. Conversely, the Emilian model, which was typical of the provinces of Bologna and Ferrara, was strongly export-oriented and it was based on the mezzadria farming system by which, for instance, Bolognese landowners could relegate most of the production costs and risks to the farmers, while also keeping for themselves the largest share of the profits.\nFrom the 18th century onwards, hemp production in Italy established itself as one of the most important industries at an international level, with the most productive areas being located in Emilia-Romagna, Campania, and Piedmont. The well renowned and flourishing Italian hemp sector continued well after the unification of the country in 1861, only to experience a sudden decline during the second half of the 20th century, with the introduction of synthetic fibers and the start of the war on drugs, and only recently it is slowly experiencing a resurgence.\n\n"
        },
        {
            "id": "c_a9_2",
            "title": "History of bitcoin",
            "text": "Bitcoin is a cryptocurrency, a digital asset that uses cryptography to control its creation and management rather than relying on central authorities. Originally designed as a medium of exchange, Bitcoin is now primarily regarded as a store of value. The history of bitcoin started with its invention and implementation by Satoshi Nakamoto, who integrated many existing ideas from the cryptography community. Over the course of bitcoin's history, it has undergone rapid growth to become a significant store of value both on- and offline. From the mid-2010s, some businesses began accepting bitcoin in addition to traditional currencies.\n\n\n== Background ==\nPrior to the release of bitcoin, there were a number of digital cash technologies, starting with the issuer-based ecash protocols of David Chaum and Stefan Brands. The idea that solutions to computational puzzles could have some value was first proposed by cryptographers Cynthia Dwork and Moni Naor in 1992. === 31 October 1996 NSA paper ===\n12 years prior to creating Bitcoin the NSA published the white paper HOW TO MAKE A MINT: THE CRYPTOGRAPHY OF ANONYMOUS ELECTRONIC CASH \n\n\n=== Adam Back ===\nThe idea was independently rediscovered by Adam Back who developed hashcash, a proof-of-work scheme for spam control in 1997. The first proposals for distributed digital scarcity-based cryptocurrencies were Wei Dai's b-money and Nick Szabo's bit gold. Hal Finney developed reusable proof of work (RPOW) using hashcash as its proof of work algorithm.\nIn the bit gold proposal which proposed a collectible market-based mechanism for inflation control, Nick Szabo also investigated some additional aspects including a Byzantine fault-tolerant agreement protocol based on quorum addresses to store and transfer the chained proof-of-work solutions, which was vulnerable to Sybil attacks, though. == Creation ==\nOn the 18th of August 2008, the domain name bitcoin.org was registered. Later that year, on 31 October, a link to a paper authored by Satoshi Nakamoto titled Bitcoin: A Peer-to-Peer Electronic Cash System was posted to a cryptography mailing list. This paper detailed methods of using a peer-to-peer network to generate what was described as \"a system for electronic transactions without relying on trust\". On 3 January 2009, the bitcoin network came into existence with Satoshi Nakamoto mining the genesis block of bitcoin (block number 0), which had a reward of 50 bitcoins. Embedded in the genesis block was the text: The Times 03/Jan/2009 Chancellor on brink of second bailout for banks\nThe text refers to a headline in The Times published on 3 January 2009. This note has been interpreted as both a timestamp of the genesis date and a derisive comment on the instability caused by fractional-reserve banking.:\u200a18\u200a\nThe first open source bitcoin client was released on 9 January 2009, hosted at SourceForge.\nOne of the first supporters, adopters, contributors to bitcoin and receiver of the first bitcoin transaction was programmer Hal Finney. Finney downloaded the bitcoin software the day it was released, and received 10 bitcoins from Nakamoto in the world's first bitcoin transaction on 12 January 2009 (block 170). Other early supporters were Wei Dai, creator of bitcoin predecessor b-money, and Nick Szabo, creator of bitcoin predecessor bit gold. One of the first miners included James Howells, who subsequently lost thousands of Bitcoin to a landfill in Newport. In the early days, Nakamoto is estimated to have mined 1 million bitcoins. Before disappearing from any involvement in bitcoin, Nakamoto in a sense handed over the reins to developer Gavin Andresen, who then became the bitcoin lead developer at the Bitcoin Foundation, the 'anarchic' bitcoin community's closest thing to an official public face. === Satoshi Nakamoto ===\n\n\"Satoshi Nakamoto\" is presumed to be a pseudonym for the person or people who designed the original bitcoin protocol in 2007 then released the whitepaper in 2008 and finally launched the",
            "link": "https://en.wikipedia.org/wiki/History_of_bitcoin",
            "snippet": "Bitcoin is a cryptocurrency, a digital asset that uses cryptography to control its creation and management rather than relying on central authorities. Originally designed as a medium of exchange, Bitcoin is now primarily regarded as a store of value. The history of bitcoin started with its invention and implementation by Satoshi Nakamoto, who integrated many existing ideas from the cryptography community. Over the course of bitcoin's history, it has undergone rapid growth to become a significant store of value both on- and offline. From the mid-2010s, some businesses began accepting bitcoin in addition to traditional currencies."
        },
        {
            "id": "c_a10_0",
            "title": "Hash function",
            "text": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing. Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527 Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords. == Overview ==\nIn a hash table, a hash function takes a key as an input, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed-length, like an integer, or variable-length, like a name.  In some cases, the key is the datum itself.  The output is a hash code used to index a hash table holding the data or records, or pointers to them.\nA hash function may be considered to perform three functions: Convert variable-length keys into fixed-length (usually machine-word-length or less) values, by folding them by words or other units using a parity-preserving operator like ADD or XOR,\nScramble the bits of the key so that the resulting values are uniformly distributed over the keyspace, and\nMap the key values into ones less than or equal to the size of the table. A good hash function satisfies two basic properties: it should be very fast to compute, and it should minimize duplication of output values (collisions).  Hash functions rely on generating favorable probability distributions for their effectiveness, reducing access time to nearly constant.  High table loading factors, pathological key sets, and poorly designed hash functions can result in access times approaching linear in the number of items in the table. Hash functions can be designed to give the best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists, or systematic probing of the table to find an empty slot. == Hash tables ==\n\nHash functions are used in conjunction with hash tables to store and retrieve data items or data records. The hash function translates the key associated with each datum or record into a hash code, which is used to index the hash table. When an item is to be added to the table, the hash code may index an empty slot (also called a bucket), in which case the item is added t",
            "link": "https://en.wikipedia.org/wiki/Hash_function",
            "snippet": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing.\nHash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527\u200a\nHash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords."
        },
        {
            "id": "c_a10_1",
            "title": "Data Encryption Standard",
            "text": "The Data Encryption Standard (DES ) is a symmetric-key algorithm for the encryption of digital data. Although its short key length of 56 bits makes it too insecure for modern applications, it has been highly influential in the advancement of cryptography.\nDeveloped in the early 1970s at IBM and based on an earlier design by Horst Feistel, the algorithm was submitted to the National Bureau of Standards (NBS) following the agency's invitation to propose a candidate for the protection of sensitive, unclassified electronic government data. In 1976, after consultation with the National Security Agency (NSA), the NBS selected a slightly modified version (strengthened against differential cryptanalysis, but weakened against brute-force attacks), which was published as an official Federal Information Processing Standard (FIPS) for the United States in 1977. The publication of an NSA-approved encryption standard led to its quick international adoption and widespread academic scrutiny. Controversies arose from classified design elements, a relatively short key length of the symmetric-key block cipher design, and the involvement of the NSA, raising suspicions about a backdoor.  The S-boxes that had prompted those suspicions were designed by the NSA to address a vulnerability they secretly knew (differential cryptanalysis). However, the NSA also ensured that the key size was drastically reduced so that they could break the cipher by brute force attack. The intense academic scrutiny the algorithm received over time led to the modern understanding of block ciphers and their cryptanalysis. DES is insecure due to the relatively short 56-bit key size. In January 1999, distributed.net and the Electronic Frontier Foundation collaborated to publicly break a DES key in 22 hours and 15 minutes (see \u00a7 Chronology). There are also some analytical results which demonstrate theoretical weaknesses in the cipher, although they are infeasible in practice. The algorithm is believed to be practically secure in the form of Triple DES, although there are theoretical attacks. This cipher has been superseded by the Advanced Encryption Standard (AES). DES has been withdrawn as a standard by the National Institute of Standards and Technology.\nSome documents distinguish between the DES standard and its algorithm, referring to the algorithm as the DEA (Data Encryption Algorithm). == History ==\nThe origins of DES date to 1972, when a National Bureau of Standards study of US government computer security identified a need for a government-wide standard for encrypting unclassified, sensitive information.\nAround the same time, engineer Mohamed Atalla in 1972 founded Atalla Corporation and developed the first hardware security module (HSM), the so-called \"Atalla Box\" which was commercialized in 1973. It protected offline devices with a secure PIN generating key, and was a commercial success. Banks and credit card companies were fearful that Atalla would dominate the market, which spurred the development of an international encryption standard. Atalla was an early competitor to IBM in the banking market, and was cited as an influence by IBM employees who worked on the DES standard. The IBM 3624 later adopted a similar PIN verification system to the earlier Atalla system. On 15 May 1973, after consulting with the NSA, NBS solicited proposals for a cipher that would meet rigorous design criteria. None of the submissions was suitable. A second request was issued on 27 August 1974. This time, IBM submitted a candidate which was deemed acceptable\u2014a cipher developed during the period 1973\u20131974 based on an earlier algorithm, Horst Feistel's Lucifer cipher. The team at IBM involved in cipher design and analysis included Feistel, Walter Tuchman, Don Coppersmith, Alan Konheim, Carl Meyer, Mike Matyas, Roy Adler, Edna Grossman, Bill Notz, Lynn Smith, and Bryant Tuckerman. === NSA's involvement in the design ===\nOn 17 March 1975, the proposed DES was published in the Federal Regis",
            "link": "https://en.wikipedia.org/wiki/Data_Encryption_Standard",
            "snippet": "The Data Encryption Standard (DES ) is a symmetric-key algorithm for the encryption of digital data. Although its short key length of 56 bits makes it too insecure for modern applications, it has been highly influential in the advancement of cryptography.\nDeveloped in the early 1970s at IBM and based on an earlier design by Horst Feistel, the algorithm was submitted to the National Bureau of Standards (NBS) following the agency's invitation to propose a candidate for the protection of sensitive, unclassified electronic government data. In 1976, after consultation with the National Security Agency (NSA), the NBS selected a slightly modified version (strengthened against differential cryptanalysis, but weakened against brute-force attacks), which was published as an official Federal Information Processing Standard (FIPS) for the United States in 1977.\nThe publication of an NSA-approved encryption standard led to its quick international adoption and widespread academic scrutiny. Controversies arose from classified design elements, a relatively short key length of the symmetric-key block cipher design, and the involvement of the NSA, raising suspicions about a backdoor.  The S-boxes that had prompted those suspicions were designed by the NSA to address a vulnerability they secretly knew (differential cryptanalysis). However, the NSA also ensured that the key size was drastically reduced so that they could break the cipher by brute force attack. The intense academic scrutiny the algorithm received over time led to the modern understanding of block ciphers and their cryptanalysis.\nDES is insecure due to the relatively short 56-bit key size. In January 1999, distributed.net and the Electronic Frontier Foundation collaborated to publicly break a DES key in 22 hours and 15 minutes (see \u00a7 Chronology). There are also some analytical results which demonstrate theoretical weaknesses in the cipher, although they are infeasible in practice. The algorithm is believed to be practically secure in the form of Triple DES, although there are theoretical attacks. This cipher has been superseded by the Advanced Encryption Standard (AES). DES has been withdrawn as a standard by the National Institute of Standards and Technology.\nSome documents distinguish between the DES standard and its algorithm, referring to the algorithm as the DEA (Data Encryption Algorithm).\n\n"
        },
        {
            "id": "c_a10_2",
            "title": "Block cipher mode of operation",
            "text": "In cryptography, a block cipher mode of operation is an algorithm that uses a block cipher to provide information security such as confidentiality or authenticity. A block cipher by itself is only suitable for the secure cryptographic transformation (encryption or decryption) of one fixed-length group of bits called a block. A mode of operation describes how to repeatedly apply a cipher's single-block operation to securely transform amounts of data larger than a block. Most modes require a unique binary sequence, often called an initialization vector (IV), for each encryption operation. The IV must be non-repeating, and for some modes must also be random. The initialization vector is used to ensure that distinct ciphertexts are produced even when the same plaintext is encrypted multiple times independently with the same key. Block ciphers may be capable of operating on more than one block size, but during transformation the block size is always fixed. Block cipher modes operate on whole blocks and require that the final data fragment be padded to a full block if it is smaller than the current block size. There are, however, modes that do not require padding because they effectively use a block cipher as a stream cipher. Historically, encryption modes have been studied extensively in regard to their error propagation properties under various scenarios of data modification. Later development regarded integrity protection as an entirely separate cryptographic goal. Some modern modes of operation combine confidentiality and authenticity in an efficient way, and are known as authenticated encryption modes. == History and standardization ==\nThe earliest modes of operation, ECB, CBC, OFB, and CFB (see below for all), date back to 1981 and were specified in FIPS 81, DES Modes of Operation. In 2001, the US National Institute of Standards and Technology (NIST) revised its list of approved modes of operation by including AES as a block cipher and adding CTR mode in SP800-38A, Recommendation for Block Cipher Modes of Operation. Finally, in January, 2010, NIST added XTS-AES in SP800-38E, Recommendation for Block Cipher Modes of Operation: The XTS-AES Mode for Confidentiality on Storage Devices. Other confidentiality modes exist which have not been approved by NIST. For example, CTS is ciphertext stealing mode and available in many popular cryptographic libraries. The block cipher modes ECB, CBC, OFB, CFB, CTR, and XTS provide confidentiality, but they do not protect against accidental modification or malicious tampering. Modification or tampering can be detected with a separate message authentication code such as CBC-MAC, or a digital signature. The cryptographic community recognized the need for dedicated integrity assurances and NIST responded with HMAC, CMAC, and GMAC. HMAC was approved in 2002 as FIPS 198, The Keyed-Hash Message Authentication Code (HMAC), CMAC was released in 2005 under SP800-38B, Recommendation for Block Cipher Modes of Operation: The CMAC Mode for Authentication, and GMAC was formalized in 2007 under SP800-38D, Recommendation for Block Cipher Modes of Operation: Galois/Counter Mode (GCM) and GMAC. The cryptographic community observed that compositing (combining)  a confidentiality mode with an authenticity mode could be difficult and error prone.  They therefore began to supply modes which combined confidentiality and data integrity into a single cryptographic primitive (an encryption algorithm). These combined modes are referred to as authenticated encryption, AE or \"authenc\". Examples of AE modes are CCM (SP800-38C), GCM (SP800-38D), CWC, EAX, IAPM, and OCB.\nModes of operation are defined by a number of national and internationally recognized standards bodies. Notable standards organizations include NIST, ISO (with ISO/IEC 10116), the IEC, the IEEE, ANSI, and the IETF. == Initialization vector (IV) ==\n\nAn initialization vector (IV) or starting variable (SV) is a block of bits that is used by several modes to r",
            "link": "https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation",
            "snippet": "In cryptography, a block cipher mode of operation is an algorithm that uses a block cipher to provide information security such as confidentiality or authenticity. A block cipher by itself is only suitable for the secure cryptographic transformation (encryption or decryption) of one fixed-length group of bits called a block. A mode of operation describes how to repeatedly apply a cipher's single-block operation to securely transform amounts of data larger than a block.\nMost modes require a unique binary sequence, often called an initialization vector (IV), for each encryption operation. The IV must be non-repeating, and for some modes must also be random. The initialization vector is used to ensure that distinct ciphertexts are produced even when the same plaintext is encrypted multiple times independently with the same key. Block ciphers may be capable of operating on more than one block size, but during transformation the block size is always fixed. Block cipher modes operate on whole blocks and require that the final data fragment be padded to a full block if it is smaller than the current block size. There are, however, modes that do not require padding because they effectively use a block cipher as a stream cipher.\nHistorically, encryption modes have been studied extensively in regard to their error propagation properties under various scenarios of data modification. Later development regarded integrity protection as an entirely separate cryptographic goal. Some modern modes of operation combine confidentiality and authenticity in an efficient way, and are known as authenticated encryption modes."
        },
        {
            "id": "c_a11_0",
            "title": "Types of artificial neural networks",
            "text": "There are many types of artificial neural networks (ANN).\nArtificial neural networks are computational models inspired by biological neural networks, and are used to approximate functions that are generally unknown. Particularly, they are inspired by the behaviour of neurons and the electrical signals they convey between input (such as from the eyes or nerve endings in the hand), processing, and output from the brain (such as reacting to light, touch, or heat). The way neurons semantically communicate is an area of ongoing research. Most artificial neural networks bear only some resemblance to their more complex biological counterparts, but are very effective at their intended tasks (e.g. classification or segmentation).\nSome artificial neural networks are adaptive systems and are used for example to model populations and environments, which constantly change. Neural networks can be hardware- (neurons are represented by physical components) or software-based (computer models), and can use a variety of topologies and learning algorithms. == Feedforward ==\n\nIn feedforward neural networks the information moves from the input to output directly in every layer. There can be hidden layers with or without cycles/loops to sequence inputs. Feedforward networks can be constructed with various types of units, such as binary McCulloch\u2013Pitts neurons, the simplest of which is the perceptron. Continuous neurons, frequently with sigmoidal activation, are used in the context of backpropagation.\n\n\n=== Group method of data handling === The Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov\u2013Gabor polynomials that permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size and depth of the resulting network depends on the task.\n\n\n=== Autoencoder === An autoencoder, autoassociator or Diabolo network:\u200a19\u200a is similar to the multilayer perceptron (MLP) \u2013 with an input layer, an output layer and one or more hidden layers connecting them. However, the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore, autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings, typically for the purpose of dimensionality reduction and for learning generative models of data.\n\n\n=== Probabilistic === A probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input, hidden pattern/summation, and output. In the PNN algorithm, the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then, using PDF of each class, the class probability of a new input is estimated and Bayes\u2019 rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition.\n\n\n=== Time delay === A time delay neural network (TDNN)  is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance, delays are added to the input so that multiple data points (points in time) are analyzed together.\nIt usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning).\n\n\n=== Convolutional ===\n\nA convolutional neural network (CNN, or ConvNet or shift invariant or space invariant) is a class of deep network, composed o",
            "link": "https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks",
            "snippet": "There are many types of artificial neural networks (ANN).\nArtificial neural networks are computational models inspired by biological neural networks, and are used to approximate functions that are generally unknown. Particularly, they are inspired by the behaviour of neurons and the electrical signals they convey between input (such as from the eyes or nerve endings in the hand), processing, and output from the brain (such as reacting to light, touch, or heat). The way neurons semantically communicate is an area of ongoing research. Most artificial neural networks bear only some resemblance to their more complex biological counterparts, but are very effective at their intended tasks (e.g. classification or segmentation).\nSome artificial neural networks are adaptive systems and are used for example to model populations and environments, which constantly change.\nNeural networks can be hardware- (neurons are represented by physical components) or software-based (computer models), and can use a variety of topologies and learning algorithms.\n\n"
        },
        {
            "id": "c_a11_1",
            "title": "Python syntax and semantics",
            "text": "The syntax of the Python programming language is the set of rules that defines how a Python program will be written and interpreted (by both the runtime system and by human readers). The Python language has many similarities to Perl, C, and Java. However, there are some definite differences between the languages. It supports multiple programming paradigms, including structured, object-oriented programming, and functional programming, and boasts a dynamic type system and automatic memory management.\nPython's syntax is simple and consistent, adhering to the principle that \"There should be one\u2014 and preferably only one \u2014obvious way to do it.\" The language incorporates built-in data types and structures, control flow mechanisms,  first-class functions, and modules for better code reusability and organization. Python also uses English keywords where other languages use punctuation, contributing to its uncluttered visual layout. The language provides robust error handling through exceptions, and includes a debugger in the standard library for efficient problem-solving. Python's syntax, designed for readability and ease of use, makes it a popular choice among beginners and professionals alike. == Design philosophy ==\nPython was designed to be a highly readable language. It has a relatively uncluttered visual layout and uses English keywords frequently where other languages use punctuation. Python aims to be simple and consistent in the design of its syntax, encapsulated in the mantra \"There should be one\u2014 and preferably only one \u2014obvious way to do it\", from the Zen of Python.\nThis mantra is deliberately opposed to the Perl and Ruby mantra, \"there's more than one way to do it\".\n\n\n== Keywords ==\nPython has 35 keywords or reserved words; they cannot be used as identifiers.\n\nIn addition, Python also has 3 soft keywords. Unlike regular hard keywords, soft keywords are reserved words only in the limited contexts where interpreting them as keywords would make syntactic sense. These words can be used as identifiers elsewhere, in other words, match and case are valid names for functions and variables.\n\n_\ncase\nmatch\nNotes == Indentation ==\nPython uses whitespace to delimit control flow blocks (following the off-side rule). Python borrows this feature from its predecessor ABC: instead of punctuation or keywords, it uses indentation to indicate the run of a block.\nIn so-called \"free-format\" languages\u2014that use the block structure derived from ALGOL\u2014blocks of code are set off with braces ({ }) or keywords. In most coding conventions for these languages, programmers conventionally indent the code within a block, to visually set it apart from the surrounding code.\nA recursive function named foo, which is passed a single parameter, x, and if the parameter is 0 will call a different function named bar and otherwise will call baz, passing x, and also call itself recursively, passing x-1 as the parameter, could be implemented like this in Python:\n\nand could be written like this in C with K&R indent style: Incorrectly indented code could be misread by a human reader differently than it would be interpreted by a compiler or interpreter. For example, if the function call foo(x - 1) on the last line in the example above was erroneously indented to be outside the if/else block: \n\nit would cause the last line to always be executed, even when x is 0, resulting in an endless recursion.\nWhile both space and tab characters are accepted as forms of indentation and any multiple of spaces can be used, spaces are recommended and 4 spaces (as in the above examples) are recommended and are by far the most commonly used. Mixing spaces and tabs on consecutive lines is not allowed starting with Python 3 because that can create bugs which are difficult to see, since many text editors do not visually distinguish spaces and tabs.\n\n\n== Data structures ==\n\nSince Python is a dynamically-typed language, Python values, not variables, carry type information. All variables in P",
            "link": "https://en.wikipedia.org/wiki/Python_syntax_and_semantics",
            "snippet": "The syntax of the Python programming language is the set of rules that defines how a Python program will be written and interpreted (by both the runtime system and by human readers). The Python language has many similarities to Perl, C, and Java. However, there are some definite differences between the languages. It supports multiple programming paradigms, including structured, object-oriented programming, and functional programming, and boasts a dynamic type system and automatic memory management.\nPython's syntax is simple and consistent, adhering to the principle that \"There should be one\u2014 and preferably only one \u2014obvious way to do it.\" The language incorporates built-in data types and structures, control flow mechanisms,  first-class functions, and modules for better code reusability and organization. Python also uses English keywords where other languages use punctuation, contributing to its uncluttered visual layout.\nThe language provides robust error handling through exceptions, and includes a debugger in the standard library for efficient problem-solving. Python's syntax, designed for readability and ease of use, makes it a popular choice among beginners and professionals alike.\n\n"
        },
        {
            "id": "c_a12_0",
            "title": "Hash function",
            "text": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing. Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527 Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords. == Overview ==\nIn a hash table, a hash function takes a key as an input, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed-length, like an integer, or variable-length, like a name.  In some cases, the key is the datum itself.  The output is a hash code used to index a hash table holding the data or records, or pointers to them.\nA hash function may be considered to perform three functions: Convert variable-length keys into fixed-length (usually machine-word-length or less) values, by folding them by words or other units using a parity-preserving operator like ADD or XOR,\nScramble the bits of the key so that the resulting values are uniformly distributed over the keyspace, and\nMap the key values into ones less than or equal to the size of the table. A good hash function satisfies two basic properties: it should be very fast to compute, and it should minimize duplication of output values (collisions).  Hash functions rely on generating favorable probability distributions for their effectiveness, reducing access time to nearly constant.  High table loading factors, pathological key sets, and poorly designed hash functions can result in access times approaching linear in the number of items in the table. Hash functions can be designed to give the best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists, or systematic probing of the table to find an empty slot. == Hash tables ==\n\nHash functions are used in conjunction with hash tables to store and retrieve data items or data records. The hash function translates the key associated with each datum or record into a hash code, which is used to index the hash table. When an item is to be added to the table, the hash code may index an empty slot (also called a bucket), in which case the item is added t",
            "link": "https://en.wikipedia.org/wiki/Hash_function",
            "snippet": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing.\nHash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527\u200a\nHash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords."
        },
        {
            "id": "c_a12_1",
            "title": "Cryptographic hash function",
            "text": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application: the probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox). Cryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions. == Properties ==\nMost cryptographic hash functions are designed to take a string of any length as input and produce a fixed-length hash value.\nA cryptographic hash function must be able to withstand all known types of cryptanalytic attack. In theoretical cryptography, the security level of a cryptographic hash function has been defined using the following properties: Pre-image resistance\nGiven a hash value h, it should be difficult to find any message m such that h = hash(m). This concept is related to that of a one-way function. Functions that lack this property are vulnerable to preimage attacks.\nSecond pre-image resistance\nGiven an input m1, it should be difficult to find a different input m2 such that hash(m1) = hash(m2). This property is sometimes referred to as weak collision resistance. Functions that lack this property are vulnerable to second-preimage attacks.\nCollision resistance\nIt should be difficult to find two different messages m1 and m2 such that hash(m1) = hash(m2). Such a pair is called a cryptographic hash collision. This property is sometimes referred to as strong collision resistance. It requires",
            "link": "https://en.wikipedia.org/wiki/Cryptographic_hash_function",
            "snippet": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application:\n\nthe probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox).\nCryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions.\n\n"
        },
        {
            "id": "c_a12_2",
            "title": "Salt (cryptography)",
            "text": "In cryptography, a salt is random data fed as an additional input to a one-way function that hashes data, a password or passphrase. Salting helps defend against attacks that use precomputed tables (e.g. rainbow tables), by vastly growing the size of table needed for a successful attack. It also helps protect passwords that occur multiple times in a database, as a new salt is used for each password instance. Additionally, salting does not place any burden on users. \nTypically, a unique salt is randomly generated for each password. The salt and the password (or its version after key stretching) are concatenated and fed to a cryptographic hash function, and the output hash value is then stored with the salt in a database. The salt does not need to be encrypted, because knowing the salt would not help the attacker.\nSalting is broadly used in cybersecurity, from Unix system credentials to Internet security.\nSalts are related to cryptographic nonces. == Example ==\nWithout a salt, identical passwords will map to identical hash values, which could make it easier for a hacker to guess the passwords from their hash value.\n\nInstead, a salt is generated and appended to each password, which causes the resultant hash to output different values for the same original password. The salt and hash are then stored in the database. To later test if a password a user enters is correct, the same process can be performed on it (appending that user's salt to the password and calculating the resultant hash): if the result does not match the stored hash, it could not have been the correct password that was entered.\nIn practice, a salt is usually generated using a Cryptographically Secure PseudoRandom Number Generator. CSPRNGs are designed to produce unpredictable random numbers which can be alphanumeric. While generally discouraged due to lower security, some systems use timestamps or simple counters as a source of salt. Sometimes, a salt may be generated by combining a random value with additional information, such as a timestamp or user-specific data, to ensure uniqueness across different systems or time periods.\n\n\n== Common mistakes == === Salt re-use ===\nUsing the same salt for all passwords is dangerous because a precomputed table which simply accounts for the salt will render the salt useless. \nGeneration of precomputed tables for databases with unique salts for every password is not viable because of the computational cost of doing so. But, if a common salt is used for all the entries, creating such a table (that accounts for the salt) then becomes a viable and possibly successful attack.\nBecause salt re-use can cause users with the same password to have the same hash, cracking a single hash can result in other passwords being compromised too. === Salt length ===\nIf a salt is too short, an attacker may precompute a table of every possible salt appended to every likely password. Using a long salt ensures such a table would be prohibitively large.  16 bytes (128 bits) or more is generally sufficient to provide a large enough space of possible values, minimizing the risk of collisions (i.e., two different passwords ending up with the same salt). == Benefits ==\nTo understand the difference between cracking a single password and a set of them, consider a file with users and their hashed passwords. Say the file is unsalted. Then an attacker could pick a string, call it attempt[0], and then compute hash(attempt[0]). A user whose hash stored in the file is hash(attempt[0]) may or may not have password attempt[0]. However, even if attempt[0] is not the user's actual password, it will be accepted as if it were, because the system can only check passwords by computing the hash of the password entered and comparing it to the hash stored in the file.  Thus, each match cracks a user password, and the chance of a match rises with the number of passwords in the file. In contrast, if salts are used, the attacker would have to compute hash(attempt[0] || s",
            "link": "https://en.wikipedia.org/wiki/Salt_(cryptography)",
            "snippet": "In cryptography, a salt is random data fed as an additional input to a one-way function that hashes data, a password or passphrase. Salting helps defend against attacks that use precomputed tables (e.g. rainbow tables), by vastly growing the size of table needed for a successful attack. It also helps protect passwords that occur multiple times in a database, as a new salt is used for each password instance. Additionally, salting does not place any burden on users. \nTypically, a unique salt is randomly generated for each password. The salt and the password (or its version after key stretching) are concatenated and fed to a cryptographic hash function, and the output hash value is then stored with the salt in a database. The salt does not need to be encrypted, because knowing the salt would not help the attacker.\nSalting is broadly used in cybersecurity, from Unix system credentials to Internet security.\nSalts are related to cryptographic nonces.\n\n"
        },
        {
            "id": "c_a13_0",
            "title": "List of algorithms",
            "text": "An algorithm is fundamentally a set of rules or defined procedures that is typically designed and used to solve a specific problem or a broad set of problems. \nBroadly, algorithms define process(es), sets of rules, or methodologies that are to be followed in calculations, data processing, data mining, pattern recognition, automated reasoning or other problem-solving operations. With the increasing automation of services, more and more decisions are being made by algorithms. Some general examples are; risk assessments, anticipatory policing, and pattern recognition technology.\nThe following is a list of well-known algorithms along with one-line descriptions for each.\n\n\n== Automated planning ==\n\n\n== Combinatorial algorithms == === General combinatorial algorithms ===\nBrent's algorithm: finds a cycle in function value iterations using only two iterators\nFloyd's cycle-finding algorithm: finds a cycle in function value iterations\nGale\u2013Shapley algorithm: solves the stable marriage problem\nPseudorandom number generators (uniformly distributed\u2014see also List of pseudorandom number generators for other PRNGs with varying degrees of convergence and varying statistical quality):\nACORN generator\nBlum Blum Shub\nLagged Fibonacci generator\nLinear congruential generator\nMersenne Twister\n\n\n=== Graph algorithms === Coloring algorithm: Graph coloring algorithm.\nHopcroft\u2013Karp algorithm: convert a bipartite graph to a maximum cardinality matching\nHungarian algorithm: algorithm for finding a perfect matching\nPr\u00fcfer coding: conversion between a labeled tree and its Pr\u00fcfer sequence\nTarjan's off-line lowest common ancestors algorithm: computes lowest common ancestors for pairs of nodes in a tree\nTopological sort: finds linear order of nodes (e.g. jobs) based on their dependencies.\n\n\n==== Graph drawing ====\n\nForce-based algorithms (also known as force-directed algorithms or spring-based algorithm)\nSpectral layout\n\n\n==== Network theory ==== Network analysis\nLink analysis\nGirvan\u2013Newman algorithm: detect communities in complex systems\nWeb link analysis\nHyperlink-Induced Topic Search (HITS) (also known as Hubs and authorities)\nPageRank\nTrustRank\nFlow networks\nDinic's algorithm: is a strongly polynomial algorithm for computing the maximum flow in a flow network.\nEdmonds\u2013Karp algorithm: implementation of Ford\u2013Fulkerson\nFord\u2013Fulkerson algorithm: computes the maximum flow in a graph\nKarger's algorithm: a Monte Carlo method to compute the minimum cut of a connected graph\nPush\u2013relabel algorithm: computes a maximum flow in a graph ==== Routing for graphs ====\nEdmonds' algorithm (also known as Chu\u2013Liu/Edmonds' algorithm): find maximum or minimum branchings\nEuclidean minimum spanning tree: algorithms for computing the minimum spanning tree of a set of points in the plane\nLongest path problem: find a simple path of maximum length in a given graph\nMinimum spanning tree\nBor\u016fvka's algorithm\nKruskal's algorithm\nPrim's algorithm\nReverse-delete algorithm\nNonblocking minimal spanning switch say, for a telephone exchange\nShortest path problem\nBellman\u2013Ford algorithm: computes shortest paths in a weighted graph (where some of the edge weights may be negative)\nDijkstra's algorithm: computes shortest paths in a graph with non-negative edge weights\nFloyd\u2013Warshall algorithm: solves the all pairs shortest path problem in a weighted, directed graph\nJohnson's algorithm: all pairs shortest path algorithm in sparse weighted directed graph\nTransitive closure problem: find the transitive closure of a given binary relation Traveling salesman problem\nChristofides algorithm\nNearest neighbour algorithm\nWarnsdorff's rule: a heuristic method for solving the Knight's tour problem ==== Graph search ====\n\nA*: special case of best-first search that uses heuristics to improve speed\nB*: a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals)\nBacktracking: abandons partial solutions when they are found n",
            "link": "https://en.wikipedia.org/wiki/List_of_algorithms",
            "snippet": "An algorithm is fundamentally a set of rules or defined procedures that is typically designed and used to solve a specific problem or a broad set of problems. \nBroadly, algorithms define process(es), sets of rules, or methodologies that are to be followed in calculations, data processing, data mining, pattern recognition, automated reasoning or other problem-solving operations. With the increasing automation of services, more and more decisions are being made by algorithms. Some general examples are; risk assessments, anticipatory policing, and pattern recognition technology.\nThe following is a list of well-known algorithms along with one-line descriptions for each."
        },
        {
            "id": "c_a13_1",
            "title": "Addiction",
            "text": "Addiction is a neuropsychological disorder characterized by a persistent and intense urge to use a drug or engage in a behavior that produces natural reward, despite substantial harm and other negative consequences. Repetitive drug use can alter brain function in synapses similar to natural rewards like food or falling in love in ways that perpetuate craving and weakens self-control for people with pre-existing vulnerabilities. This phenomenon \u2013 drugs reshaping brain function \u2013 has led to an understanding of addiction as a brain disorder with a complex variety of psychosocial as well as neurobiological factors that are implicated in the development of addiction. While mice given cocaine showed the compulsive and involuntary nature of addiction, for humans this is more complex, related to behavior or personality traits Classic signs of addiction include compulsive engagement in rewarding stimuli, preoccupation with substances or behavior, and continued use despite negative consequences. Habits and patterns associated with addiction are typically characterized by immediate gratification (short-term reward), coupled with delayed deleterious effects (long-term costs).\nExamples of substance addiction include alcoholism, cannabis addiction, amphetamine addiction, cocaine addiction, nicotine addiction, opioid addiction, and eating or food addiction. Behavioral addictions may include gambling addiction, shopping addiction, stalking, internet addiction, social media addiction, video game addiction, and sexual addiction. The DSM-5 and ICD-10 only recognize gambling addictions as behavioral addictions, but the ICD-11 also recognizes gaming addictions. == Signs and symptoms ==\nSigns and symptoms of addiction can vary depending on the type of addiction. Symptoms of drug addictions may include:\n\nContinuation of drug use despite the knowledge of consequences\nDisregarding financial status when it comes to drug purchases\nEnsuring a stable supply of the drug\nNeeding more of the drug over time to achieve similar effects\nSocial and work life impacted due to drug use\nUnsuccessful attempts to stop drug use\nUrge to use drug regularly\nSigns and symptoms of addiction may include: == Definitions ==\n\"Addiction\" and \"addictive behaviour\" are polysemes denoting a category of mental disorders, of neuropsychological symptoms, or of merely maladaptive/harmful habits and lifestyles. A common use of \"addiction\" in medicine is for neuropsychological symptoms denoting pervasive/excessive and intense urges to engage in a category of behavioral compulsions or impulses towards sensory rewards (e.g., alcohol, betel quid, drugs, sex, gambling, video gaming). Addictive disorders or addiction disorders are mental disorders involving high intensities of addictions (as neuropsychological symptoms) that induce functional disabilities (i.e., limit subjects' social/family and occupational activities); the two categories of such disorders are substance-use addictions and behavioral addictions. The DSM-5 classifies addiction the most severe stage of substance use disorder, due to significant loss of control and the presence of compulsive behaviours despite the desire to stop. It is a definition that many scientific papers and reports use.\n\"Dependence\" is also a polyseme denoting either neuropsychological symptoms or mental disorders. In the DSM-5, dependences differ from addictions and can even normally happen without addictions; besides, substance-use dependences are severe stages of substance-use addictions (i.e. mental disorders) involving withdrawal issues. In the ICD-11, \"substance-use dependence\" is a synonym of \"substance-use addiction\" (i.e. neuropsychological symptoms) that can but do not necessarily involve withdrawal issues. == Substance addiction ==\n\n\n=== Drug addiction ===\nDrug addiction, which belongs to the class of substance-related disorders, is a chronic and relapsing brain disorder that features drug seeking and drug abuse, despite their harmful effec",
            "link": "https://en.wikipedia.org/wiki/Addiction",
            "snippet": "Addiction is a neuropsychological disorder characterized by a persistent and intense urge to use a drug or engage in a behavior that produces natural reward, despite substantial harm and other negative consequences. Repetitive drug use can alter brain function in synapses similar to natural rewards like food or falling in love in ways that perpetuate craving and weakens self-control for people with pre-existing vulnerabilities. This phenomenon \u2013 drugs reshaping brain function \u2013 has led to an understanding of addiction as a brain disorder with a complex variety of psychosocial as well as neurobiological factors that are implicated in the development of addiction. While mice given cocaine showed the compulsive and involuntary nature of addiction, for humans this is more complex, related to behavior or personality traits\nClassic signs of addiction include compulsive engagement in rewarding stimuli, preoccupation with substances or behavior, and continued use despite negative consequences. Habits and patterns associated with addiction are typically characterized by immediate gratification (short-term reward), coupled with delayed deleterious effects (long-term costs).\nExamples of substance addiction include alcoholism, cannabis addiction, amphetamine addiction, cocaine addiction, nicotine addiction, opioid addiction, and eating or food addiction. Behavioral addictions may include gambling addiction, shopping addiction, stalking, internet addiction, social media addiction, video game addiction, and sexual addiction. The DSM-5 and ICD-10 only recognize gambling addictions as behavioral addictions, but the ICD-11 also recognizes gaming addictions."
        },
        {
            "id": "c_a14_0",
            "title": "Cryptographic hash function",
            "text": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application: the probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox). Cryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions. == Properties ==\nMost cryptographic hash functions are designed to take a string of any length as input and produce a fixed-length hash value.\nA cryptographic hash function must be able to withstand all known types of cryptanalytic attack. In theoretical cryptography, the security level of a cryptographic hash function has been defined using the following properties: Pre-image resistance\nGiven a hash value h, it should be difficult to find any message m such that h = hash(m). This concept is related to that of a one-way function. Functions that lack this property are vulnerable to preimage attacks.\nSecond pre-image resistance\nGiven an input m1, it should be difficult to find a different input m2 such that hash(m1) = hash(m2). This property is sometimes referred to as weak collision resistance. Functions that lack this property are vulnerable to second-preimage attacks.\nCollision resistance\nIt should be difficult to find two different messages m1 and m2 such that hash(m1) = hash(m2). Such a pair is called a cryptographic hash collision. This property is sometimes referred to as strong collision resistance. It requires",
            "link": "https://en.wikipedia.org/wiki/Cryptographic_hash_function",
            "snippet": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application:\n\nthe probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox).\nCryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions.\n\n"
        },
        {
            "id": "c_a14_1",
            "title": "MD5",
            "text": "The MD5 message-digest algorithm is a widely used hash function producing a 128-bit hash value. MD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function MD4, and was specified in 1992 as RFC 1321.\nMD5 can be used as a checksum to verify data integrity against unintentional corruption. Historically it was widely used as a cryptographic hash function; however it has been found to suffer from extensive vulnerabilities. It remains suitable for other non-cryptographic purposes, for example for determining the partition for a particular key in a partitioned database, and may be preferred due to lower computational requirements than more recent Secure Hash Algorithms. == History and cryptanalysis ==\nMD5 is one in a series of message digest algorithms designed by Professor Ronald Rivest of MIT (Rivest, 1992).  When analytic work indicated that MD5's predecessor MD4 was likely to be insecure, Rivest designed MD5 in 1991 as a secure replacement. (Hans Dobbertin did indeed later find weaknesses in MD4.)\nIn 1993, Den Boer and Bosselaers gave an early, although limited, result of finding a \"pseudo-collision\" of the MD5 compression function; that is, two different initialization vectors that produce an identical digest.\nIn 1996, Dobbertin announced a collision of the compression function of MD5 (Dobbertin, 1996). While this was not an attack on the full MD5 hash function, it was close enough for cryptographers to recommend switching to a replacement, such as SHA-1 (also compromised since) or RIPEMD-160. The size of the hash value (128 bits) is small enough to contemplate a birthday attack. MD5CRK was a distributed project started in March 2004 to demonstrate that MD5 is practically insecure by finding a collision using a birthday attack.\nMD5CRK ended shortly after 17 August 2004, when collisions for the full MD5 were announced by Xiaoyun Wang, Dengguo Feng, Xuejia Lai, and Hongbo Yu. Their analytical attack was reported to take only one hour on an IBM p690 cluster. On 1 March 2005, Arjen Lenstra, Xiaoyun Wang, and Benne de Weger demonstrated construction of two X.509 certificates with different public keys and the same MD5 hash value, a demonstrably practical collision. The construction included private keys for both public keys. A few days later, Vlastimil Klima described an improved algorithm, able to construct MD5 collisions in a few hours on a single notebook computer. On 18 March 2006, Klima published an algorithm that could find a collision within one minute on a single notebook computer, using a method he calls tunneling.\nVarious MD5-related RFC errata have been published. \nIn 2009, the United States Cyber Command used an MD5 hash value of their mission statement as a part of their official emblem. On 24 December 2010, Tao Xie and Dengguo Feng announced the first published single-block (512-bit) MD5 collision. (Previous collision discoveries had relied on multi-block attacks.) For \"security reasons\", Xie and Feng did not disclose the new attack method. They issued a challenge to the cryptographic community, offering a US$10,000 reward to the first finder of a different 64-byte collision before 1 January 2013. Marc Stevens responded to the challenge and published colliding single-block messages as well as the construction algorithm and sources.\nIn 2011 an informational RFC 6151 was approved to update the security considerations in MD5 and HMAC-MD5. == Security ==\nOne basic requirement of any cryptographic hash function is that it should be computationally infeasible to find two distinct messages that hash to the same value. MD5 fails this requirement catastrophically. On 31 December 2008, the CMU Software Engineering Institute concluded that  MD5 was essentially \"cryptographically broken and unsuitable for further use\". The weaknesses of MD5 have been exploited in the field, most infamously by the Flame malware in 2012. As of 2019, MD5 continues to be widely used, despite its well-documented weaknesses and dep",
            "link": "https://en.wikipedia.org/wiki/MD5",
            "snippet": "The MD5 message-digest algorithm is a widely used hash function producing a 128-bit hash value. MD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function MD4, and was specified in 1992 as RFC 1321.\nMD5 can be used as a checksum to verify data integrity against unintentional corruption. Historically it was widely used as a cryptographic hash function; however it has been found to suffer from extensive vulnerabilities. It remains suitable for other non-cryptographic purposes, for example for determining the partition for a particular key in a partitioned database, and may be preferred due to lower computational requirements than more recent Secure Hash Algorithms.\n\n"
        },
        {
            "id": "c_a14_2",
            "title": "Rainbow table",
            "text": "A rainbow table is a precomputed table for caching the outputs of a cryptographic hash function, usually for cracking password hashes. Passwords are typically stored not in plain text form, but as hash values. If such a database of hashed passwords falls into the hands of attackers, they can use a precomputed rainbow table to recover the plaintext passwords. A common defense against this attack is to compute the hashes using a key derivation function that adds a \"salt\" to each password before hashing it, with different passwords receiving different salts, which are stored in plain text along with the hash.\nRainbow tables are a practical example of a space\u2013time tradeoff: they use less computer processing time and more storage than a brute-force attack which calculates a hash on every attempt, but more processing time and less storage than a simple table that stores the hash of every possible password. Rainbow tables were invented by Philippe Oechslin as an application of an earlier, simpler algorithm by Martin Hellman. == Background ==\nFor user authentication, passwords are stored either as plaintext or hashes. Since passwords stored as plaintext are easily stolen if database access is compromised, databases typically store hashes instead. Thus, no one \u2013 including the authentication system \u2013 can learn a password merely by looking at the value stored in the database.\nWhen a user enters a password for authentication, a hash is computed for it and then compared to the stored hash for that user. Authentication fails if the two hashes do not match; moreover, authentication would equally fail if a hashed value were entered as a password, since the authentication system would hash it a second time.\nTo learn a password from a hash is to find a string which, when input into the hash function, creates that same hash. This is the same as inverting the hash function. Though brute-force attacks (e.g. dictionary attacks) may be used to try to invert a hash function, they can become infeasible when the set of possible passwords is large enough. An alternative to brute-force is to use precomputed hash chain tables. Rainbow tables are a special kind of such table that overcome certain technical difficulties. == Etymology ==\n\nThe term rainbow tables was first used in Oechslin's initial paper. The term refers to the way different reduction functions are used to increase the success rate of the attack. The original method by Hellman uses many small tables with a different reduction function each. Rainbow tables are much bigger and use a different reduction function in each column. When colors are used to represent the reduction functions, a rainbow appears in the rainbow table. \nFigure 2 of Oechslin's paper contains a black-and-white graphic that illustrates how these sections are related. For his presentation at the Crypto 2003 conference, Oechslin added color to the graphic in order to make the rainbow association more clear. The enhanced graphic that was presented at the conference is shown in the illustration.\n\n\n== Precomputed hash chains == Given a password hash function H and a finite set of passwords P, the goal is to precompute a data structure that, given any output h of the hash function, can either locate an element p in P such that H(p) = h, or determine that there is no such p in P. The simplest way to do this is compute H(p) for all p in P, but then storing the table requires \u0398(|P|n) bits of space, where |P| is the size of the set P and n is the size of an output of H, which is prohibitive for large |P|. Hash chains are a technique for decreasing this space requirement. The idea is to define a reduction function R that maps hash values back into values in P. Note, however, that the reduction function is not actually an inverse of the hash function, but rather a different function with a swapped domain and codomain of the hash function. By alternating the hash function with the reduction function, chains of alternating passwords",
            "link": "https://en.wikipedia.org/wiki/Rainbow_table",
            "snippet": "A rainbow table is a precomputed table for caching the outputs of a cryptographic hash function, usually for cracking password hashes. Passwords are typically stored not in plain text form, but as hash values. If such a database of hashed passwords falls into the hands of attackers, they can use a precomputed rainbow table to recover the plaintext passwords. A common defense against this attack is to compute the hashes using a key derivation function that adds a \"salt\" to each password before hashing it, with different passwords receiving different salts, which are stored in plain text along with the hash.\nRainbow tables are a practical example of a space\u2013time tradeoff: they use less computer processing time and more storage than a brute-force attack which calculates a hash on every attempt, but more processing time and less storage than a simple table that stores the hash of every possible password.\nRainbow tables were invented by Philippe Oechslin as an application of an earlier, simpler algorithm by Martin Hellman."
        },
        {
            "id": "c_a15_0",
            "title": "Cryptographic hash function",
            "text": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application: the probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox). Cryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions. == Properties ==\nMost cryptographic hash functions are designed to take a string of any length as input and produce a fixed-length hash value.\nA cryptographic hash function must be able to withstand all known types of cryptanalytic attack. In theoretical cryptography, the security level of a cryptographic hash function has been defined using the following properties: Pre-image resistance\nGiven a hash value h, it should be difficult to find any message m such that h = hash(m). This concept is related to that of a one-way function. Functions that lack this property are vulnerable to preimage attacks.\nSecond pre-image resistance\nGiven an input m1, it should be difficult to find a different input m2 such that hash(m1) = hash(m2). This property is sometimes referred to as weak collision resistance. Functions that lack this property are vulnerable to second-preimage attacks.\nCollision resistance\nIt should be difficult to find two different messages m1 and m2 such that hash(m1) = hash(m2). Such a pair is called a cryptographic hash collision. This property is sometimes referred to as strong collision resistance. It requires",
            "link": "https://en.wikipedia.org/wiki/Cryptographic_hash_function",
            "snippet": "A cryptographic hash function (CHF) is a hash algorithm (a map of an arbitrary binary string to a binary string with a fixed size of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits) that has special properties desirable for a cryptographic application:\n\nthe probability of a particular \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n-bit output result (hash value) for a random input string (\"message\") is \n  \n    \n      \n        \n          2\n          \n            \u2212\n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{-n}}\n  \n (as for any good hash), so the hash value can be used as a representative of the message;\nfinding an input string that matches a given hash value (a pre-image) is infeasible, assuming all input strings are equally likely.  The resistance to such search is quantified as security strength: a cryptographic hash with \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits of hash value is expected to have a preimage resistance strength of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n bits, unless the space of possible input values is significantly smaller than \n  \n    \n      \n        \n          2\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 2^{n}}\n  \n (a practical example can be found in \u00a7 Attacks on hashed passwords);\na second preimage resistance strength, with the same expectations, refers to a similar problem of finding a second message that matches the given hash value when one message is already known;\nfinding any pair of different messages that yield the same hash value (a collision) is also infeasible: a cryptographic hash is expected to have a collision resistance strength of \n  \n    \n      \n        n\n        \n          /\n        \n        2\n      \n    \n    {\\displaystyle n/2}\n  \n bits (lower due to the birthday paradox).\nCryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\nNon-cryptographic hash functions are used in hash tables and to detect accidental errors; their constructions frequently provide no resistance to a deliberate attack. For example, a denial-of-service attack on hash tables is possible if the collisions are easy to find, as in the case of linear cyclic redundancy check (CRC) functions.\n\n"
        },
        {
            "id": "c_a15_1",
            "title": "Hash function",
            "text": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing. Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527 Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords. == Overview ==\nIn a hash table, a hash function takes a key as an input, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed-length, like an integer, or variable-length, like a name.  In some cases, the key is the datum itself.  The output is a hash code used to index a hash table holding the data or records, or pointers to them.\nA hash function may be considered to perform three functions: Convert variable-length keys into fixed-length (usually machine-word-length or less) values, by folding them by words or other units using a parity-preserving operator like ADD or XOR,\nScramble the bits of the key so that the resulting values are uniformly distributed over the keyspace, and\nMap the key values into ones less than or equal to the size of the table. A good hash function satisfies two basic properties: it should be very fast to compute, and it should minimize duplication of output values (collisions).  Hash functions rely on generating favorable probability distributions for their effectiveness, reducing access time to nearly constant.  High table loading factors, pathological key sets, and poorly designed hash functions can result in access times approaching linear in the number of items in the table. Hash functions can be designed to give the best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists, or systematic probing of the table to find an empty slot. == Hash tables ==\n\nHash functions are used in conjunction with hash tables to store and retrieve data items or data records. The hash function translates the key associated with each datum or record into a hash code, which is used to index the hash table. When an item is to be added to the table, the hash code may index an empty slot (also called a bucket), in which case the item is added t",
            "link": "https://en.wikipedia.org/wiki/Hash_function",
            "snippet": "A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable-length output. The values returned by a hash function are called hash values, hash codes, hash digests, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter-storage addressing.\nHash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally- and storage-space-efficient form of data access that avoids the non-constant access time of ordered and unordered lists and structured trees, and the often-exponential storage requirements of direct access of state spaces of large or variable-length keys.\nUse of hash functions relies on statistical properties of key and function interaction: worst-case behavior is intolerably bad but rare, and average-case behavior can be nearly optimal (minimal collision).:\u200a527\u200a\nHash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity. Hash tables may use non-cryptographic hash functions, while cryptographic hash functions are used in cybersecurity to secure sensitive data such as passwords."
        },
        {
            "id": "c_a15_2",
            "title": "Transport Layer Security",
            "text": "Transport Layer Security (TLS) is a cryptographic protocol designed to provide communications security over a computer network, such as the Internet. The protocol is widely used in applications such as email, instant messaging, and voice over IP, but its use in securing HTTPS remains the most publicly visible.\nThe TLS protocol aims primarily to provide security, including privacy (confidentiality), integrity, and authenticity through the use of cryptography, such as the use of certificates, between two or more communicating computer applications. It runs in the presentation layer and is itself composed of two layers: the TLS record and the TLS handshake protocols.\nThe closely related Datagram Transport Layer Security (DTLS) is a communications protocol that provides security to datagram-based applications. In technical writing, references to \"(D)TLS\" are often seen when it applies to both versions. TLS is a proposed Internet Engineering Task Force (IETF) standard, first defined in 1999, and the current version is TLS 1.3, defined in August 2018. TLS builds on the now-deprecated SSL (Secure Sockets Layer) specifications (1994, 1995, 1996) developed by Netscape Communications for adding the HTTPS protocol to their Netscape Navigator web browser. == Description ==\nClient-server applications use the TLS protocol to communicate across a network in a way designed to prevent eavesdropping and tampering.\nSince applications can communicate either with or without TLS (or SSL), it is necessary for the client to request that the server set up a TLS connection. One of the main ways of achieving this is to use a different port number for TLS connections. Port 80 is typically used for unencrypted HTTP traffic while port 443 is the common port used for encrypted HTTPS traffic. Another mechanism is to make a protocol-specific STARTTLS request to the server to switch the connection to TLS \u2013 for example, when using the mail and news protocols. Once the client and server have agreed to use TLS, they negotiate a stateful connection by using a handshaking procedure (see \u00a7 TLS handshake). The protocols use a handshake with an asymmetric cipher to establish not only cipher settings but also a session-specific shared key with which further communication is encrypted using a symmetric cipher. During this handshake, the client and server agree on various parameters used to establish the connection's security: The handshake begins when a client connects to a TLS-enabled server requesting a secure connection and the client presents a list of supported cipher suites (ciphers and hash functions).\nFrom this list, the server picks a cipher and hash function that it also supports and notifies the client of the decision.\nThe server usually then provides identification in the form of a digital certificate. The certificate contains the server name, the trusted certificate authority (CA) that vouches for the authenticity of the certificate, and the server's public encryption key.\nThe client confirms the validity of the certificate before proceeding.\nTo generate the session keys used for the secure connection, the client either: encrypts a random number (PreMasterSecret) with the server's public key and sends the result to the server (which only the server should be able to decrypt with its private key); both parties then use the random number to generate a unique session key for subsequent encryption and decryption of data during the session, or\nuses Diffie\u2013Hellman key exchange (or its variant elliptic-curve DH) to securely generate a random and unique session key for encryption and decryption that has the additional property of forward secrecy: if the server's private key is disclosed in future, it cannot be used to decrypt the current session, even if the session is intercepted and recorded by a third party.\nThis concludes the handshake and begins the secured connection, which is encrypted and decrypted with the session key until the connection closes. If any",
            "link": "https://en.wikipedia.org/wiki/Transport_Layer_Security",
            "snippet": "Transport Layer Security (TLS) is a cryptographic protocol designed to provide communications security over a computer network, such as the Internet. The protocol is widely used in applications such as email, instant messaging, and voice over IP, but its use in securing HTTPS remains the most publicly visible.\nThe TLS protocol aims primarily to provide security, including privacy (confidentiality), integrity, and authenticity through the use of cryptography, such as the use of certificates, between two or more communicating computer applications. It runs in the presentation layer and is itself composed of two layers: the TLS record and the TLS handshake protocols.\nThe closely related Datagram Transport Layer Security (DTLS) is a communications protocol that provides security to datagram-based applications. In technical writing, references to \"(D)TLS\" are often seen when it applies to both versions.\nTLS is a proposed Internet Engineering Task Force (IETF) standard, first defined in 1999, and the current version is TLS 1.3, defined in August 2018. TLS builds on the now-deprecated SSL (Secure Sockets Layer) specifications (1994, 1995, 1996) developed by Netscape Communications for adding the HTTPS protocol to their Netscape Navigator web browser."
        },
        {
            "id": "c_a16_0",
            "title": "Block cipher",
            "text": "In cryptography, a block cipher is a deterministic algorithm that operates on fixed-length groups of bits, called blocks. Block ciphers are the elementary building blocks of many cryptographic protocols. They are ubiquitous in the storage and exchange of data, where such data is secured and authenticated via encryption.\nA block cipher uses blocks as an unvarying transformation. Even a secure block cipher is suitable for the encryption of only a single block of data at a time, using a fixed key. A multitude of modes of operation have been designed to allow their repeated use in a secure way to achieve the security goals of confidentiality and authenticity. However, block ciphers may also feature as building blocks in other cryptographic protocols, such as universal hash functions and pseudorandom number generators.\n\n\n== Definition == A block cipher consists of two paired algorithms, one for encryption, E, and the other for decryption, D. Both algorithms accept two inputs: an input block of size n bits and a key of size k bits; and both yield an n-bit output block. The decryption algorithm D is defined to be the inverse function of encryption, i.e., D = E\u22121. More formally, a block cipher is specified by an encryption function E\n          \n            K\n          \n        \n        (\n        P\n        )\n        :=\n        E\n        (\n        K\n        ,\n        P\n        )\n        :\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            k\n          \n        \n        \u00d7\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            n\n          \n        \n        \u2192\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle E_{K}(P):=E(K,P):\\{0,1\\}^{k}\\times \\{0,1\\}^{n}\\rightarrow \\{0,1\\}^{n},}\n  \n\nwhich takes as input a key K, of bit length k (called the key size), and a bit string P, of length n (called the block size), and returns a string C of n bits. P is called the plaintext, and C is termed the ciphertext. For each K, the function EK(P) is required to be an invertible mapping on {0,1}n. The inverse for E is defined as a function E\n          \n            K\n          \n          \n            \u2212\n            1\n          \n        \n        (\n        C\n        )\n        :=\n        \n          D\n          \n            K\n          \n        \n        (\n        C\n        )\n        =\n        D\n        (\n        K\n        ,\n        C\n        )\n        :\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            k\n          \n        \n        \u00d7\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            n\n          \n        \n        \u2192\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            n\n          \n        \n        ,\n      \n    \n    {\\displaystyle E_{K}^{-1}(C):=D_{K}(C)=D(K,C):\\{0,1\\}^{k}\\times \\{0,1\\}^{n}\\rightarrow \\{0,1\\}^{n},}\n  \n\ntaking a key K and a ciphertext C to return a plaintext value P, such that \u2200\n        P\n        :\n        \n          D\n          \n            K\n          \n        \n        (\n        \n          E\n          \n            K\n          \n        \n        (\n        P\n        )\n        )\n        =\n        P\n        .\n      \n    \n    {\\displaystyle \\forall P:D_{K}(E_{K}(P))=P.}\n  \n\nFor example, a block cipher encryption algorithm might take a 128-bit block of plaintext as input, and output a corresponding 128-bit block of ciphertext. The exact transformation is controlled using a second input \u2013 the secret key. Decryption is similar: the decryption algorithm takes, in this example, a 128-bit block of ciphertext together with the secret key, and yields the original 128-bit block of plain text.\nFor each key K, EK is a permutation (a bijective mapping) over the set of input blocks. Each key selects one permutation from the",
            "link": "https://en.wikipedia.org/wiki/Block_cipher",
            "snippet": "In cryptography, a block cipher is a deterministic algorithm that operates on fixed-length groups of bits, called blocks. Block ciphers are the elementary building blocks of many cryptographic protocols. They are ubiquitous in the storage and exchange of data, where such data is secured and authenticated via encryption.\nA block cipher uses blocks as an unvarying transformation. Even a secure block cipher is suitable for the encryption of only a single block of data at a time, using a fixed key. A multitude of modes of operation have been designed to allow their repeated use in a secure way to achieve the security goals of confidentiality and authenticity. However, block ciphers may also feature as building blocks in other cryptographic protocols, such as universal hash functions and pseudorandom number generators."
        },
        {
            "id": "c_a16_1",
            "title": "Quantum key distribution",
            "text": "Quantum key distribution (QKD) is a secure communication method that implements a cryptographic protocol involving components of quantum mechanics. It enables two parties to produce a shared random secret key known only to them, which then can be used to encrypt and decrypt messages. The process of quantum key distribution is not to be confused with quantum cryptography, as it is the best-known example of a quantum-cryptographic task. An important and unique property of quantum key distribution is the ability of the two communicating users to detect the presence of any third party trying to gain knowledge of the key. This results from a fundamental aspect of quantum mechanics: the process of measuring a quantum system in general disturbs the system. A third party trying to eavesdrop on the key must in some way measure it, thus introducing detectable anomalies. By using quantum superpositions or quantum entanglement and transmitting information in quantum states, a communication system can be implemented that detects eavesdropping. If the level of eavesdropping is below a certain threshold, a key can be produced that is guaranteed to be secure (i.e., the eavesdropper has no information about it). Otherwise no secure key is possible, and communication is aborted. The security of encryption that uses quantum key distribution relies on the foundations of quantum mechanics, in contrast to traditional public key cryptography, which relies on the computational difficulty of certain mathematical functions, and cannot provide any mathematical proof as to the actual complexity of reversing the one-way functions used. QKD has provable security based on information theory, and forward secrecy. The main drawback of quantum-key distribution is that it usually relies on having an authenticated classical channel of communication. In modern cryptography, having an authenticated classical channel means that one already has exchanged either a symmetric key of sufficient length or public keys of sufficient security level. With such information already available, in practice one can achieve authenticated and sufficiently secure communication without using QKD, such as by using the Galois/Counter Mode of the Advanced Encryption Standard. Thus QKD does the work of a stream cipher at many times the cost. Quantum key distribution is used to produce and distribute only a key, not to transmit any message data. This key can then be used with any chosen encryption algorithm to encrypt (and decrypt) a message, which can then be transmitted over a standard communication channel. The algorithm most commonly associated with QKD is the one-time pad, as it is provably secure when used with a secret, random key. In real-world situations, it is often also used with encryption using symmetric key algorithms like the Advanced Encryption Standard algorithm. == Quantum key exchange ==\nQuantum communication involves encoding information in quantum states, or qubits, as opposed to classical communication's use of bits. Usually, photons are used for these quantum states. Quantum key distribution exploits certain properties of these quantum states to ensure its security. There are several different approaches to quantum key distribution, but they can be divided into two main categories depending on which property they exploit. Prepare and measure protocols\nIn contrast to classical physics, the act of measurement is an integral part of quantum mechanics. In general, measuring an unknown quantum state changes that state in some way. This is a consequence of quantum indeterminacy and can be exploited in order to detect any eavesdropping on communication (which necessarily involves measurement) and, more importantly, to calculate the amount of information that has been intercepted.\nEntanglement based protocols\nThe quantum states of two (or more) separate objects can become linked together in such a way that they must be described by a combined quantum state, not as indivi",
            "link": "https://en.wikipedia.org/wiki/Quantum_key_distribution",
            "snippet": "Quantum key distribution (QKD) is a secure communication method that implements a cryptographic protocol involving components of quantum mechanics. It enables two parties to produce a shared random secret key known only to them, which then can be used to encrypt and decrypt messages. The process of quantum key distribution is not to be confused with quantum cryptography, as it is the best-known example of a quantum-cryptographic task.\nAn important and unique property of quantum key distribution is the ability of the two communicating users to detect the presence of any third party trying to gain knowledge of the key. This results from a fundamental aspect of quantum mechanics: the process of measuring a quantum system in general disturbs the system. A third party trying to eavesdrop on the key must in some way measure it, thus introducing detectable anomalies. By using quantum superpositions or quantum entanglement and transmitting information in quantum states, a communication system can be implemented that detects eavesdropping. If the level of eavesdropping is below a certain threshold, a key can be produced that is guaranteed to be secure (i.e., the eavesdropper has no information about it). Otherwise no secure key is possible, and communication is aborted.\nThe security of encryption that uses quantum key distribution relies on the foundations of quantum mechanics, in contrast to traditional public key cryptography, which relies on the computational difficulty of certain mathematical functions, and cannot provide any mathematical proof as to the actual complexity of reversing the one-way functions used. QKD has provable security based on information theory, and forward secrecy.\nThe main drawback of quantum-key distribution is that it usually relies on having an authenticated classical channel of communication. In modern cryptography, having an authenticated classical channel means that one already has exchanged either a symmetric key of sufficient length or public keys of sufficient security level. With such information already available, in practice one can achieve authenticated and sufficiently secure communication without using QKD, such as by using the Galois/Counter Mode of the Advanced Encryption Standard. Thus QKD does the work of a stream cipher at many times the cost.\nQuantum key distribution is used to produce and distribute only a key, not to transmit any message data. This key can then be used with any chosen encryption algorithm to encrypt (and decrypt) a message, which can then be transmitted over a standard communication channel. The algorithm most commonly associated with QKD is the one-time pad, as it is provably secure when used with a secret, random key. In real-world situations, it is often also used with encryption using symmetric key algorithms like the Advanced Encryption Standard algorithm."
        },
        {
            "id": "c_a16_2",
            "title": "Blockchain",
            "text": "A blockchain is a distributed ledger with growing lists of records (blocks) that are securely linked together via cryptographic hashes. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree, where data nodes are represented by leaves). Since each block contains information about the previous block, they effectively form a chain (compare linked list data structure), with each additional block linking to the ones before it. Consequently, blockchain transactions are resistant to alteration because, once recorded, the data in any given block cannot be changed retroactively without altering all subsequent blocks and obtaining network consensus to accept these changes. This protects blockchains against nefarious activities such as creating assets \"out of thin air\", double-spending, counterfeiting, fraud, and theft. Blockchains are typically managed by a peer-to-peer (P2P) computer network for use as a public distributed ledger, where nodes collectively adhere to a consensus algorithm protocol to add and validate new transaction blocks. Although blockchain records are not unalterable, since blockchain forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance. A blockchain was created by a person (or group of people) using the name (or pseudonym) Satoshi Nakamoto in 2008 to serve as the public distributed ledger for bitcoin cryptocurrency transactions, based on previous work by Stuart Haber, W. Scott Stornetta, and Dave Bayer. The implementation of the blockchain within bitcoin made it the first digital currency to solve the double-spending problem without the need for a trusted authority or central server. The bitcoin design has inspired other applications and blockchains that are readable by the public and are widely used by cryptocurrencies. The blockchain may be considered a type of payment rail.\nPrivate blockchains have been proposed for business use. Computerworld called the marketing of such privatized blockchains without a proper security model \"snake oil\"; however, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones. == History ==\nCryptographer David Chaum first proposed a blockchain-like protocol in his 1982 dissertation \"Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups\". Further work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta. They wanted to implement a system wherein document timestamps could not be tampered with. In 1992, Haber, Stornetta, and Dave Bayer incorporated Merkle trees into the design, which improved its efficiency by allowing several document certificates to be collected into one block. Under their company Surety, their document certificate hashes have been published in The New York Times every week since 1995. The first decentralized blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to timestamp blocks without requiring them to be signed by a trusted party and introducing a difficulty parameter to stabilize the rate at which blocks are added to the chain. The design was implemented the following year by Nakamoto as a core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network.\nIn August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes). In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size. The ledger size had exceeded 200 GB by early 2020.\nThe words block and chain w",
            "link": "https://en.wikipedia.org/wiki/Blockchain",
            "snippet": "A blockchain is a distributed ledger with growing lists of records (blocks) that are securely linked together via cryptographic hashes. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree, where data nodes are represented by leaves). Since each block contains information about the previous block, they effectively form a chain (compare linked list data structure), with each additional block linking to the ones before it. Consequently, blockchain transactions are resistant to alteration because, once recorded, the data in any given block cannot be changed retroactively without altering all subsequent blocks and obtaining network consensus to accept these changes. This protects blockchains against nefarious activities such as creating assets \"out of thin air\", double-spending, counterfeiting, fraud, and theft.\nBlockchains are typically managed by a peer-to-peer (P2P) computer network for use as a public distributed ledger, where nodes collectively adhere to a consensus algorithm protocol to add and validate new transaction blocks. Although blockchain records are not unalterable, since blockchain forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance.\nA blockchain was created by a person (or group of people) using the name (or pseudonym) Satoshi Nakamoto in 2008 to serve as the public distributed ledger for bitcoin cryptocurrency transactions, based on previous work by Stuart Haber, W. Scott Stornetta, and Dave Bayer. The implementation of the blockchain within bitcoin made it the first digital currency to solve the double-spending problem without the need for a trusted authority or central server. The bitcoin design has inspired other applications and blockchains that are readable by the public and are widely used by cryptocurrencies. The blockchain may be considered a type of payment rail.\nPrivate blockchains have been proposed for business use. Computerworld called the marketing of such privatized blockchains without a proper security model \"snake oil\"; however, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones."
        },
        {
            "id": "c_a17_0",
            "title": "SHA-2",
            "text": "SHA-2 (Secure Hash Algorithm 2) is a set of cryptographic hash functions designed by the United States National Security Agency (NSA) and first published in 2001. They are built using the Merkle\u2013Damg\u00e5rd construction, from a one-way compression function itself built using the Davies\u2013Meyer structure from a specialized block cipher. SHA-2 includes significant changes from its predecessor, SHA-1. The SHA-2 family consists of six hash functions with digests (hash values) that are 224, 256, 384 or 512 bits: SHA-224, SHA-256, SHA-384, SHA-512, SHA-512/224, SHA-512/256. SHA-256 and SHA-512 are hash functions whose digests are eight 32-bit and 64-bit words, respectively. They use different shift amounts and additive constants, but their structures are otherwise virtually identical, differing only in the number of rounds. SHA-224 and SHA-384 are truncated versions of SHA-256 and SHA-512 respectively, computed with different initial values. SHA-512/224 and SHA-512/256 are also truncated versions of SHA-512, but the initial values are generated using the method described in Federal Information Processing Standards (FIPS) PUB 180-4. SHA-2 was first published by the National Institute of Standards and Technology (NIST) as a U.S. federal standard. The SHA-2 family of algorithms are patented in the U.S. The United States has released the patent under a royalty-free license.\nAs of 2011, the best public attacks break preimage resistance for 52 out of 64 rounds of SHA-256 or 57 out of 80 rounds of SHA-512, and collision resistance for 46 out of 64 rounds of SHA-256. == Hash standard == With the publication of FIPS PUB 180-2, NIST added three additional hash functions in the SHA family. The algorithms are collectively known as SHA-2, named after their digest lengths (in bits): SHA-256, SHA-384, and SHA-512.\nThe algorithms were first published in 2001 in the draft FIPS PUB 180-2, at which time public review and comments were accepted. In August 2002, FIPS PUB 180-2 became the new Secure Hash Standard, replacing FIPS PUB 180-1, which was released in April 1995. The updated standard included the original SHA-1 algorithm, with updated technical notation consistent with that describing the inner workings of the SHA-2 family. In February 2004, a change notice was published for FIPS PUB 180-2, specifying an additional variant, SHA-224, defined to match the key length of two-key Triple DES. In October 2008, the standard was updated in FIPS PUB 180-3, including SHA-224 from the change notice, but otherwise making no fundamental changes to the standard. The primary motivation for updating the standard was relocating security information about the hash algorithms and recommendations for their use to Special Publications 800-107 and 800-57. Detailed test data and example message digests were also removed from the standard, and provided as separate documents. In January 2011, NIST published SP800-131A, which specified a move from the then-current minimum of 80-bit security (provided by SHA-1) allowable for federal government use until the end of 2013, to 112-bit security (provided by SHA-2) being both the minimum requirement (starting in 2014) and the recommended security level (starting from the publication date in 2011).\nIn March 2012, the standard was updated in FIPS PUB 180-4, adding the hash functions SHA-512/224 and SHA-512/256, and describing a method for generating initial values for truncated versions of SHA-512. Additionally, a restriction on padding the input data prior to hash calculation was removed, allowing hash data to be calculated simultaneously with content generation, such as a real-time video or audio feed. Padding the final data block must still occur prior to hash output. In July 2012, NIST revised SP800-57, which provides guidance for cryptographic key management. The publication disallowed creation of digital signatures with a hash security lower than 112 bits after 2013. The previous revision from 2007 specified the cutoff to be t",
            "link": "https://en.wikipedia.org/wiki/SHA-2",
            "snippet": "SHA-2 (Secure Hash Algorithm 2) is a set of cryptographic hash functions designed by the United States National Security Agency (NSA) and first published in 2001. They are built using the Merkle\u2013Damg\u00e5rd construction, from a one-way compression function itself built using the Davies\u2013Meyer structure from a specialized block cipher.\nSHA-2 includes significant changes from its predecessor, SHA-1. The SHA-2 family consists of six hash functions with digests (hash values) that are 224, 256, 384 or 512 bits: SHA-224, SHA-256, SHA-384, SHA-512, SHA-512/224, SHA-512/256. SHA-256 and SHA-512 are hash functions whose digests are eight 32-bit and 64-bit words, respectively. They use different shift amounts and additive constants, but their structures are otherwise virtually identical, differing only in the number of rounds. SHA-224 and SHA-384 are truncated versions of SHA-256 and SHA-512 respectively, computed with different initial values. SHA-512/224 and SHA-512/256 are also truncated versions of SHA-512, but the initial values are generated using the method described in Federal Information Processing Standards (FIPS) PUB 180-4.\nSHA-2 was first published by the National Institute of Standards and Technology (NIST) as a U.S. federal standard. The SHA-2 family of algorithms are patented in the U.S. The United States has released the patent under a royalty-free license.\nAs of 2011, the best public attacks break preimage resistance for 52 out of 64 rounds of SHA-256 or 57 out of 80 rounds of SHA-512, and collision resistance for 46 out of 64 rounds of SHA-256."
        },
        {
            "id": "c_a17_1",
            "title": "History of cryptography",
            "text": "Cryptography, the use of codes and ciphers to protect secrets, began thousands of years ago. Until recent decades, it has been the story of what might be called classical cryptography \u2014 that is, of methods of encryption that use pen and paper, or perhaps simple mechanical aids. In the early 20th century, the invention of complex mechanical and electromechanical machines, such as the Enigma rotor machine, provided more sophisticated and efficient means of encryption; and the subsequent introduction of electronics and computing has allowed elaborate schemes of still greater complexity, most of which are entirely unsuited to pen and paper. The development of cryptography has been paralleled by the development of cryptanalysis \u2014  the \"breaking\" of codes and ciphers. The discovery and application, early on, of frequency analysis to the reading of encrypted communications has, on occasion, altered the course of history.  Thus the Zimmermann Telegram triggered the United States' entry into World War I; and Allies reading of Nazi Germany's ciphers shortened World War II, in some evaluations by as much as two years.\nUntil the 1960s, secure cryptography was largely the preserve of governments. Two events have since brought it squarely into the public domain: the creation of a public encryption standard (DES), and the invention of public-key cryptography. == Antiquity == The earliest known use of cryptography is found in non-standard hieroglyphs carved into the wall of a tomb from the Old Kingdom of Egypt circa 1900 BC. These are not thought to be serious attempts at secret communications, however, but rather to have been attempts at mystery, intrigue, or even amusement for literate onlookers.\nSome clay tablets from Mesopotamia somewhat later are clearly meant to protect information\u2014one dated near 1500 BC was found to encrypt a craftsman's recipe for pottery glaze, presumably commercially valuable. Furthermore, Hebrew scholars made use of simple monoalphabetic substitution ciphers (such as the Atbash cipher) beginning perhaps around 600 to 500 BC. In India around 400 BC to 200 AD, Mlecchita vikalpa or \"the art of understanding writing in cypher, and the writing of words in a peculiar way\" was documented in the Kama Sutra for the purpose of communication between lovers. This was also likely a simple substitution cipher. Parts of the Egyptian demotic Greek Magical Papyri were written in a cypher script. The ancient Greeks are said to have known of ciphers. The scytale transposition cipher was used by the Spartan military, but it is not definitively known whether the scytale was for encryption, authentication, or avoiding bad omens in speech. Herodotus tells us of secret messages physically concealed beneath wax on wooden tablets or as a tattoo on a slave's head concealed by regrown hair, although these are not properly examples of cryptography per se as the message, once known, is directly readable; this is known as steganography. Another Greek method was developed by Polybius (now called the \"Polybius Square\"). The Romans knew something of cryptography (e.g., the Caesar cipher and its variations). == Medieval cryptography ==\n\nDavid Kahn notes in The Codebreakers that modern cryptology originated among the Arabs, the first people to systematically document cryptanalytic methods. Al-Khalil (717\u2013786) wrote the Book of Cryptographic Messages, which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels.\nThe invention of the frequency analysis technique for breaking monoalphabetic substitution ciphers, by Al-Kindi, an Arab mathematician, sometime around AD 800, proved to be the single most significant cryptanalytic advance until World War II. Al-Kindi wrote a book on cryptography entitled Risalah fi Istikhraj al-Mu'amma (Manuscript for the Deciphering Cryptographic Messages), in which he described the first cryptanalytic techniques, including some for polyalphabetic ciphers, ciphe",
            "link": "https://en.wikipedia.org/wiki/History_of_cryptography",
            "snippet": "Cryptography, the use of codes and ciphers to protect secrets, began thousands of years ago. Until recent decades, it has been the story of what might be called classical cryptography \u2014 that is, of methods of encryption that use pen and paper, or perhaps simple mechanical aids. In the early 20th century, the invention of complex mechanical and electromechanical machines, such as the Enigma rotor machine, provided more sophisticated and efficient means of encryption; and the subsequent introduction of electronics and computing has allowed elaborate schemes of still greater complexity, most of which are entirely unsuited to pen and paper.\nThe development of cryptography has been paralleled by the development of cryptanalysis \u2014  the \"breaking\" of codes and ciphers. The discovery and application, early on, of frequency analysis to the reading of encrypted communications has, on occasion, altered the course of history.  Thus the Zimmermann Telegram triggered the United States' entry into World War I; and Allies reading of Nazi Germany's ciphers shortened World War II, in some evaluations by as much as two years.\nUntil the 1960s, secure cryptography was largely the preserve of governments. Two events have since brought it squarely into the public domain: the creation of a public encryption standard (DES), and the invention of public-key cryptography."
        },
        {
            "id": "c_a17_2",
            "title": "Diffie\u2013Hellman key exchange",
            "text": "Diffie\u2013Hellman (DH) key exchange is a mathematical method of securely generating a symmetric cryptographic key over a public channel and was one of the first public-key protocols as conceived by Ralph Merkle and named after Whitfield Diffie and Martin Hellman. DH is one of the earliest practical examples of public key exchange implemented within the field of cryptography. Published in 1976 by Diffie and Hellman, this is the earliest publicly known work that proposed the idea of a private key and a corresponding public key.\nTraditionally, secure encrypted communication between two parties required that they first exchange keys by some secure physical means, such as paper key lists transported by a trusted courier. The Diffie\u2013Hellman key exchange method allows two parties that have no prior knowledge of each other to jointly establish a shared secret key over an insecure channel. This key can then be used to encrypt subsequent communications using a symmetric-key cipher. Diffie\u2013Hellman is used to secure a variety of Internet services. However, research published in October 2015 suggests that the parameters in use for many DH Internet applications at that time are not strong enough to prevent compromise by very well-funded attackers, such as the security services of some countries.\nThe scheme was published by Whitfield Diffie and Martin Hellman in 1976, but in 1997 it was revealed that James H. Ellis, Clifford Cocks, and Malcolm J. Williamson of GCHQ, the British signals intelligence agency, had previously shown in 1969 how public-key cryptography could be achieved.\nAlthough Diffie\u2013Hellman key exchange itself is a non-authenticated key-agreement protocol, it provides the basis for a variety of authenticated protocols, and is used to provide forward secrecy in Transport Layer Security's ephemeral modes (referred to as EDH or DHE depending on the cipher suite). The method was followed shortly afterwards by RSA, an implementation of public-key cryptography using asymmetric algorithms.\nExpired US patent 4,200,770 from 1977 describes the now public-domain algorithm. It credits Hellman, Diffie, and Merkle as inventors. == Name ==\nIn 2006, Hellman suggested the algorithm be called Diffie\u2013Hellman\u2013Merkle key exchange in recognition of Ralph Merkle's contribution to the invention of public-key cryptography (Hellman, 2006), writing:\n\nThe system...has since become known as Diffie\u2013Hellman key exchange. While that system was first described in a paper by Diffie and me, it is a public key distribution system, a concept developed by Merkle, and hence should be called 'Diffie\u2013Hellman\u2013Merkle key exchange' if names are to be associated with it. I hope this small pulpit might help in that endeavor to recognize Merkle's equal contribution to the invention of public key cryptography.\n\n\n== Description ==\n\n\n=== General overview === Diffie\u2013Hellman key exchange establishes a shared secret between two parties that can be used for secret communication for exchanging data over a public network. An analogy illustrates the concept of public key exchange by using colors instead of very large numbers: The process begins by having the two parties, Alice and Bob, publicly agree on an arbitrary starting color that does not need to be kept secret. In this example, the color is yellow. Each person also selects a secret color that they keep to themselves \u2013 in this case, red and cyan. The crucial part of the process is that Alice and Bob each mix their own secret color together with their mutually shared color, resulting in orange-tan and light-blue mixtures respectively, and then publicly exchange the two mixed colors. Finally, each of them mixes the color they received from the partner with their own private color. The result is a final color mixture (yellow-brown in this case) that is identical to their partner's final color mixture.\nIf a third party listened to the exchange, they would only know the common color (yellow) and the first mixed colors (orange-tan an",
            "link": "https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange",
            "snippet": "Diffie\u2013Hellman (DH) key exchange is a mathematical method of securely generating a symmetric cryptographic key over a public channel and was one of the first public-key protocols as conceived by Ralph Merkle and named after Whitfield Diffie and Martin Hellman. DH is one of the earliest practical examples of public key exchange implemented within the field of cryptography. Published in 1976 by Diffie and Hellman, this is the earliest publicly known work that proposed the idea of a private key and a corresponding public key.\nTraditionally, secure encrypted communication between two parties required that they first exchange keys by some secure physical means, such as paper key lists transported by a trusted courier. The Diffie\u2013Hellman key exchange method allows two parties that have no prior knowledge of each other to jointly establish a shared secret key over an insecure channel. This key can then be used to encrypt subsequent communications using a symmetric-key cipher.\nDiffie\u2013Hellman is used to secure a variety of Internet services. However, research published in October 2015 suggests that the parameters in use for many DH Internet applications at that time are not strong enough to prevent compromise by very well-funded attackers, such as the security services of some countries.\nThe scheme was published by Whitfield Diffie and Martin Hellman in 1976, but in 1997 it was revealed that James H. Ellis, Clifford Cocks, and Malcolm J. Williamson of GCHQ, the British signals intelligence agency, had previously shown in 1969 how public-key cryptography could be achieved.\nAlthough Diffie\u2013Hellman key exchange itself is a non-authenticated key-agreement protocol, it provides the basis for a variety of authenticated protocols, and is used to provide forward secrecy in Transport Layer Security's ephemeral modes (referred to as EDH or DHE depending on the cipher suite).\nThe method was followed shortly afterwards by RSA, an implementation of public-key cryptography using asymmetric algorithms.\nExpired US patent 4,200,770 from 1977 describes the now public-domain algorithm. It credits Hellman, Diffie, and Merkle as inventors.\n\n"
        }
    ]
}