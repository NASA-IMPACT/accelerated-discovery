import asyncio
import networkx as nx
import os
from typing import List, Tuple

from pydantic.fields import Field

from akd._base import InputSchema, OutputSchema
from akd.agents._base import BaseAgent, BaseAgentConfig

from akd.tools.scrapers import (
    DoclingScraper,
    OmniScraperInputSchema,
    DoclingScraperConfig
)
from akd.tools.search import (SemanticScholarSearchTool, 
                              SemanticScholarSearchToolInputSchema,
                              SemanticScholarSearchToolConfig)
from akd.structures import SearchResultItem, PaperDataItem

from langchain_openai import ChatOpenAI

from .parsing_utils import parse_html, group_section_titles, create_sections_from_parsed_html
from .graph_utils import add_paper_to_graph, select_nodes, generate_final_answer
from .structures import ParsedPaper

 
class GapInputSchema(InputSchema):
    """Input schema for gap agent"""
    search_results: List[SearchResultItem] = Field(..., description="List of SearchResultItems.")
    gap: str = Field(..., description="Type of gap to investigate.")
    # pre-selected gap, or user-defined


class GapOutputSchema(OutputSchema):
    """Output schema for gap agent"""
    output: str = Field(..., description="Final output generated by the agent.")


class GapAgentConfig(BaseAgentConfig):
    """Configuration for Gap Agent"""
    docling_config: DoclingScraperConfig = Field(..., 
                                                         description="Configuration for Docling Scraper.")
    
    s2_tool_config: SemanticScholarSearchToolConfig = Field(..., 
                                                         description="Configuration for S2 Tool.")



class GapAgent(BaseAgent):
    input_schema = GapInputSchema
    output_schema = GapOutputSchema
    config_schema = GapAgentConfig


    def _post_init(
        self,
    ) -> None:
        super()._post_init()
        docling_config = self.config.docling_config if self.config.docling_config else DoclingScraperConfig(do_table_structure=True, 
                                                                                                                            pdf_mode='accurate', 
                                                                                                                            export_type='html', 
                                                                                                                            debug=False)
        s2_tool_config = self.config.s2_tool_config if self.config.s2_tool_config else SemanticScholarSearchToolConfig(debug=False,
                                                                                                                  external_id="ARXIV",
                                                                                                                  fields = ["paperId", "externalIds", "url", "title", "abstract", "year", "authors", "isOpenAccess", "openAccessPdf"])
        self.docling_scraper = DoclingScraper(docling_config)
        self.semantic_search_tool = SemanticScholarSearchTool(s2_tool_config)

        self.llm = ChatOpenAI(model_name=self.config.model_name,
                              temperature=self.config.temperature,
                              api_key=self.config.api_key)


    async def _fetch_paper_items(self,
                                 search_results: List[SearchResultItem]) -> List[PaperDataItem]:
        """
        Fetches paper metadata using arXiv IDs extracted from a list of search results.
        This function is restricted to Arxiv URLs for now.

        Args:
            search_results (List[SearchResultItem]): A list of search result items containing URLs to arXiv papers.

        Returns:
            List[PaperDataItem]: A list of paper data items retrieved from the Semantic Scholar tool.
        """
        arxiv_ids = [res.url.path.split("/")[-1].split("v")[0] for res in search_results]
        paper_items = await self.semantic_search_tool.fetch_paper_by_external_id(SemanticScholarSearchToolInputSchema(queries=arxiv_ids))   
        fetched_paper_ids = [paper_item.external_id for paper_item in paper_items]
        skipped_ids = (set(arxiv_ids)) - set(fetched_paper_ids)
        search_results = [search_results[i] for i in range(len(search_results)) if arxiv_ids[i] not in skipped_ids] 
        return paper_items, search_results


    async def _fetch_parsed_pdfs(self, search_results: List[SearchResultItem]) -> List[str]:
        """
        Parses PDFs from search result items using Docling.

        Args:
            search_results (List[SearchResultItem]): A list of search result items containing PDF URLs.

        Returns:
            List[str]: A list of parsed PDF contents as strings.
        """
        pdf_urls = [res.pdf_url for res in search_results if res is not None]
        tasks = [
            self.docling_scraper.arun(OmniScraperInputSchema(url=url))
            for url in pdf_urls
        ]
        results = await asyncio.gather(*tasks)
        parsed_pdfs = [res.content for res in results]
        return parsed_pdfs


    async def _create_parsed_paper(self, parsed_pdf: str, paper_item: PaperDataItem) -> ParsedPaper:
        """
        Converts parsed HTML content of a paper and its metadata into a structured ParsedPaper object.

        Args:
            parsed_pdf (str): The HTML content extracted and parsed from the paper's PDF.
            paper_item (PaperDataItem): PaperDataItem of the parsed_pdf.

        Returns:
            ParsedPaper: A structured representation of the paper including section titles, content, and metadata.
        """
        parsed_content = parse_html(parsed_pdf)
        section_titles = await group_section_titles(parsed_content, self.llm)
        sections, clean_section_titles = create_sections_from_parsed_html(parsed_content, 
                                                                        section_titles, 
                                                                        paper_title=paper_item.title)
        parsed_paper = ParsedPaper(section_titles=clean_section_titles, sections=sections, **paper_item.model_dump())
        return parsed_paper


    async def _fetch_parsed_papers(self, parsed_pdfs: list[str], paper_items: List[PaperDataItem]) -> List[ParsedPaper]:
        """
        Creates structured ParsedPaper objects from parsed PDF content and corresponding paper metadata.

        Args:
            parsed_pdfs (list[str]): A list of HTML strings parsed from the original paper PDFs.
            paper_items (List[PaperDataItem]): A list of PaperDataItem objects corresponding to each parsed PDF.

        Returns:
            List[ParsedPaper]: A list of structured ParsedPaper objects containing organized content and metadata.
        """
        tasks = [
            self._create_parsed_paper(parsed_pdf, paper_item)
            for parsed_pdf, paper_item in zip(parsed_pdfs, paper_items)
        ]
        parsed_papers = await asyncio.gather(*tasks)
        return parsed_papers


    async def create_graph(self, parsed_papers: List[ParsedPaper]) -> nx.Graph:
        """
        Constructs a NetworkX graph from a list of parsed papers by adding their structured content and metadata.

        Args:
            parsed_papers (List[ParsedPaper]): A list of ParsedPaper objects containing sectioned content and metadata.

        Returns:
            nx.Graph: A graph where nodes and edges represent the structure and relationships within and between papers.
        """
        G = nx.Graph()
        for paper in parsed_papers:
            G = await add_paper_to_graph(G, paper, self.llm)
        return G


    async def get_response_async(
        self,
        params: GapInputSchema,
        **kwargs,
    ) -> GapOutputSchema:
        """
        Obtains a response from the language model asynchronously.

        Args:
            response_model (Optional[OutputSchema]):
                The schema for the response data. If not set,
                self.output_schema is used.

        Returns:
            OutputSchema: The response from the language model.
        """
        search_results = params.search_results
        gap = params.gap
        paper_items, search_results = await self._fetch_paper_items(search_results)
        parsed_pdfs = await self._fetch_parsed_pdfs(search_results=search_results)
        parsed_papers = await self._fetch_parsed_papers(parsed_pdfs, paper_items)
        G = await self.create_graph(parsed_papers=parsed_papers)
        selected_nodes = await select_nodes(G, query=gap, llm=self.llm)
        output = await generate_final_answer(G, query=gap, all_selected_nodes=selected_nodes, llm=self.llm)
        return GapOutputSchema(output=output.content)


    async def _arun(self, params: GapInputSchema, **kwargs) -> GapOutputSchema:
        return await self.get_response_async(params, **kwargs)
