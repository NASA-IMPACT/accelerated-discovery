import asyncio
from typing import Dict, List, Tuple

import networkx as nx
from langchain_openai import ChatOpenAI
from loguru import logger
from pydantic.fields import Field

from akd._base import InputSchema, OutputSchema
from akd.agents._base import BaseAgent, BaseAgentConfig
from akd.structures import PaperDataItem, SearchResultItem
from akd.tools.scrapers import (
    DoclingScraper,
    DoclingScraperConfig,
    OmniScraperInputSchema,
)
from akd.tools.search import (
    SemanticScholarSearchTool,
    SemanticScholarSearchToolConfig,
    SemanticScholarSearchToolInputSchema,
)

from .graph_utils import add_paper_to_graph, generate_final_answer, select_nodes
from .parsing_utils import (
    create_sections_from_parsed_html,
    group_section_titles,
    parse_html,
)
from .prompts import gap_query_map
from .structures import ParsedPaper


class GapInputSchema(InputSchema):
    """Input schema for gap agent"""

    search_results: List[SearchResultItem] = Field(
        ...,
        description="List of SearchResultItems.",
    )
    gap: str = Field(..., description="Type of gap to investigate.")


class GapOutputSchema(OutputSchema):
    """Output schema for gap agent"""

    model_config = {"arbitrary_types_allowed": True}
    output: str = Field(..., description="Final output generated by the agent.")
    attributed_source_answers: Dict = Field(
        ...,
        description="Answers for each node selected from the graph.",
    )
    G: nx.Graph = Field(..., description="Graph created from the ingested papers.")


class GapAgentConfig(BaseAgentConfig):
    """Configuration for Gap Agent"""

    docling_config: DoclingScraperConfig = Field(
        default_factory=lambda: DoclingScraperConfig(
            do_table_structure=True,
            pdf_mode="accurate",
            export_type="html",
            debug=False,
        ),
        description="Configuration for Docling Scraper.",
    )
    s2_tool_config: SemanticScholarSearchToolConfig = Field(
        default_factory=lambda: SemanticScholarSearchToolConfig(
            debug=False,
            external_id="ARXIV",
            fields=[
                "paperId",
                "externalIds",
                "url",
                "title",
                "abstract",
                "year",
                "authors",
                "isOpenAccess",
                "openAccessPdf",
            ],
        ),
        description="Configuration for S2 Tool.",
    )


class GapAgent(BaseAgent):
    input_schema = GapInputSchema
    output_schema = GapOutputSchema
    config_schema = GapAgentConfig

    def _post_init(
        self,
    ) -> None:
        super()._post_init()
        self.docling_scraper = DoclingScraper(self.config.docling_config)
        self.semantic_search_tool = SemanticScholarSearchTool(
            self.config.s2_tool_config,
        )

        self.llm = ChatOpenAI(
            model_name=self.config.model_name,
            temperature=self.config.temperature,
            api_key=self.config.api_key,
        )

    async def _fetch_paper_items(
        self,
        search_results: List[SearchResultItem],
    ) -> Tuple[list[PaperDataItem], list[SearchResultItem]]:
        """
        Fetches paper metadata using arXiv IDs extracted from a list of search results.
        This function is restricted to Arxiv URLs for now.

        Args:
            search_results (List[SearchResultItem]): A list of search result items containing URLs to arXiv papers.

        Returns:
            List[PaperDataItem]: A list of paper data items retrieved from the Semantic Scholar tool.
            List[SearchResultItem]: The subset of search_results corresponding to the successfully fetched papers.
        """
        arxiv_ids = [
            res.url.path.split("/")[-1].split("v")[0] for res in search_results
        ]
        paper_items = await self.semantic_search_tool.fetch_paper_by_external_id(
            SemanticScholarSearchToolInputSchema(queries=arxiv_ids),
        )
        fetched_paper_ids = [paper_item.external_id for paper_item in paper_items]
        skipped_ids = (set(arxiv_ids)) - set(fetched_paper_ids)
        search_results = [
            search_results[i]
            for i in range(len(search_results))
            if arxiv_ids[i] not in skipped_ids
        ]
        for paper_item, res in zip(paper_items, search_results):
            if paper_item.url is None:
                paper_item.url = str(res.url)
        return paper_items, search_results

    async def _fetch_parsed_pdfs(
        self,
        search_results: List[SearchResultItem],
    ) -> List[str]:
        """
        Parses PDFs from search result items using Docling.

        Args:
            search_results (List[SearchResultItem]): A list of search result items containing PDF URLs.

        Returns:
            List[str]: A list of parsed PDF contents as strings.
        """
        pdf_urls = [res.pdf_url for res in search_results if res is not None]
        tasks = [
            self.docling_scraper.arun(OmniScraperInputSchema(url=url))
            for url in pdf_urls
        ]
        results = await asyncio.gather(*tasks)
        parsed_pdfs = [res.content for res in results]
        return parsed_pdfs

    async def _create_parsed_paper(
        self,
        parsed_pdf: str,
        paper_item: PaperDataItem,
    ) -> ParsedPaper:
        """
        Converts parsed HTML content of a paper and its metadata into a structured ParsedPaper object.

        Args:
            parsed_pdf (str): The HTML content extracted and parsed from the paper's PDF.
            paper_item (PaperDataItem): PaperDataItem of the parsed_pdf.

        Returns:
            ParsedPaper: A structured representation of the paper including section titles, content, and metadata.
        """
        parsed_content = parse_html(parsed_pdf)
        section_titles = await group_section_titles(parsed_content, self.llm)
        sections, clean_section_titles = create_sections_from_parsed_html(
            parsed_content,
            section_titles,
            paper_title=paper_item.title,
        )
        parsed_paper = ParsedPaper(
            section_titles=clean_section_titles,
            sections=sections,
            **paper_item.model_dump(),
        )
        return parsed_paper

    async def _fetch_parsed_papers(
        self,
        parsed_pdfs: list[str],
        paper_items: List[PaperDataItem],
    ) -> List[ParsedPaper]:
        """
        Creates structured ParsedPaper objects from parsed PDF content and corresponding paper metadata.

        Args:
            parsed_pdfs (list[str]): A list of HTML strings parsed from the original paper PDFs.
            paper_items (List[PaperDataItem]): A list of PaperDataItem objects corresponding to each parsed PDF.

        Returns:
            List[ParsedPaper]: A list of structured ParsedPaper objects containing organized content and metadata.
        """
        tasks = [
            self._create_parsed_paper(parsed_pdf, paper_item)
            for parsed_pdf, paper_item in zip(parsed_pdfs, paper_items)
        ]
        parsed_papers = await asyncio.gather(*tasks)
        return parsed_papers

    async def create_graph(self, parsed_papers: List[ParsedPaper]) -> nx.Graph:
        """
        Constructs a NetworkX graph from a list of parsed papers by adding their structured content and metadata.

        Args:
            parsed_papers (List[ParsedPaper]): A list of ParsedPaper objects containing sectioned content and metadata.

        Returns:
            nx.Graph: A graph where nodes and edges represent the structure and relationships within and between papers.
        """
        G = nx.Graph()
        for paper in parsed_papers:
            G = await add_paper_to_graph(G, paper, self.llm)
        return G

    def get_node_data(self, G: nx.Graph, node_id: str) -> Dict:
        """
        Retrieves the data associated with a specific node in the graph.

        Args:
            G (networkx.Graph): The graph containing nodes with metadata.
            node_id (str): The identifier of the node whose data is to be retrieved.

        Returns:
            Dict: A dictionary containing all attributes stored for the specified node.
        """
        return G.nodes[node_id]

    async def get_response_async(
        self,
        params: GapInputSchema,
        **kwargs,
    ) -> GapOutputSchema:
        """
        Obtains a response from the language model asynchronously.

        Args:
            response_model (Optional[OutputSchema]):
                The schema for the response data. If not set,
                self.output_schema is used.

        Returns:
            OutputSchema: The response from the language model.
        """
        search_results = params.search_results
        if self.debug:
            if params.gap not in gap_query_map.keys():
                logger.debug(
                    "You are running the gap agent with your own defined gap. Please ensure you have described the gap you want to investigate in detail.",
                )
            else:
                logger.debug(f"Running gap analysis to investigate {params.gap} gap.")
        gap = (
            gap_query_map[params.gap]
            if params.gap in gap_query_map.keys()
            else params.gap
        )
        paper_items, search_results = await self._fetch_paper_items(search_results)
        if self.debug:
            logger.debug(f"Fetch {len(paper_items)} papers from semantic scholar.")
        parsed_pdfs = await self._fetch_parsed_pdfs(search_results=search_results)
        if self.debug:
            logger.debug(f"Parsed {len(parsed_pdfs)} pdf documents.")
        parsed_papers = await self._fetch_parsed_papers(parsed_pdfs, paper_items)
        if self.debug:
            logger.debug(f"Fetched {len(parsed_papers)} parsed papers.")
        G = await self.create_graph(parsed_papers=parsed_papers)
        if self.debug:
            logger.debug(f"Created {G}")
        selected_nodes = await select_nodes(G, query=gap, llm=self.llm)
        if self.debug:
            logger.debug(
                f"Selected nodes from {len(selected_nodes)} papers. Now generating answer",
            )
        output, attributed_source_answers = await generate_final_answer(
            G,
            query=gap,
            all_selected_nodes=selected_nodes,
            llm=self.llm,
        )
        return GapOutputSchema(
            output=output,
            attributed_source_answers=attributed_source_answers,
            G=G,
        )

    async def _arun(self, params: GapInputSchema, **kwargs) -> GapOutputSchema:
        return await self.get_response_async(params, **kwargs)
