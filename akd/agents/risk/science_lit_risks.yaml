riskgroups:
- id: science-risk-atlas-legal
  name: Legal
  isDefinedByTaxonomy: science-risk-atlas
- id: science-risk-atlas-safety-harmfulness-and-detection
  name: Safety - harm and detection
  isDefinedByTaxonomy: science-risk-atlas
- id: science-risk-atlas-safety-robustness
  name: Safety - robustness
  isDefinedByTaxonomy: science-risk-atlas
- id: science-risk-atlas-ethics-and-bias
  name: Ethics and bias
  isDefinedByTaxonomy: science-risk-atlas
- id: science-risk-atlas-trustworthiness-factuality
  name: Trustworthiness - factuality
  isDefinedByTaxonomy: science-risk-atlas
- id: science-risk-atlas-trustworthiness-explainability
  name: Trustworthiness - explainability
  isDefinedByTaxonomy: science-risk-atlas
- id: science-risk-atlas-trustworthiness-factuality-time-sensitivity
  name: Time sensitivity
  isDefinedByTaxonomy: science-risk-atlas
- id: science-risk-atlas-trustworthiness-factuality-knowledge-contextualization
  name: Knowledge contextualization
  isDefinedByTaxonomy: science-risk-atlas
risks:
- id: compliance
  name: Compliance
  description: Ensuring that scientific agent outputs follow applicable institutional,
    ethical, legal, and publication guidelines.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-legal
  tag: compliance
  type: non-technical
  descriptor: traditional risk of AI
  concern: Noncompliance may lead to regulatory breaches, academic misconduct, or duplicate
    research, undermining trust and credibility.
- id: privacy
  name: Privacy
  description: Preventing unauthorized exposure of sensitive or personally identifiable
    information (PII) when processing scientific content.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-legal
  tag: privacy
  type: non-technical
  descriptor: traditional risk of AI
  concern: Disclosure of confidential data (e.g., participant details) may violate privacy laws (e.g., GDPR)
    and damage participant trust or legal standing.
- id: maliciousness
  name: Maliciousness
  description: Guarding against outputs that facilitate harmful or unethical actions, including sabotage
    or dangerous protocol suggestions.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-safety-harmfulness-and-detection
  tag: maliciousness
  type: output
  descriptor: amplified by generative AI
  concern: Malicious recommendations could lead to misuse of scientific knowledge, deliberate harm, or dual-use applications.
- id: profanity
  name: Profanity
  description: Detecting and filtering crude, obscene, or offensive language in agent outputs.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-safety-harmfulness-and-detection
  tag: profanity
  type: output
  descriptor: amplified by generative AI
  concern: Profanity erodes professional tone, may offend users, and can reflect poorly on scientific credibility.
- id: toxicity
  name: Toxicity
  description: Identifying harmful or discriminatory content—such as harassment or defamation—in generated text.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-safety-harmfulness-and-detection
  tag: toxicity
  type: output
  descriptor: amplified by generative AI
  concern: Toxic language or bias can damage the user experience, propagate harmful stereotypes, and violate ethical norms.
- id: out-of-distribution-checks
  name: Out-of-Distribution Checks
  description: Recognizing when inputs or prompts fall outside the model's supported scientific domain or training distribution.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-safety-robustness
  tag: out-of-distribution-checks
  type: inference
  descriptor: amplified by generative AI
  concern: Without detection, agents may hallucinate or deliver erroneous information, as the're unreliable in unfamiliar contexts.
- id: jailbreak-prevention
  name: Jailbreak Prevention
  description: Protecting against attempts to bypass safety constraints or provoke forbidden behavior in the agent.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-safety-robustness
  tag: jailbreak-prevention
  type: agentic
  descriptor: specific to agentic AI
  concern: Jailbreaks can lead to unsafe outputs—e.g., undisclosed manipulations or policy violations—jeopardizing system reliability.
- id: fairness
  name: Fairness
  description: Ensuring equitable treatment in output, avoiding bias related to demographics, geography, or research paradigms.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-ethics-and-bias
  tag: fairness
  type: non-technical
  descriptor: traditional risk of AI
  concern: Biased outputs may reproduce structural inequities, misrepresent minority voices, or skew scientific perspectives.
- id: consistency
  name: Consistency
  description: Maintaining internal coherence across multi-step outputs or related queries, ensuring uniform responses.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-trustworthiness-factuality
  tag: consistency
  type: inference
  descriptor: specific to generative AI
  concern: Inconsistent statements can confuse users and undermine scientific reliability, especially across repeated interactions.
- id: uncertainty-identification
  name: Uncertainty Identification
  description: Signaling when the model is unsure, or confidence is low in its response.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-trustworthiness-explainability
  tag: uncertainty-identification
  type: inference
  descriptor: amplified by generative AI
  concern: Overconfident yet incorrect outputs may mislead researchers, promoting false consensus or conclusions.
- id: verification
  name: Verification
  description: Enabling cross-checking of output facts or claims against trusted sources or repositories.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-trustworthiness-explainability
  tag: verification
  type: agentic
  descriptor: specific to agentic AI
  concern: Without verification, agents risk generating unchecked or fabricated scientific claims that may propagate misinformation.
- id: ip-and-copyright
  name: IP & Copyright
  description: Managing the reproduction or transformation of copyrighted or proprietary content in generated outputs.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-legal
  tag: ip-and-copyright
  type: output
  descriptor: amplified by generative AI
  concern: Generative models may reproduce protected text or data without attribution or permission, leading to intellectual
    property violations or legal disputes.
- id: societal-impact
  name: Societal Impact
  description: Evaluating and mitigating broader social harms that may emerge from scientific content generation and deployment.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-ethics-and-bias
  tag: societal-impact
  type: non-technical
  descriptor: traditional risk of AI
  concern: Outputs may unintentionally support harmful narratives or contribute to environmental, economic, or cultural harm at
    scale, especially when deployed without oversight.
- id: hallucination-identification
  name: Hallucination Identification
  description: Detecting content that is syntactically plausible but factually fabricated or unsupported by evidence.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-trustworthiness-factuality
  tag: hallucination-identification
  type: inference
  descriptor: specific to generative AI
  concern: Hallucinated content may sound authoritative while conveying false or unverifiable information, undermining scientific integrity.
- id: attribution
  name: Attribution
  description: Ensuring that cited ideas, findings, or data are accurately and transparently attributed to their original sources.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-trustworthiness-explainability
  tag: attribution
  type: agentic
  descriptor: amplified by agentic AI
  concern: Inadequate attribution risks plagiarism, undermines research transparency, and violates scholarly norms, especially
    when models generate citations without grounding.
- id: static-knowledge
  name: Static Knowledge
  description: Reliance on fixed or outdated knowledge sources that do not reflect the most recent developments in scientific domains.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-trustworthiness-factuality-time-sensitivity
  tag: static-knowledge
  type: inference
  descriptor: amplified by generative AI
  concern: LLMs trained on static corpora may present outdated scientific knowledge as current, especially in fast-moving fields like medicine or AI research.
- id: outdated-confidence
  name: Outdated Information and False Confidence
  description: Presentation of obsolete or retracted claims with unwarranted confidence, leading to misleading conclusions.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-trustworthiness-factuality-time-sensitivity
  tag: outdated-confidence
  type: inference
  descriptor: amplified by generative AI
  concern: Agents may confidently cite superseded or retracted studies without signaling uncertainty, misleading users and reinforcing invalid conclusions.
- id: overgeneralization
  name: Overgeneralization
  description: Failure to acknowledge the scope, limitations, or conditionality of scientific claims during generation.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-trustworthiness-factuality-knowledge-contextualization
  tag: overgeneralization
  type: inference
  descriptor: specific to generative AI
  concern: Generated claims may exaggerate applicability or validity by omitting qualifiers, causing overconfidence in results or inappropriate extrapolation.
- id: multidisciplinary-failure
  name: Failure to Integrate Multidisciplinary Knowledge
  description: Inability to synthesize relevant insights from different fields that contribute to understanding a scientific question.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-trustworthiness-factuality-knowledge-contextualization
  tag: multidisciplinary-failure
  type: inference
  descriptor: specific to generative AI
  concern: Scientific problems often require cross-domain reasoning, but agents may silo knowledge, missing connections or presenting narrow viewpoints.
- id: lack-of-adaptive-reasoning
  name: Lack of Adaptive Reasoning
  description: Rigid application of learned patterns without adjusting reasoning to specific scientific contexts or novel conditions.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-trustworthiness-factuality-knowledge-contextualization
  tag: lack-of-adaptive-reasoning
  type: inference
  descriptor: specific to generative AI
  concern: Generative systems may default to formulaic patterns and fail to reason flexibly in novel or interdisciplinary scenarios, reducing scientific utility.
- id: positivity-bias
  name: Positivity Bias
  description: Tendency to overemphasize favorable results or consensus while underrepresenting null, negative, or controversial findings.
  url: ""
  dateCreated: "2025-08-27"
  dateModified: "2025-08-27"
  isDefinedByTaxonomy: ""
  isPartOf: science-risk-atlas-trustworthiness-factuality-knowledge-contextualization
  tag: positivity-bias
  type: output
  descriptor: amplified by generative AI
  concern: Generative agents may preferentially surface optimistic or popular conclusions, obscuring the true state of scientific uncertainty or debate.
