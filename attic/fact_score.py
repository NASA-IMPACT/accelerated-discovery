# coding=utf-8
# Copyright 2023-present the International Business Machines.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Our implementation of the FactScore paper using LLAMA3 models

import os
import json
import sys
import argparse
import string
import torch
import numpy as np
import pandas as pd

from typing import List
from tqdm import tqdm
from dotenv import dotenv_values, load_dotenv
from litellm import batch_completion

# Local
from attic.atomizer import Atomizer
from attic.context_retriever import ContextRetriever
from fm_factual.fact_utils import Atom, Context, build_atoms, build_contexts
from attic.decontextualizer import Decontextualizer
from fm_factual.utils import RITS_MODELS

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

class FactScore:
    """
    Our implementation of the FactScore paper.
    """

    def __init__(
            self,
            query: str = None,
            response: str = None,
            topic: str = "",
            db_address: str = "9.59.197.15",
            db_port: int = 5000,
            db_remote: bool = True,
            service_type: str = "chromadb",
            model: str = "llama-3.1-70b-instruct",
            nli_scorer: str = "nli",
            nli_granularity: str = "sentence",
            nli_checkpoint: str = "/home/radu/ckpts/AlignScore-large.ckpt"
    ):
        """
        FactScore constructor

        Args:
            query: str
                The query posed to the LLM.
            response: str
                The response generated by the LLM to the given query.
            db_address: str
                The IP address to the source used for retrieving contexts (wikipedia_en)
            db_port: int
                The port of the source used for retrieving contexts (wikipedia_en)
        """

        self.query = query
        self.response = response
        self.topic = topic
        self.db_address = db_address
        self.db_port = db_port
        self.db_remote = db_remote
        self.service_type = service_type
        self.nli_granularity = nli_granularity
        self.nli_checkpoint = nli_checkpoint

        self.model = model
        self.rits_model_info = RITS_MODELS[model]

        self.prompt_template = self.rits_model_info.get("prompt_template", None)
        self.max_new_tokens = self.rits_model_info.get("max_new_tokens", None)
        self.api_base = self.rits_model_info.get("api_base", None)
        self.model_id = self.rits_model_info.get("model_id", None)

        assert self.prompt_template is not None \
            and self.max_new_tokens is not None \
            and self.api_base is not None \
            and self.model_id is not None
        
        self.parameters = dict(
            max_new_tokens=1000,
            min_new_tokens=1,
            decoding_method="greedy",
        )

        load_dotenv(override=True)
        self.RITS_API_KEY = os.getenv("RITS_API_KEY")

        self.retriever = ContextRetriever(
            db_address=self.db_address,
            db_port=self.db_port,
            db_remote=self.db_remote,
            service_type=self.service_type
        )

        self.atoms = {} # indexed by atom id
        self.contexts = {} # indexed by context id

        self.labels_human = None
        self.labels_chatgpt = None
        self.labels_llamanp = None

    def from_json(
            self,
            json_file: str,
            is_gold: bool = True,
            has_contexts: bool = False
    ):
        """
        Create a problem instance from a json file.

        Args:
            json_file: str
                The path to the json file containing the problem instance.
            is_gold: bool
                A boolean flag indicating gold/annotated problem instance
            has_contexts: bool
                A boolean flag indicating that atoms/contexts are given
        """

        print(f"[FactScore] Reading the problem instance from {json_file}")
        if has_contexts: # Reading atoms and contexts data
            with open(json_file, "r") as f:
                data = json.load(f)
                self.query = data["input"]
                self.response = data["output"]
                self.topic = data["topic"]

                atoms = data["atoms"]
                print(f"[FactScore] Extracting atoms ...")
                gold_labels = []
                atom_ids = []
                atom_contexts = {}
                self.atoms = {}
                for elem in atoms:
                    aid = elem["id"]
                    text = elem["text"]
                    label = None if "label" not in elem else elem["label"]
                    context_ids = elem["contexts"]
                    atom = Atom(id=aid, text=text, label=label)
                    atom_contexts[aid] = context_ids
                    if label is not None:
                        gold_labels.append(label)
                        atom_ids.append(aid)
                    self.atoms[aid] = atom
                
                contexts = data["contexts"]
                print(f"[FactScore] Extracting contexts ...")
                self.contexts = {}
                for elem in contexts:
                    cid = elem["id"]
                    title = elem["title"]
                    text = elem["text"]
                    context = Context(id=cid, atom=None, text=text, title=title)
                    self.contexts[cid] = context
                
                # Update the contexts per atom (if any)
                print(f"[FactScore] Updating contexts per atom ...")
                for aid, atom in self.atoms.items():
                    cid_list = atom_contexts[aid]
                    if len(cid_list) > 0:
                        contexts_per_atom = [self.contexts[cid] for cid in cid_list]
                        atom.add_contexts(contexts_per_atom)

                print(f"[FactScore] Atoms found: {len(self.atoms)}")
                for _, atom in self.atoms.items():
                    print(atom)
                print(f"[FactScore] Contexts found: {len(self.contexts)}")
                for _, context in self.contexts.items():
                    print(context)
                if len(gold_labels) > 0:
                    self.labels_human = dict(zip(atom_ids, gold_labels))
        else:             
            if is_gold: # Reading a gold/annotated problem instance (bio)
                with open(json_file, "r") as f:
                    data = json.load(f)
                    self.query = data["input"]
                    self.response = data["output"]
                    self.topic = data["topic"]

                    annotations = data["annotations"]
                    print(f"[FactScore] Extracting human annotated atoms ...")                
                    gold_labels = []
                    atom_ids = []
                    self.atoms = {}
                    i = 0
                    for elem in annotations:
                        if elem["is-relevant"] == True:
                            for atom in elem["human-atomic-facts"]:
                                aid = f"a{i}"
                                if atom["label"] in ["S", "NS"]:
                                    self.atoms[aid] = Atom(
                                        id=aid, 
                                        text=atom["text"],
                                        label=atom["label"]
                                    )
                                    atom_ids.append(aid)
                                    gold_labels.append(atom["label"])
                                    i += 1
                    print(f"[FactScore] Labeled atoms found: {len(self.atoms)}")
                    for _, atom in self.atoms.items():
                        print(atom)

                    self.labels_human = dict(zip(atom_ids, gold_labels))
            else:
                with open(json_file, "r") as f:
                    data = json.load(f)
                    self.query = data["input"]
                    self.response = data["output"]
                    self.topic = data["topic"]

                    facts = data["facts"]
                    print(f"[FactScore] Loading atoms ...")
                    atom_ids = []
                    self.atoms = {}
                    i = 0
                    for fact in facts:
                        aid = f"a{i}"
                        self.atoms[aid] = Atom(
                            id=aid,
                            text=fact
                        )
                        atom_ids.append(aid)
                        i += 1
                    
                    print(f"[FactScore] Atomic facts found: {len(self.atoms)}")
                    
                    if "ChatGPT_Labels" in data:
                        gpt = data["ChatGPT_Labels"]
                        if isinstance(gpt, list):
                            self.labels_chatgpt = dict(zip(atom_ids, gpt))
                    if "LLAMA+NP_Labels" in data:
                        llama = data["LLAMA+NP_Labels"]
                        if isinstance(llama, list):
                            self.labels_llamanp = dict(zip(atom_ids, llama))

    def to_json(
            self,
            json_file: str
    ):
        """
        Save the instance to a JSON file. Specifically, we're saving the
        atoms and retrieved contexts, for debugging purposes.
        """

        data = {}
        data["input"] = self.query
        data["output"] = self.response
        data["topic"] = self.topic
        data["atoms"] = []
        data["contexts"] = []
        
        for aid, atom in self.atoms.items():
            atom_data = dict(
                id=aid,
                text=atom.get_text(),
                contexts=[ctxt.get_id() for ctxt in atom.get_contexts()]
            )
            data["atoms"].append(atom_data)

        for cid, ctxt in self.contexts.items():
            ctxt_data = dict(
                id=cid,
                title=ctxt.get_title(),
                text=ctxt.get_text()
            )
            data["contexts"].append(ctxt_data)

        with open(json_file, "w") as f:
            f.write(f"{json.dumps(data)}\n")
        f.close()
        print(f"[FactScore] Problem instance written to: {json_file}")

    def from_dict(
            self,
            data: dict,
            is_gold: bool = True
    ):
        """
        Create a problem instance from a json file.

        Args:
            json_file: str
                The path to the json file containing the problem instance.
            is_gold: bool
                A boolean flag indicating gold/annotated problem instance
        """

        if is_gold: # Reading a gold/annotated problem instance (bio)
            self.query = data["input"]
            self.response = data["output"]
            self.topic = data["topic"]

            annotations = data["annotations"]
            if isinstance(annotations, list) == False:
                return False
            
            print(f"[FactScore] Extracting human annotated atoms ...")                
            gold_labels = []
            atom_ids = []
            self.atoms = {}
            i = 0
            for elem in annotations:
                if elem["is-relevant"] == True:
                    for atom in elem["human-atomic-facts"]:
                        aid = f"a{i}"
                        if atom["label"] in ["S", "NS"]:
                            self.atoms[aid] = Atom(
                                id=aid, 
                                text=atom["text"],
                                label=atom["label"]
                            )
                            atom_ids.append(aid)
                            gold_labels.append(atom["label"])
                            i += 1
            print(f"[FactScore] Labeled atoms found (S, NS): {len(self.atoms)}")
            for _, atom in self.atoms.items():
                print(atom)

            self.labels_human = dict(zip(atom_ids, gold_labels))
        else: # Reading an unlabeled prediction
            self.query = data["input"]
            self.response = data["output"]
            self.topic = data["topic"]

            facts = data["facts"]
            print(f"[FactScore] Loading atoms ...")
            atom_ids = []
            self.atoms = {}
            i = 0
            for fact in facts:
                aid = f"a{i}"
                self.atoms[aid] = Atom(
                    id=aid,
                    text=fact
                )
                atom_ids.append(aid)
                i += 1
            
            print(f"[FactScore] Atomic facts found: {len(self.atoms)}")
            
            if "ChatGPT_Labels" in data:
                gpt = data["ChatGPT_Labels"]
                if isinstance(gpt, list):
                    self.labels_chatgpt = dict(zip(atom_ids, gpt))
            if "LLAMA+NP_Labels" in data:
                llama = data["LLAMA+NP_Labels"]
                if isinstance(llama, list):
                    self.labels_llamanp = dict(zip(atom_ids, llama))
        return True

    def from_dict_with_contexts(
            self,
            data: dict,
    ):
        """
        Create a problem instance from a dict containing both atoms and contexts.

        Args:
            data: str
                The path to the json file containing the problem instance.
        """

        self.query = data["input"]
        self.response = data["output"]
        self.topic = data["topic"]
        
        print(f"[FactScore] Extracting human annotated atoms ...")                
        gold_labels = []
        atom_ids = []
        self.atoms = {}
        atom2contexts = {}
        for elem in data["atoms"]:
            aid = elem["id"]
            text = elem["text"]
            original = elem["original"]
            label = elem["label"]
            contexts = elem["contexts"]
            a = Atom(id=aid, text=text, label=label)
            a.set_original(original)
            atom_ids.append(aid)
            gold_labels.append(label)
            self.atoms[aid] = a
            atom2contexts[aid] = contexts
        print(f"[FactScore] Labeled atoms found (S, NS): {len(self.atoms)}")
        for _, atom in self.atoms.items():
            print(atom)
        self.labels_human = dict(zip(atom_ids, gold_labels))

        print(f"[FactScore] Extracting contexts ...")
        for elem in data["contexts"]:
            cid = elem["id"]
            title = elem["title"]
            text = elem["text"]
            c = Context(id=cid, atom=None, text=text, title=title)
            self.contexts[cid] = c

        print(f"[FactScore] Contexts retrieved: {len(self.contexts)}")
        for aid, atom in self.atoms.items():
            ctxts = []
            for c in atom2contexts[aid]:
                ctxts.append(self.contexts[c])
                self.contexts[c].set_atom(atom)
            atom.add_contexts(ctxts)
        return True

    def build(
            self,
            top_k: int = 3,
            n_results: int = 1,
            response_granularity: str = "fact",
            debug_mode: bool = False,
            retrieval_method: str = "opt1",
            has_atoms: bool = False,
            has_contexts: bool = False,
            decontextualize_atoms: bool = True,
            no_contexts: bool = False
    ):
        """
        Build the atoms and contexts using the retrieval service.

        Args:
            top_k: int
                Top k most relevant retrieved contexts.
            n_results: int
                Maximum number of chunks to be retrived from the vector store.
            response_granularity: str
                Granularity level used by the atomizer to split the response into
                chunks. The allowed values are: [none, fact, sentence, paragraph].
                We use the atomizer to split the response into atoms.
            model_id: str
                The model id used by the atomizer to decompse a text into atomic facts.
            debug_mode: bool
                Boolean flag indicating debugging mode (default False)
            retrieval_method: str
                The retrieval method. The following options are implemented:
                opt1 - for each atom, retrieve n closest articles, and top k paragraphs
                opt2 - for each atom, retrieve k closest articles 
            has_atoms: bool
                A boolean flag indicating if the atoms have already been created.
            has_contexts: bool
                A boolean flag indicating if the contexts have already been created.
            decontextualize_atoms: bool
                A boolean flag indicating that the atoms need to be decontextualized
                (i.e., pronouns he, she, it, ... replaced by the actual entity)
            no_contexts: bool
                A boolean flag indicating if contexts are to be retrieved or not.
                If True, then we run a version that only leverages the internal
                knowledge of the language model.
        """

        # Initialize the scorer
        self.top_k = top_k
        self.n_results = n_results
        self.response_granularity = response_granularity
        self.debug_mode = debug_mode
        self.retrieval_method = retrieval_method
        self.no_contexts = no_contexts

        assert retrieval_method in ["opt1", "opt2"], f"Unknown retrieval method!"

        # Create the atomizer (for the response)
        self.atomizer = Atomizer(
            granularity=self.response_granularity,
            model=self.model
        )

        print(f"[Building the FactScore instance ...]")
        print(f"Using contexts: {not no_contexts}")
        
        # Build the atoms 
        if has_atoms == False:
            self.atoms = build_atoms(
                response=self.response,
                atomizer=self.atomizer
            )

        assert len(self.atoms) > 0, f"Atoms must be initialized if `has_atoms` is True!"

        # Decontextualize the atoms
        if decontextualize_atoms:
            print(f"[FactScore] Decontextualize the atoms ...")
            atom_ids = [aid for aid in sorted(self.atoms.keys())]
            old_atoms = [(self.atoms[aid].get_text(), self.response) for aid in atom_ids]
            decontextualizer = Decontextualizer(
                model=self.model, 
                data=old_atoms, 
                datatype="contexts"
            )
            new_atoms = decontextualizer()
            for i, aid in enumerate(atom_ids):
                self.atoms[aid].set_text(new_atoms[i][0])
                print(self.atoms[aid])


        # Build the contexts (per atom)
        if no_contexts:
            self.contexts = {}
        else:
            if has_contexts == False: # check if contexts already in file
                self.contexts = build_contexts(
                    topic=self.topic,
                    atoms=self.atoms,
                    retrieval_method=self.retrieval_method,
                    retriever=self.retriever,
                    top_k=self.top_k,
                    n_results=self.n_results
                )

    def _build_prompt(
            self,
            atom: str,
            topic: str,
            passages: List[dict],
    ):
        """
        Create the prompt for predicting the label of the atom given contexts.

        Args:
            atom: str
                The string representing the atom.
            topic: str
                The topic (str) associated with the atom.
            passages: List[dict]
                A list of dictionaries representing the retrieved passages 
                relevant to the atom. Each passage is a dict with two keys:
                title - title of the article and text - passage in that article.
            model_id: str
                The model id used for prediction.

        Returns:
            A string representing the prompt (follow the FactScore paper instructions).
        """

        prompt = ""
        if self.no_contexts: # FactScore Zero without any retrieved contexts
            if len(topic) > 0:
                definition = "Answer the question about {} based on your internal knowledge.\n\n".format(topic)
            else:
                definition = "Answer the input question based on your internal knowledge.\n\n"
        else: # FactScore with retrieved contexts
            if len(topic) > 0:
                definition = "Answer the question about {} based on the given context.\n\n".format(topic)
            else:
                definition = "Answer the input question based on the given context.\n\n"

        context = ""
        for _, psg in enumerate(reversed(passages)):
            context += "Title: {}\nText: {}\n\n".format(psg["title"], psg["text"].replace("<s>", "").replace("</s>", ""))
        definition += context.strip()

        if not definition[-1] in string.punctuation:
            definition += "."
        
        # Make sure the prompt is within max_length
        if len(definition) > self.max_new_tokens:
            definition = definition[:self.max_new_tokens]

        prompt = "{}\n\nInput: {} True or False?\nOutput:".format(definition.strip(), atom.strip())

        # Format the prompt according to the model id
        try:
            prompt = self.prompt_template.format(prompt)
        except KeyError:
            None

        return prompt

    def _postprocess(
            self,
            output
    ):
        """
        Postprocess the LLM response to extract the label. The response generated
        is expected to consist of either True or False.
        """
        if type(output[1])==np.ndarray:
            # when logits are available
            logits = np.array(output[1])
            assert logits.shape[0] in [32000, 32001]
            true_score = logits[5852]
            false_score = logits[7700]
            is_supported = true_score > false_score
        else:
            # when logits are unavailable
            generated_answer = output[0].lower()
            if "true" in generated_answer or "false" in generated_answer:
                if "true" in generated_answer and "false" not in generated_answer:
                    is_supported = True
                elif "false" in generated_answer and "true" not in generated_answer:
                    is_supported = False
                else:
                    is_supported = generated_answer.index("true") > generated_answer.index("false")
            else:
                is_supported = all([keyword not in generated_answer.lower().translate(str.maketrans("", "", string.punctuation)).split() for keyword in ["not", "cannot", "unknown", "information"]])

        label = "S" if is_supported else "NS"
        return label

    def get_labels(
            self,
            atoms: dict,
            topic: str = ""
    ) -> dict:
        """
        Use a strong LLM to predict the label S or NS of an atom given its contexts.
        """

        assert len(atoms) > 0

        # Use the LLM to label the atom
        print(f"[FactScore] Labeling atoms with {self.model_id} ...")
        prompts = []
        atom_ids = []
        # Create the prompts for each of the atoms
        for aid, atom in atoms.items():
            atom_ids.append(aid)
            contexts = atom.get_contexts()
            if contexts is not None and len(contexts) > 0:
                passages = [dict(title=c.get_title(), text=c.get_text()) for c in contexts]
            else:
                passages = [] # no passages retrieved for the atom

            prompt = self._build_prompt(
                atom=atom.get_text(),
                topic=topic,
                passages=passages
            )

            prompts.append(prompt)

        print(f"[FactScore] Prompts created: {len(prompts)}")

        # Prepare the LLM call
        results = []
        messages = [[dict(role="user", content=prompt)] for prompt in prompts]
        for idx, response in tqdm(
            enumerate(
                batch_completion(
                    model=self.model_id,
                    api_base=self.api_base,
                    messages=messages,
                    api_key=self.RITS_API_KEY,
                    extra_headers={
                        "RITS_API_KEY": self.RITS_API_KEY
                    }
                )
            ),
            total=len(messages),
            desc="Prediction",
            unit="prompts",
            ):
                results.append(response.choices[0].message.content)

        # Postprocess the generated answers (expected True or False answers)
        atom_labels = []
        generated_answers = [r.strip().lower() for r in results]
        for generated_answer in generated_answers:
            if "true" in generated_answer or "false" in generated_answer:
                if "true" in generated_answer and "false" not in generated_answer:
                    is_supported = True
                elif "false" in generated_answer and "true" not in generated_answer:
                    is_supported = False
                else:
                    is_supported = generated_answer.index("true") > generated_answer.index("false")
            else:
                is_supported = all([keyword not in generated_answer.lower().translate(str.maketrans("", "", string.punctuation)).split() for keyword in ["not", "cannot", "unknown", "information"]])

            label = "S" if is_supported else "NS"
            atom_labels.append(label)

        # label = self._postprocess(output)
        return dict(zip(atom_ids, atom_labels))
    
    def score(self):
        """
        Compute the factuality score taking into consideration the contexts 
        retrieved for each of the atom in the answer.

        Factuality score = # atoms(true) / # atoms

        Intuitively, a score of 100% means that all atoms in the answer are
        factually correct. If none of them are correct, then the score is 0%. If
        only half of the atoms are correct, then the score is 50%.

        Returns:
            dict
                The results dictionary containing the factuality score i.e., a real value in [0, 1]
        """

        # Safety checks
        # assert len(self.atoms) > 0
        # assert len(self.contexts) > 0

        # Compute the FactScore
        num_true_atoms = 0
        labels = self.get_labels(self.atoms)
        for _, label in labels.items():
            if label == "S":
                num_true_atoms += 1
        fscore = float(num_true_atoms)/float(len(self.atoms))

        results = {}
        results["factuality_score"] = fscore
        results["num_atoms"] = len(self.atoms)
        results["num_contexts"] = len(self.contexts)
        results["num_true_atoms"] = num_true_atoms
        results["num_false_atoms"] = len(self.atoms) - num_true_atoms

        print(f"[FactScore] Predictions: {labels}")
        if self.labels_human is not None:
            true_atoms = 0
            false_atoms = 0
            num_true_positive = 0
            num_true_negative = 0
            num_false_positive = 0
            num_false_negative = 0
            for aid, l in self.labels_human.items():
                if l == "S":
                    true_atoms += 1
                    if labels[aid] == "S":
                        num_true_positive += 1
                    else:
                        num_false_negative += 1
                else:
                    false_atoms += 1
                    if labels[aid] == "NS":
                        num_true_negative += 1
                    else:
                        num_false_positive += 1                    
            fscore_gold = true_atoms/len(self.labels_human)
            print(f"[FactScore] Gold labels: {self.labels_human}")
            print(f"[FactScore] Predictions: {labels}")
            print(f"[FactScore] Gold fscore: {fscore_gold} ({true_atoms}/{len(self.labels_human)})")
            results["gold_factuality_score"] = fscore_gold
            results["gold_true_atoms"] = true_atoms
            results["true_positive"] = num_true_positive
            results["true_negative"] = num_true_negative
            results["false_positive"] = num_false_positive
            results["false_negative"] = num_false_negative

        if len(self.topic) > 0:
            results["topic"] = self.topic
        results["input"] = self.query

        return results

def test():
    query = "When was the Apollo 14 mission to the Moon?"
    response = "The Apollo 14 mission to the Moon took place on January 31, 1971. This mission was significant as it marked the third time humans set foot on the lunar surface, with astronauts Alan Shepard and Edgar Mitchell joining Captain Stuart Roosa, who had previously flown on Apollo 13. The mission lasted for approximately 8 days, during which the crew conducted various experiments and collected samples from the lunar surface. Apollo 14 brought back approximately 70 kilograms of lunar material, including rocks, soil, and core samples, which have been invaluable for scientific research ever since."

    model = "llama-3.1-70b-instruct"
    scorer = FactScore(
        query=query,
        response=response,
        topic="Apollo 14 mission",
        db_address="9.59.197.15",
        db_port=5000,
        db_remote=False,
        service_type="langchain",
        model=model,
    )

    # Load the problem instance from a file
    # scorer.from_json(
    #     json_file="/home/radu/git/fm-factual/examples/bio-labeled.json",
    #     is_gold=True
    # )
    json_file = "/home/radu/git/fm-factual/examples/test.json"
    with open(json_file, "r") as f:
        data = json.load(f)
    scorer.from_dict_with_contexts(data)

    # Build the scorer
    scorer.build(
        top_k=3,
        n_results=1,
        response_granularity="fact",
        retrieval_method="opt2",
        has_atoms=True,
        has_contexts=True,
        decontextualize_atoms=False
    )

    results = scorer.score()
    print(f"[FactScore] Results: {results}")
    print(f"Done.")


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--labeled_dataset', 
        type=str, 
        default=None, 
        help="Path to the labeled dataset (gold)."
    )
    
    parser.add_argument(
        '--unlabeled_dataset', 
        type=str, 
        default=None, 
        help="Path to the unlabeled dataset."
    )
    
    parser.add_argument(
        '--unlabeled_predictions', 
        type=str, 
        default=None, 
        help="Path to the unlabeled predictions (goes together with unlabeled dataset)."
    )

    parser.add_argument(
        '--output_dir', 
        type=str, 
        default=None, 
        help="Path to the output directory."
    )

    parser.add_argument(
        '--dataset_name', 
        type=str, 
        default=None, 
        help="Name of the dataset."
    )

    parser.add_argument(
        '--model', 
        type=str, 
        default="llama-3.1-70b-instruct", 
        help="Name of the underlying LLM."
    )

    parser.add_argument(
        '--test', 
        default=False, 
        action='store_true', 
        help="Debugging mode."
    )

    parser.add_argument(
        '--db_remote', 
        default=False, 
        action='store_true', 
        help="Remote wiki database service."
    )

    parser.add_argument(
        '--service_type', 
        type=str,
        default="chromadb", 
        help="Retriever type (chromadb, langchain)."
    )

    parser.add_argument(
        '--no_contexts', 
        default=False, 
        action='store_true', 
        help="Flag for enabling FactScore Zero, without contexts."
    )

    args = parser.parse_args()

    if args.test == True:
        test()
        sys.exit(0)

    retrieval_method = "opt2"
    option = "0" if args.no_contexts else ""

    if args.labeled_dataset is not None:
        print(f"[FactScore] Processing gold, labeled dataset ...")
        filename = args.labeled_dataset # a jsonl file

        with open(filename) as f:
            lines = f.read().splitlines()
        df_inter = pd.DataFrame(lines)
        df_inter.columns = ['json_element']
        df_inter['json_element'].apply(json.loads)
        df = pd.json_normalize(df_inter['json_element'].apply(json.loads))
        labeled_dataset = df.to_dict('records')
        f.close()

        print(f"[FactScore] Loading data from: {filename}")
        print(f"[FactScore] Found {len(labeled_dataset)} elements")

        # Check if previous results exist. If yes, load them and skip over them
        # when processing the input dataset.
        filename = "eval_results_factscore{}_{}_{}_{}_{}.jsonl".format(
            option,
            args.service_type,
            retrieval_method,
            args.dataset_name,
            args.model
        )
        output_filename = os.path.join(args.output_dir, filename)
        print(f"[FactScore] Reading previous results from: {output_filename}")
        evaluation_data = []
        if os.path.isfile(output_filename):
            with open(output_filename, "r") as f:
                lines = f.readlines()
                for line in lines:
                    evaluation_data.append(json.loads(line))

        print(f"[FactScore] Found {len(evaluation_data)} existing evaluations data.")
        for data in labeled_dataset: # 183 labeled bios (needs decontextualization)

            # Check if current data has been processed already
            processed = False
            for eval_data in evaluation_data:
                if eval_data["input"] == data["input"]:
                    processed = True
                    break
            if processed:
                print(f"[FactScore] Input {data} already processed.")
                continue

            # Processed the data point with FactScore
            scorer = FactScore(
                query="",
                response="",
                topic="",
                db_address="9.59.197.15",
                db_port=5000,
                db_remote=args.db_remote,
                service_type=args.service_type,
                model=args.model,
                nli_scorer="nli",
                nli_granularity="sentence"
            )

            # Load the problem instance from a file
            ok = scorer.from_dict_with_contexts(data)
            if not ok:
                continue # annotations are null (ignore)

            # Build the scorer
            scorer.build(
                top_k=3,
                n_results=1,
                response_granularity="fact",
                retrieval_method=retrieval_method,
                has_atoms=True,
                has_contexts=True,
                decontextualize_atoms=False,
                no_contexts=args.no_contexts
            )

            results = scorer.score()
            results["model_name"] = args.model
            evaluation_data.append(results)
            print(f"[FactScore] Results: {results}")

            # Save results to a file (progressively)
            filename = "eval_results_factscore{}_{}_{}_{}_{}.jsonl".format(
                option,
                args.service_type,
                retrieval_method,
                args.dataset_name,
                args.model
            )
            output_filename = os.path.join(args.output_dir, filename)
            print(f"[FactScore] Writing results to: {output_filename}")
            with open(output_filename, "w") as f:
                for res in evaluation_data:
                    f.write(f"{json.dumps(res)}\n")
            f.close()

    else: # 500 unlabeled bios
        assert args.unlabeled_dataset is not None
        assert args.unlabeled_predictions is not None

        # Read the unlabeled dataset
        with open(args.unlabeled_dataset) as f:
            lines = f.read().splitlines()
        df_inter = pd.DataFrame(lines)
        df_inter.columns = ['json_element']
        df_inter['json_element'].apply(json.loads)
        df_unlabeled_dataset = pd.json_normalize(df_inter['json_element'].apply(json.loads))
        f.close()

        # Read the unlabeled predictions
        with open(args.unlabeled_predictions) as f:
            lines = f.read().splitlines()
        df_inter = pd.DataFrame(lines)
        df_inter.columns = ['json_element']
        df_inter['json_element'].apply(json.loads)
        df_unlabeled_predictions = pd.json_normalize(df_inter['json_element'].apply(json.loads))
        f.close()

        # Merge the two pieces
        df = pd.merge(
            df_unlabeled_dataset,
            df_unlabeled_predictions,
            left_on='input',
            right_on='prompt',
            how='left'
        ).drop('prompt', axis=1)

        unlabeled_dataset = df.to_dict('records')
        print(f"[FactScore] Loading data from {args.unlabeled_dataset}")
        print(f"[FactScore] Found {len(unlabeled_dataset)} elements")

        # Check if previous results exist. If yes, load them and skip over them
        # when processing the input dataset.
        filename = "eval_results_factscore{}_{}_{}_{}_{}.jsonl".format(
            option,
            args.service_type,
            retrieval_method,
            args.dataset_name,
            args.model
        )
        output_filename = os.path.join(args.output_dir, filename)
        print(f"[FactScore] Reading previous results from: {output_filename}")
        evaluation_data = []
        if os.path.isfile(output_filename):
            with open(output_filename, "r") as f:
                lines = f.readlines()
                for line in lines:
                    evaluation_data.append(json.loads(line))

        print(f"[FactScore] Found {len(evaluation_data)} existing evaluations data.")

        for data in unlabeled_dataset: # 500 unlabeled bios (needs decontextualization)
            scorer = FactScore(
                query="",
                response="",
                topic="",
                db_address="9.59.197.15",
                db_port=5000,
                db_remote=args.db_remote,
                service_type=args.service_type,
                model=args.model,
                nli_scorer="nli",
                nli_granularity="sentence"
            )

            # Load the problem instance from a file
            scorer.from_dict(data, is_gold=False)

            # Build the scorer
            scorer.build(
                top_k=3,
                n_results=3,
                response_granularity="fact",
                retrieval_method=retrieval_method,
                has_atoms=True,
                decontextualize_atoms=True,
                no_contexts=args.no_contexts
            )

            results = scorer.score()
            results["model_name"] = args.model
            evaluation_data.append(results)
            print(f"[FactScore] Results: {results}")

            # Save results to a file (progressively)
            filename = "eval_results_factscore{}_{}_{}_{}_{}.jsonl".format(
                option,
                args.service_type,
                retrieval_method,
                args.dataset_name,
                args.model
            )
            output_filename = os.path.join(args.output_dir, filename)
            print(f"[FactScore] Writing results to: {output_filename}")
            with open(output_filename, "w") as f:
                for res in evaluation_data:
                    f.write(f"{json.dumps(res)}\n")
            f.close()

    print("Done.")
