# coding=utf-8
# Copyright 2023-present the International Business Machines.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Factuality reasoner
import sys
import os
import json
import math
import torch
import argparse
import subprocess
import uuid
import pandas as pd
import litellm

# from bert_score import BERTScorer
from pgmpy.models import MarkovNetwork
from pgmpy.factors.discrete import DiscreteFactor
from pgmpy.inference import VariableElimination
from pgmpy.readwrite import UAIWriter
from pgmpy.global_vars import logger
from dotenv import dotenv_values, load_dotenv

# Local
from attic.atomizer import Atomizer
from attic.decontextualizer import Decontextualizer
from attic.context_retriever import ContextRetriever
# from fm_factual.nli_scorer import NLIScorer
# from fm_factual.align_scorer import AlignScorer
from fm_factual.nli_scorer_prompt import NLIScorerPrompting
from fm_factual.fact_graph import FactGraph
from fm_factual.fact_utils import Atom, Context, Relation, build_atoms, build_contexts, build_relations
from fm_factual.fact_utils import remove_duplicated_contexts

from fm_factual.utils import RITS_MODELS

# pgmpy set the root logger to INFO -- changed it to WARNING
import logging
logger.setLevel(logging.WARNING)
os.environ["LITELLM_LOG"] = 'ERROR'
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("datasets").setLevel(logging.WARNING)
logging.getLogger("sentence_transformers.SentenceTransformer").setLevel(logging.WARNING)
logging.getLogger("litellm").setLevel(logging.WARNING)
logging.getLogger("pgmpy").setLevel(logging.WARNING)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

PRIOR_PROB_ATOM = 0.5
PRIOR_PROB_CONTEXT = 0.9

class FactReasoner:
    def __init__(
            self,
            query: str = None,
            response: str = None,
            topic: str = "",
            db_address: str = "9.59.197.15",
            db_port: int = 5000,
            db_remote: bool = True,
            service_type: str = "chromadb",
            model: str = "llama-3.1-70b-instruct",
            use_merlin: bool = False,
            nli_scorer: str = "nli",
            nli_scoring_method: str = "logprobs",
            nli_granularity: str = "sentence",
            nli_checkpoint: str = None
    ):
        """
        FactReasoner constructor

        Args:
            query: str
                The query posed to the LLM.
            response: str
                The response generated by the LLM to the given query.
            topic: str
                The topic associated with the query/response.
            db_address: str
                The IP address to the source used for retrieving contexts (wikipedia_en)
            db_port: int
                The port of the source used for retrieving contexts (wikipedia_en)
            service_type: str
                The context retriever implementation (chromadb or langchain)
            model: str
                The underlying LLM used for decontextualization, NLI prompting, etc.
            use_merlin: bool
                Flag indicating the probabilistic inference engine (default is False)
            nli_scorer: str
                The NLI scorer to be used (nli, align)
            nli_scoring_method: str
                The UQ method used by the NLI scorer with prompting an LLM.
            nli_granularity: str
                The granularity used by the NLI scorers (sentence, paragraph)
            nli_checkpoint: str
                The path to the NLI model checkpoint (for AlignScore only)
        """

        self.query = query
        self.response = response
        self.topic = topic
        self.db_address = db_address
        self.db_port = db_port
        self.db_remote = db_remote
        self.service_type = service_type
        self.nli_granularity = nli_granularity
        self.nli_checkpoint = nli_checkpoint
        self.nli_scoring_method = nli_scoring_method
        self.use_merlin = use_merlin

        self.model = model
        self.rits_model_info = RITS_MODELS[model]

        self.prompt_template = self.rits_model_info.get("prompt_template", None)
        self.max_new_tokens = self.rits_model_info.get("max_new_tokens", None)
        self.api_base = self.rits_model_info.get("api_base", None)
        self.model_id = self.rits_model_info.get("model_id", None)

        assert self.prompt_template is not None \
            and self.max_new_tokens is not None \
            and self.api_base is not None \
            and self.model_id is not None
        
        self.parameters = dict(
            max_new_tokens=1000,
            min_new_tokens=1,
            decoding_method="greedy",
        )

        load_dotenv(override=True)
        self.RITS_API_KEY = os.getenv("RITS_API_KEY")

        # Initialize the BERT scorer
        self.bert_scorer = None
        # self.bert_scorer = BERTScorer(
        #     model_type='bert-base-uncased', 
        #     device=DEVICE
        # )

        # Create the context retriever
        self.retriever = ContextRetriever(
            db_address=self.db_address,
            db_port=self.db_port,
            db_remote=self.db_remote,
            service_type=self.service_type
        )

        self.atoms = {} # indexed by atom id
        self.contexts = {} # indexed by context id
        self.relations = []

        # Instantiate the NLI scorer
        self.nli_scorer = None
        assert nli_scorer in ["nli", "align", "nli_prompt"], \
            f"Unknown NLI scorer: {nli_scorer}"
        assert nli_granularity in ["sentence", "paragraph"], \
            f"Unknown NLI granularity: {nli_granularity}"

        if nli_scorer == 'nli': # NLIScorer implementation
            # self.nli_scorer = NLIScorer(
            #     model_name="vitc",
            #     granularity=nli_granularity,
            # )
            raise ValueError(f"NLIScorer should not be used anymore.")
        elif nli_scorer == 'align': # AlignScorer implementation
            # assert self.nli_checkpoint is not None, f"AlignScore requires a checkpoint of the model."
            # self.nli_scorer = AlignScorer(
            #     model="roberta-large",
            #     ckpt_path=self.nli_checkpoint,
            #     evaluation_mode="nli_sp",
            #     granularity=nli_granularity
            # )
            raise ValueError(f"AlignScorer should not be used anymore.")
        elif nli_scorer == 'nli_prompt': # prompting llama3 for nli scores
            self.nli_scorer = NLIScorerPrompting(
                model=self.model,
                granularity=nli_granularity,
                scoring_method=self.nli_scoring_method
            )

        # The fact graph and probabilistic model (Markov Network)
        self.fact_graph = None
        self.markov_network = None

        self.labels_human = None
        self.labels_chatgpt = None
        self.labels_llamanp = None

    def from_json2(
            self, 
            json_file: str,
            rel_atom_atom: bool = False,
            rel_atom_context: bool = True,
            rel_context_context: bool = True,
            rel_threshold: float = None,
    ):
        """
        Create a problem instance from a json file. To be used only for small
        scale, toy examples to demonstrate the capability of the FactReasoner.

        Args:
            json_file: str
                The path to the json file containing the problem instance.
            rel_atom_atom: bool (default is False)
                Flag indicating the presence of atom-to-atom relationships.
            rel_atom_context: bool (default is True)
                Flag indicating the presence of atom-to-context relationships.
            rel_context_context: bool (default is False)
                Flag indicating the presence of context-to-context relationships.

        """

        # Read the atoms and corresponding contexts from the json file
        with open(json_file, "r") as f:
            data = json.load(f)
            assert ("atoms" in data and "contexts" in data), \
                f"JSON file has unknown problem instance format."
            
            self.atoms = {}
            for elem in data["atoms"]:
                aid = elem["id"]
                self.atoms[aid] = Atom(id=aid, text=elem["text"])
            
            self.contexts = {}
            for elem in data["contexts"]:
                cid = elem["id"]
                atom_id = elem["atom_id"]
                self.contexts[cid] = Context(
                    id=cid,
                    atom=self.atoms[atom_id],
                    text=elem["text"]
                )
        
        # Build the relations
        self.relations = build_relations(
            atoms=self.atoms,
            contexts=self.contexts,
            rel_atom_atom=rel_atom_atom,
            rel_atom_context=rel_atom_context, 
            rel_context_context=rel_context_context,
            rel_threshold=rel_threshold,
            nli_scorer=self.nli_scorer
        )

        # Create the corresponding fact graph
        self._build_fact_graph()

        # Create the corresponding probabilistic model (Markov Network)
        self._build_markov_network()

    def from_fact_graph(
            self, 
            fact_graph: FactGraph
    ):
        """
        Create a FactReasoner instance from a FactGraph instance.

        Args:
            fact_graph: FactGraph
                A FactGraph instance.
        """

        # Create the atoms, contexts and relations
        self.atoms = {}
        self.contexts = {}
        self.relations = []

        for node_id, node in fact_graph.nodes.items():
            if node.type == "atom":
                self.atoms[node_id] = Atom(
                    id=node_id,
                    text=""
                )
            elif node.type == "context":
                self.contexts[node_id] = Context(
                        id=node_id,
                        atom=None,
                        text=""
                )

        for edge in fact_graph.edges:
            id_source = edge.source
            id_target = edge.target
            if id_source in self.atoms:
                src = self.atoms[id_source]
            elif id_source in self.contexts:
                src = self.contexts[id_source]
            if id_target in self.atoms:
                trg = self.atoms[id_target]
            elif id_target in self.contexts:
                trg = self.contexts[id_target]

            rel = Relation(
                source=src,
                target=trg,
                type=edge.type,
                probability=edge.probability
            )

            self.relations.append(rel)

        # Create the corresponding fact graph
        self.fact_graph = fact_graph

        # Create the corresponding probabilistic model (Markov Network)
        self._build_markov_network()

    def from_json(
            self,
            json_file: str,
            is_gold: bool = True,
            has_contexts: bool = False
    ):
        """
        Create a problem instance from a json file.

        Args:
            json_file: str
                The path to the json file containing the problem instance.
            is_gold: bool
                A boolean flag indicating gold/annotated problem instance
            has_contexts: bool
                A boolean flag indicating that atoms/contexts are given.
        """

        print(f"[FactReasoner] Reading the problem instance from {json_file}")
        if has_contexts: # Reading atoms and contexts data
            with open(json_file, "r") as f:
                data = json.load(f)
                self.query = data["input"]
                self.response = data["output"]
                self.topic = data["topic"]

                atoms = data["atoms"]
                print(f"[FactReasoner] Extracting atoms ...")
                gold_labels = []
                atom_ids = []
                atom_contexts = {}
                self.atoms = {}
                for elem in atoms:
                    aid = elem["id"]
                    text = elem["text"]
                    label = None if "label" not in elem else elem["label"]
                    context_ids = elem["contexts"]
                    atom = Atom(id=aid, text=text, label=label)
                    atom_contexts[aid] = context_ids
                    if label is not None:
                        gold_labels.append(label)
                        atom_ids.append(aid)
                    self.atoms[aid] = atom
                
                contexts = data["contexts"]
                print(f"[FactReasoner] Extracting contexts ...")
                self.contexts = {}
                for elem in contexts:
                    cid = elem["id"]
                    title = elem["title"]
                    text = elem["text"]
                    context = Context(id=cid, atom=None, text=text, title=title)
                    self.contexts[cid] = context
                
                # Update the contexts per atom (if any)
                print(f"[FactReasoner] Updating contexts per atom ...")
                for aid, atom in self.atoms.items():
                    cid_list = atom_contexts[aid]
                    if len(cid_list) > 0:
                        contexts_per_atom = [self.contexts[cid] for cid in cid_list]
                        atom.add_contexts(contexts_per_atom)

                print(f"[FactReasoner] Atoms found: {len(self.atoms)}")
                for _, atom in self.atoms.items():
                    print(atom)
                print(f"[FactReasoner] Contexts found: {len(self.contexts)}")
                for _, context in self.contexts.items():
                    print(context)
                if len(gold_labels) > 0:
                    self.labels_human = dict(zip(atom_ids, gold_labels))
        else:
            if is_gold: # Reading a gold/annotated problem instance (bio)
                with open(json_file, "r") as f:
                    data = json.load(f)
                    self.query = data["input"]
                    self.response = data["output"]
                    self.topic = data["topic"]

                    annotations = data["annotations"]
                    print(f"[FactReasoner] Extracting human annotated atoms ...")                
                    gold_labels = []
                    atom_ids = []
                    self.atoms = {}
                    i = 0
                    for elem in annotations:
                        if elem["is-relevant"] == True:
                            for atom in elem["human-atomic-facts"]:
                                aid = f"a{i}"
                                if atom["label"] in ["S", "NS"]:
                                    self.atoms[aid] = Atom(
                                        id=aid, 
                                        text=atom["text"],
                                        label=atom["label"]
                                    )
                                    atom_ids.append(aid)
                                    gold_labels.append(atom["label"])
                                    i += 1
                    print(f"[FactReasoner] Labeled atoms found: {len(self.atoms)}")
                    for _, atom in self.atoms.items():
                        print(atom)

                    self.labels_human = dict(zip(atom_ids, gold_labels))
            else:
                with open(json_file, "r") as f:
                    data = json.load(f)
                    self.query = data["input"]
                    self.response = data["output"]
                    self.topic = data["topic"]

                    facts = data["facts"]
                    print(f"[FactReasoner] Loading atoms ...")
                    atom_ids = []
                    self.atoms = {}
                    i = 0
                    for fact in facts:
                        aid = f"a{i}"
                        self.atoms[aid] = Atom(
                            id=aid,
                            text=fact
                        )
                        atom_ids.append(aid)
                        i += 1
                    
                    print(f"[FactReasoner] Atomic facts found: {len(self.atoms)}")
                    
                    if "ChatGPT_Labels" in data:
                        gpt = data["ChatGPT_Labels"]
                        if isinstance(gpt, list):
                            self.labels_chatgpt = dict(zip(atom_ids, gpt))
                    if "LLAMA+NP_Labels" in data:
                        llama = data["LLAMA+NP_Labels"]
                        if isinstance(llama, list):
                            self.labels_llamanp = dict(zip(atom_ids, llama))

    def from_dict_with_contexts(
            self,
            data: dict,
    ):
        """
        Create a problem instance from a dict containing both atoms and contexts.

        Args:
            data: str
                The path to the json file containing the problem instance.
        """

        self.query = data["input"]
        self.response = data["output"]
        self.topic = data["topic"]
        
        print(f"[FactReasoner] Extracting human annotated atoms ...")                
        gold_labels = []
        atom_ids = []
        self.atoms = {}
        atom2contexts = {}
        for elem in data["atoms"]:
            aid = elem["id"]
            text = elem["text"]
            original = elem["original"]
            label = elem["label"]
            contexts = elem["contexts"]
            a = Atom(id=aid, text=text, label=label)
            a.set_original(original)
            atom_ids.append(aid)
            gold_labels.append(label)
            self.atoms[aid] = a
            atom2contexts[aid] = contexts
        print(f"[FactReasoner] Labeled atoms found (S, NS): {len(self.atoms)}")
        for _, atom in self.atoms.items():
            print(atom)
        self.labels_human = dict(zip(atom_ids, gold_labels))

        print(f"[FactReasoner] Extracting contexts ...")
        for elem in data["contexts"]:
            cid = elem["id"]
            title = elem["title"]
            text = elem["text"]
            c = Context(id=cid, atom=None, text=text, title=title)
            self.contexts[cid] = c

        print(f"[FactReasoner] Contexts retrieved: {len(self.contexts)}")
        for aid, atom in self.atoms.items():
            ctxts = []
            for c in atom2contexts[aid]:
                ctxts.append(self.contexts[c])
                self.contexts[c].set_atom(atom)
            atom.add_contexts(ctxts)
        return True

    def to_json(
            self,
            json_file: str
    ):
        """
        Save the instance to a JSON file. Specifically, we're saving the
        atoms and retrieved contexts, for debugging purposes.
        """

        data = {}
        data["input"] = self.query
        data["output"] = self.response
        data["topic"] = self.topic
        data["atoms"] = []
        data["contexts"] = []
        
        for aid, atom in self.atoms.items():
            atom_data = dict(
                id=aid,
                text=atom.get_text(),
                contexts=[ctxt.get_id() for ctxt in atom.get_contexts()]
            )
            if atom.get_label() is not None:
                atom_data["label"] = atom.get_label()
            data["atoms"].append(atom_data)

        for cid, ctxt in self.contexts.items():
            ctxt_data = dict(
                id=cid,
                title=ctxt.get_title(),
                text=ctxt.get_text()
            )
            data["contexts"].append(ctxt_data)

        with open(json_file, "w") as f:
            f.write(f"{json.dumps(data)}\n")
        f.close()
        print(f"[FactReasoner] Problem instance written to: {json_file}")
        
    def from_dict(
            self,
            data: dict,
            is_gold: bool = True
    ):
        """
        Create a problem instance from a json file.

        Args:
            json_file: str
                The path to the json file containing the problem instance.
            is_gold: bool
                A boolean flag indicating gold/annotated problem instance
        """

        if is_gold: # Reading a gold/annotated problem instance (bio)
            self.query = data["input"]
            self.response = data["output"]
            self.topic = data["topic"]

            annotations = data["annotations"]
            if isinstance(annotations, list) == False:
                return False
            
            print(f"[FactReasoner] Extracting human annotated atoms ...")                
            gold_labels = []
            atom_ids = []
            self.atoms = {}
            i = 0
            for elem in annotations:
                if elem["is-relevant"] == True:
                    for atom in elem["human-atomic-facts"]:
                        aid = f"a{i}"
                        if atom["label"] in ["S", "NS"]:
                            self.atoms[aid] = Atom(
                                id=aid, 
                                text=atom["text"],
                                label=atom["label"]
                            )
                            atom_ids.append(aid)
                            gold_labels.append(atom["label"])
                            i += 1
            print(f"[FactReasoner] Labeled atoms found (S, NS): {len(self.atoms)}")
            for _, atom in self.atoms.items():
                print(atom)

            self.labels_human = dict(zip(atom_ids, gold_labels))
        else: # Reading an unlabeled prediction
            self.query = data["input"]
            self.response = data["output"]
            self.topic = data["topic"]

            facts = data["facts"]
            print(f"[FactScore] Loading atoms ...")
            atom_ids = []
            self.atoms = {}
            i = 0
            for fact in facts:
                aid = f"a{i}"
                self.atoms[aid] = Atom(
                    id=aid,
                    text=fact
                )
                atom_ids.append(aid)
                i += 1
            
            print(f"[FactReasoner] Atomic facts found: {len(self.atoms)}")
            
            if "ChatGPT_Labels" in data:
                gpt = data["ChatGPT_Labels"]
                if isinstance(gpt, list):
                    self.labels_chatgpt = dict(zip(atom_ids, gpt))
            if "LLAMA+NP_Labels" in data:
                llama = data["LLAMA+NP_Labels"]
                if isinstance(llama, list):
                    self.labels_llamanp = dict(zip(atom_ids, llama))
        return True

    def build(
            self,
            top_k: int = 3,
            n_results: int = 3,
            response_granularity: str = "fact",
            debug_mode: bool = False,
            has_atoms: bool = False,
            has_contexts: bool = False,
            decontextualize_atoms: bool = True,
            remove_duplicates: bool = False,
            contexts_per_atom_only: bool = False,
            top_k_per_atom: int = 1,
            retrieval_method: str = "opt1",
            rel_atom_atom: bool = False,
            rel_atom_context: bool = True,
            rel_context_context: bool = True,
            rel_threshold: float = None
    ):
        """
        Build the atoms and contexts using the retrieval service.

        Args:
            top_k: int
                Top k most relevant retrieved contexts.
            n_results: int
                Maximum number of chunks to be retrived from the vector store.
            response_granularity: str
                Granularity level used by the atomizer to split the response into
                chunks. The allowed values are: [none, fact, sentence, paragraph].
                We use the atomizer to split the response into atoms.
            debug_mode: bool
                Boolean flag indicating debugging mode (default False)
            has_atoms: bool
                Flag indicating if the atoms were previously initialized.
            has_contexts: bool
                Flag indicating is the contexts were previously initialized.
            remove_duplicates: bool
                Flag indicating if duplicated contexts are to be removed.
            retrieval_method: str
                The retrieval method. The following options are implemented:
                opt1 - for each atom, retrieve n articles, and top k paragraphs
                opt2 - for each atom, retrieve top k articles
            rel_atom_atom: bool (default is False)
                Flag indicating the presence of atom-to-atom relationships.
            rel_atom_context: bool (default is True)
                Flag indicating the presence of atom-to-context relationships.
            rel_context_context: bool (default is False)
                Flag indicating the presence of context-to-context relationships.
            rel_threshold: float
                A read valued threshold for considering the relationships.
        """

        # Initialize the reasoner
        self.fact_graph = None
        self.markov_network = None
        self.top_k = top_k
        self.n_results = n_results
        self.response_granularity = response_granularity
        self.debug_mode = debug_mode
        self.retrieval_method = retrieval_method

        assert retrieval_method in ["opt1", "opt2"], f"Unknown retrieval method!"

        # Create the atomizer (for the response)
        self.atomizer = Atomizer(
            granularity=self.response_granularity,
            model=self.model
        )

        print(f"[Building the FactReasoner instance ...]")
        
        # Build the atoms
        if has_atoms == False:
            self.atoms = build_atoms(
                response=self.response,
                atomizer=self.atomizer
            )
        
        # Safety checks
        assert len(self.atoms) > 0, f"Atoms must be initialized if `has_atoms` is True!"

        # Decontextualize the atoms
        if decontextualize_atoms:
            print(f"[FactReasoner] Decontextualize the atoms ...")
            atom_ids = [aid for aid in sorted(self.atoms.keys())]
            old_atoms = [(self.atoms[aid].get_text(), self.response) for aid in atom_ids]
            decontextualizer = Decontextualizer(
                model=self.model, 
                data=old_atoms, 
                datatype="contexts"
            )
            new_atoms = decontextualizer()
            for i, aid in enumerate(atom_ids):
                self.atoms[aid].set_text(new_atoms[i][0])
                print(self.atoms[aid])

        # Build contexts
        if has_contexts == False:
            self.contexts = build_contexts(
                topic=self.topic,
                atoms=self.atoms,
                retrieval_method=self.retrieval_method,
                retriever=self.retriever,
                top_k=self.top_k,
                n_results=self.n_results,
            )

        # Safety checks
        assert len(self.contexts) > 0, "Contexts must be initialized if `has_contexts` is True!"
        if remove_duplicates:
            self.contexts = remove_duplicated_contexts(self.contexts)
            print(f"[FactReasoner] Found {len(self.contexts)} unique contexts.")

        # Build relationships
        self.relations = build_relations(
            response=self.response,
            atoms=self.atoms,
            contexts=self.contexts,
            rel_atom_atom=rel_atom_atom,
            rel_atom_context=rel_atom_context, 
            rel_context_context=rel_context_context,
            rel_threshold=rel_threshold,
            contexts_per_atom_only=contexts_per_atom_only,
            top_k_per_atom=top_k_per_atom,
            nli_scorer=self.nli_scorer
        )

        # Build the fact graph and Markov network
        self._build_fact_graph()
        self._build_markov_network()
        
        print(f"[FactReasoner] Instance created.")

    def rebuild_relations(
            self,
            contexts_per_atom_only: bool = False,
            top_k_per_atom: int = 1,
            rel_atom_atom: bool = False,
            rel_atom_context: bool = True,
            rel_context_context: bool = True,
            rel_threshold: float = None
    ):
        """
        Build the atoms and contexts using the retrieval service.

        Args:
            top_k: int
                Top k most relevant retrieved contexts.
            n_results: int
                Maximum number of chunks to be retrived from the vector store.
            response_granularity: str
                Granularity level used by the atomizer to split the response into
                chunks. The allowed values are: [none, fact, sentence, paragraph].
                We use the atomizer to split the response into atoms.
            model_id: str
                The model id used by the atomizer to decompse a text into atomic facts.
            debug_mode: bool
                Boolean flag indicating debugging mode (default False)
            has_atoms: bool
                Flag indicating if the atoms were previously initialized.
            remove_duplicates: bool
                Flag indicating if duplicated contexts are to be removed.
            retrieval_method: str
                The retrieval method. The following options are implemented:
                opt1 - for each atom, retrieve n articles, and top k paragraphs
                opt2 - for each atom, retrieve top k articles
            rel_atom_atom: bool (default is False)
                Flag indicating the presence of atom-to-atom relationships.
            rel_atom_context: bool (default is True)
                Flag indicating the presence of atom-to-context relationships.
            rel_context_context: bool (default is False)
                Flag indicating the presence of context-to-context relationships.
            rel_threshold: float
                A read valued threshold for considering the relationships.
        """

        print(f"[FactReasoner] Rebuilding the relations ...]")

        assert len(self.atoms) > 0
        assert len(self.contexts) > 0        
        
        # Build relationships
        self.relations = build_relations(
            response=self.response,
            atoms=self.atoms,
            contexts=self.contexts,
            rel_atom_atom=rel_atom_atom,
            rel_atom_context=rel_atom_context, 
            rel_context_context=rel_context_context,
            rel_threshold=rel_threshold,
            contexts_per_atom_only=contexts_per_atom_only,
            top_k_per_atom=top_k_per_atom,
            nli_scorer=self.nli_scorer
        )

        # Build the fact graph and Markov network
        self._build_fact_graph()
        self._build_markov_network()
        
        print(f"[FactReasoner] Instance created.")


    def dump(self):
        """
        Dump the content of the fact reasoner for debugging purposes.
        """

        print("Atoms:")
        for _, atom in self.atoms.items():
            print(atom)
        print("Contexts:")
        for _, context in self.contexts.items():
            print(context)
        print("Relations:")
        for rel in self.relations:
            print(rel)

    def _build_fact_graph(self):
        """
        Create the fact graph representation from atoms, contexts and relations.
        """
        self.fact_graph = FactGraph(
            atoms=list(self.atoms.values()),
            contexts=list(self.contexts.values()),
            relations=self.relations
        )

    def _build_markov_network(
            self, 
            skip_at_most_one: bool = True
    ):
        """
        Create the Markov Network corresponding to the FactGraph.

        Args:
            fact_graph: FactGraph
                The FactGraph representation of the atoms, contexts and relations.

        Return:
            A MarkovNetwork encoding of the problem.
        """

        assert self.fact_graph is not None, f"The FactGraph must be built."

        # Create an empty Markov Network
        self.markov_network = MarkovNetwork()

        # Create the variables corresponding to the nodes in the fact graph
        print(f"[Building the Markov network...]")
        for node in self.fact_graph.get_nodes():
            x = node.id
            self.markov_network.add_node(x)
            if node.type == "context":
                prob = PRIOR_PROB_CONTEXT
                factor = DiscreteFactor(
                    variables=[x],
                    cardinality=[2],
                    values=[1.0 - prob, prob]
                )
                self.markov_network.add_factors(factor)
                print(f"Adding context variable {x} with discrete factor (prior)")
            elif node.type == "atom":
                prob = PRIOR_PROB_ATOM
                factor = DiscreteFactor(
                    variables=[x],
                    cardinality=[2],
                    values=[1.0 - prob, prob]
                )
                self.markov_network.add_factors(factor)
                print(f"Adding atom variable {x} with discrete factor (prior)")
            else:
                raise ValueError(f"Unknown node type: {node.type}")

        # Create the factors corresponding to the edges in the fact graph
        for edge in self.fact_graph.get_edges():
            x, y = edge.source, edge.target
            self.markov_network.add_edge(x, y)
            if edge.type == "entailment": # add factor X -> Y
                prob = edge.probability
                if edge.link == "context_atom":
                    values = [1.0 - PRIOR_PROB_ATOM, PRIOR_PROB_ATOM, 1.0 - prob, prob]
                elif edge.link == "context_context":
                    values = [1.0 - PRIOR_PROB_CONTEXT, PRIOR_PROB_CONTEXT, 1.0 - prob, prob]
                elif edge.link == "atom_atom":
                    values = [1.0 - PRIOR_PROB_ATOM, PRIOR_PROB_ATOM, 1.0 - prob, prob]
                else:
                    raise ValueError(f"Unknown link type: {edge.link}")
                factor = DiscreteFactor(
                    variables=[x, y],
                    cardinality=[2, 2],
                    values=values #[prob, prob, 1.0 - prob, prob]
                )
                self.markov_network.add_factors(factor)
                print(f"Adding edge {x} - {y} with discrete factor (entailment)")
            elif edge.type == "contradiction": # add factor X -> !Y
                prob = edge.probability
                if edge.link == "context_atom":
                    values = [1.0 - PRIOR_PROB_ATOM, PRIOR_PROB_ATOM, prob, 1.0 - prob]
                elif edge.link == "context_context":
                    values = [1.0 - PRIOR_PROB_CONTEXT, PRIOR_PROB_CONTEXT, prob, 1.0 - prob]
                elif edge.link == "atom_atom":
                    values = [1.0 - PRIOR_PROB_ATOM, PRIOR_PROB_ATOM, prob, 1.0 - prob]
                else:
                    raise ValueError(f"Unknown link type: {edge.link}")
                factor = DiscreteFactor(
                    variables=[x, y],
                    cardinality=[2, 2],
                    values=values #[prob, prob, prob, 1.0 - prob]
                )
                self.markov_network.add_factors(factor)
                print(f"Adding edge {x} - {y} with discrete factor (contradiction)")
            elif edge.type == "equivalence":
                prob = edge.probability
                factor = DiscreteFactor(
                    variables=[x, y],
                    cardinality=[2, 2],
                    values=[prob, 1.0 - prob, 1.0 - prob, prob]
                )
                self.markov_network.add_factors(factor)
                print(f"Adding edge {x} - {y} with discrete factor (equivalence)")
        
        # [Optional] Create at-most-one constraints, namely, each atom has
        # at most one active context (chosen from its corresp. contexts)
        if skip_at_most_one == False:
            raise NotImplementedError(f"Not yet implemented.")

        # Output the content of the network
        print("[Markov network created.]")
        print(self.markov_network)

        if self.debug_mode:
            print("[Markov network content...]")
            for f in self.markov_network.get_factors():
                print(f)

        # writer = UAIWriter(model)
        # writer.write_uai("/home/radu/git/fm-factual/examples/markov_network.uai")

    def run_merlin(self):
        """
        Run inference with merlin (executable)
        """

        # Prepare the query variables (i.e., atoms)
        query_variables = [var for var in sorted(self.atoms.keys())]

        # Dump the markov network to a temporary file
        net_id = str(uuid.uuid1())
        input_filename = f"/tmp/markov_network_{net_id}.uai"
        writer = UAIWriter(self.markov_network)
        writer.write_uai(input_filename)

        # Get the variable name to index mapping {0: ('a0', '2'), 1: ('a1', '2')}
        vars_mapping = {}
        variables = sorted(writer.domain.items(), key=lambda x: (x[1], x[0]))
        for i, var in enumerate(variables):
            vars_mapping[i] = var[0]

        # Run merlin as a subprocess and collect the results
        exefile = "/home/radu/git/fm-factual/lib/merlin"
        output_format = "json"
        output_file = f"/tmp/output_{net_id}"
        algorithm = "wmb"
        task = "MAR"
        ibound = "6"

        args = [
            exefile,
            "--input-file", input_filename,
            "--ibound", ibound,
            "--task", task,
            "--algorithm", algorithm,
            "--output-format", output_format,
            "--output-file", output_file
        ]

        proc = subprocess.run(args)
        
        print(f"[Merlin] return code: {proc.returncode}")
        output_filename = f"{output_file}.{task}.{output_format}"
        with open(output_filename) as f:
            results = json.load(f)

        marginals = []
        for marginal in results["marginals"]:
            var_index = marginal["variable"]
            var_name = vars_mapping[var_index]
            if var_name in query_variables:
                probs = marginal["probabilities"]
                marginals.append({"variable": var_name, "probabilities": probs})

        # Cleanup -- delete input_filename and output_filename
        if os.path.exists(input_filename): os.remove(input_filename)
        if os.path.exists(output_filename): os.remove(output_filename)

        return marginals
    
    def run_pgmpy(self) -> dict:
        """
        Run the pgmpy default inference (Variable Elimination)
        """

        # Prepare the query variables (i.e., atoms)
        query_variables = [var for var in sorted(self.atoms.keys())]

        # Get the marginals of the query variables
        marginals = []
        print(f"[FactReasoner] Computing marginals for: {query_variables}")
        for qvar in query_variables:
            infer = VariableElimination(self.markov_network)
            print(f"processing query variable: {qvar}")

            # A dict containing the marginal distributions of the query variables.
            # e.g., {"a": factor(a), "b": factor(b), "c": factor(c)}
            marginal = infer.query(
                variables=[qvar],
                elimination_order="MinFill",
                joint=False,
                show_progress=True
            )

            factor = marginal[qvar]
            norm_factor = factor.normalize(inplace=False)
            probs = [norm_factor.values[0], norm_factor.values[1]]

            marginals.append({"variable": qvar, "probabilities": probs})
        
        return marginals
    
    def score(self):
        """
        Compute the factuality score taking into consideration the contexts 
        retrieved for each of the atom in the answer.

        Factuality score = # atoms(true) / # atoms

        Intuitively, a score of 100% means that all atoms in the answer are
        factually correct. If none of them are correct, then the score is 0%. If
        only half of the atoms are correct, then the score is 50%.

        Returns:
            dict
                The results dictionary containing the marginals, factuality score i.e., a real value in [0, 1]
        """

        # Safety checks
        if len(self.atoms) == 0:
            print("WARNING: no atoms have been identified!")
        if len(self.contexts) == 0:
            print("WARNING: no contexts have been retrieved!")
        if len(self.relations) == 0:
            print("WARNING: no relationships have been identified!")

        # assert len(self.atoms) > 0
        # assert len(self.contexts) > 0
        # assert len(self.relations) > 0
        assert self.fact_graph is not None
        assert self.markov_network is not None

        if self.use_merlin:
            marginals = self.run_merlin()
        else:
            marginals = self.run_pgmpy()

        # Prepare the results
        num_true_atoms = 0
        num_uniform_atoms = 0
        avg_logprob = 0.0
        labels = {}
        for marginal in marginals:
            var = marginal["variable"]
            probs = marginal["probabilities"]

            print(f"[{var}]: Probability for {var}=0 is: {probs[0]}")
            print(f"[{var}]: Probability for {var}=1 is: {probs[1]}")
            
            # Check if atom is true or not
            if probs[1] > probs[0]:
                num_true_atoms += 1
                labels[var] = "S"
            else:
                labels[var] = "NS"

            probval = probs[1]
            if probval < 1e-6:
                probval = 1e-6
            elif probval == 0.5:
                num_uniform_atoms += 1
            avg_logprob += math.log(probval)

        # For now, return a dict with the posterior marginals of the atoms
        avg_logprob /= len(self.atoms)
        fscore = num_true_atoms / len(self.atoms)

        results = {}
        results["factuality_score"] = fscore
        results["factuality_score2"] = math.exp(avg_logprob)
        results["num_atoms"] = len(self.atoms)
        results["num_contexts"] = len(self.contexts)
        results["num_true_atoms"] = num_true_atoms
        results["num_false_atoms"] = len(self.atoms) - num_true_atoms
        results["num_uniform_atoms"] = num_uniform_atoms

        # Print the predicted labels
        str_predictions = ""
        for aid in sorted(labels.keys()):
            str_predictions += f" {aid}: {labels[aid]}"
        print(f"[FactReasoner] Predictions: {str_predictions}")

        if self.labels_human is not None:
            true_atoms = 0
            false_atoms = 0
            num_true_positive = 0
            num_true_negative = 0
            num_false_positive = 0
            num_false_negative = 0
            for aid, l in self.labels_human.items():
                if aid not in labels: continue # skip non-existing atoms
                if l == "S":
                    true_atoms += 1
                    if labels[aid] == "S":
                        num_true_positive += 1
                    else:
                        num_false_negative += 1
                else:
                    false_atoms += 1
                    if labels[aid] == "NS":
                        num_true_negative += 1
                    else:
                        num_false_positive += 1
            fscore_gold = true_atoms/len(self.labels_human)
            str_references = ""
            for aid in sorted(self.labels_human.keys()):
                str_references += f" {aid}: {self.labels_human[aid]}"
            print(f"[FactReasoner] Gold labels: {str_references}")
            print(f"[FactReasoner] Gold fscore: {fscore_gold} ({true_atoms}/{len(self.labels_human)})")
            results["gold_factuality_score"] = fscore_gold
            results["gold_true_atoms"] = true_atoms
            results["true_positive"] = num_true_positive
            results["true_negative"] = num_true_negative
            results["false_positive"] = num_false_positive
            results["false_negative"] = num_false_negative

        if len(self.topic) > 0:
            results["topic"] = self.topic
        results["input"] = self.query

        return results, marginals

def test():
    query = "When was the Apollo 14 mission to the Moon?"
    response = "The Apollo 14 mission to the Moon took place on January 31, 1971. This mission was significant as it marked the third time humans set foot on the lunar surface, with astronauts Alan Shepard and Edgar Mitchell joining Captain Stuart Roosa, who had previously flown on Apollo 13. The mission lasted for approximately 8 days, during which the crew conducted various experiments and collected samples from the lunar surface. Apollo 14 brought back approximately 70 kilograms of lunar material, including rocks, soil, and core samples, which have been invaluable for scientific research ever since."

    reasoner = FactReasoner(
        query=query,
        response=response,
        topic="Apollo 14",
        db_address="9.59.197.15",
        db_port=5000,
        db_remote=False,
        service_type="langchain",
        model="granite-3.0-8b-instruct",
        use_merlin=True,
        nli_scorer="nli_prompt",
        nli_scoring_method="logprobs",
        nli_granularity="paragraph",
        nli_checkpoint="/home/radu/ckpts/AlignScore-large.ckpt"
    )

    # Load the problem instance from a file
    # reasoner.from_json(
    #     json_file="/home/radu/git/fm-factual/examples/bio-flaherty-orig.json",
    #     is_gold=True,
    #     has_contexts=False
    # )
    json_file = "/home/radu/git/fm-factual/examples/test4.json"
    with open(json_file, "r") as f:
        data = json.load(f)
        reasoner.from_dict_with_contexts(data)

    # Build the scorer
    reasoner.build(
        top_k=3,
        n_results=3,
        response_granularity="fact",
        retrieval_method="opt2",
        has_atoms=True,
        has_contexts=True,
        decontextualize_atoms=False,
        remove_duplicates=True,
        contexts_per_atom_only=False,
        top_k_per_atom=3,
        rel_atom_context=True, 
        rel_context_context=True,
        rel_atom_atom=False,
        rel_threshold=None
    )

    results, marginals = reasoner.score()
    print(f"[FactReasoner] Marginals: {marginals}")
    print(f"[FactReasoner] Results: {results}")
    print(f"Done.")


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--labeled_dataset', 
        type=str, 
        default=None, 
        help="Path to the labeled dataset (gold)."
    )
    
    parser.add_argument(
        '--unlabeled_dataset', 
        type=str, 
        default=None, 
        help="Path to the unlabeled dataset."
    )
    
    parser.add_argument(
        '--unlabeled_predictions', 
        type=str, 
        default=None, 
        help="Path to the unlabeled predictions (goes together with unlabeled dataset)."
    )

    parser.add_argument(
        '--output_dir', 
        type=str, 
        default=None, 
        help="Path to the output directory."
    )

    parser.add_argument(
        '--dataset_name', 
        type=str, 
        default=None, 
        help="Name of the dataset."
    )

    parser.add_argument(
        '--service_type', 
        type=str, 
        default=None, 
        help="Service type (langchain, chromadb)."
    )

    parser.add_argument(
        '--model', 
        type=str, 
        default=None, 
        help="Name of the RITS model used internally"
    )

    parser.add_argument(
        '--version', 
        type=int, 
        default=1, 
        help="FactReasoner version: 1, 2 or 3"
    )

    parser.add_argument(
        '--retriever', 
        type=str, 
        default="opt2", 
        help="Context retriever option: opt1 - passage, opt2 - document"
    )

    parser.add_argument(
        '--nli_checkpoint',
        type=str,
        default="/home/radu/ckpts/AlignScore-large.ckpt",
        help="Path to the AlignScore checkpoint."
    )

    parser.add_argument(
        '--nli_scorer',
        type=str,
        default="nli_prompt",
        help="NLI scorer used: nli, nli_prompt, align"
    )

    parser.add_argument(
        '--test', 
        default=False, 
        action='store_true', 
        help="Debugging mode."
    )
    
    parser.add_argument(
        '--db_remote', 
        default=False, 
        action='store_true', 
        help="Remote wiki database service."
    )

    parser.add_argument(
        '--use_merlin', 
        default=False, 
        action='store_true', 
        help="Use the merlin inference engine instead of pgmpy (faster, better)."
    )

    args = parser.parse_args()

    if args.test == True:
        test()
        sys.exit(0)

    # FactReasoner versions:
    if args.version == 1: # 1 - context-atom relationships only, allow duplicated contexts    
        rel_context_context = False
        rel_atom_atom = False
        remove_duplicates = False
        contexts_per_atom_only = True
        rel_threshold = None
        retrieval_method = "opt2" # langchain.WikipediaRetriever; top k pages (capped at 4000chars)
        top_k_per_atom = 3
        option = "1"
    elif args.version == 2: # 2 - context-atom relationships only, no duplicated contexts    
        rel_context_context = False
        rel_atom_atom = False
        remove_duplicates = True
        contexts_per_atom_only = False
        rel_threshold = None
        retrieval_method = "opt2" # langchain.WikipediaRetriever; top k pages (capped at 4000chars)    
        top_k_per_atom = 3
        option = "2"
    elif args.version == 3: # 3 - context-atom and context-context relationships, no duplicated contexts
        rel_context_context = True
        rel_atom_atom = False
        remove_duplicates = True
        contexts_per_atom_only = False
        rel_threshold = None
        retrieval_method = "opt2" # langchain.WikipediaRetriever; top k pages (capped at 4000chars)    
        top_k_per_atom = 3
        option = "3"
    else:
        raise ValueError(f"Unknown FactReasoner version: {args.version}")
    
    if args.labeled_dataset is not None:
        print(f"[FactReasoner] Processing gold, labeled dataset ...")
        filename = args.labeled_dataset # a jsonl file

        with open(filename) as f:
            lines = f.read().splitlines()
        df_inter = pd.DataFrame(lines)
        df_inter.columns = ['json_element']
        df_inter['json_element'].apply(json.loads)
        df = pd.json_normalize(df_inter['json_element'].apply(json.loads))
        labeled_dataset = df.to_dict('records')
        
        print(f"[FactReasoner] Loading data from: {filename}")
        print(f"[FactReasoner] Found {len(labeled_dataset)} elements")

        # Check if previous results exist. If yes, load them and skip over them
        # when processing the input dataset.
        filename = "eval_results_factreasoner{}_{}_{}_{}_{}.jsonl".format(
            option,
            args.service_type,
            retrieval_method,
            args.dataset_name,
            args.model
        )
        output_filename = os.path.join(args.output_dir, filename)
        print(f"[FactReasoner] Reading previous results from: {output_filename}")
        evaluation_data = []
        if os.path.isfile(output_filename):
            with open(output_filename, "r") as f:
                lines = f.readlines()
                for line in lines:
                    evaluation_data.append(json.loads(line))

        print(f"[FactReasoner] Found {len(evaluation_data)} existing evaluations data.")

        for data in labeled_dataset: # 183 labeled bios (needs decontextualization)

            # Check if current data has been processed already
            processed = False
            for eval_data in evaluation_data:
                if eval_data["input"] == data["input"]:
                    processed = True
                    break
            if processed:
                input_query = data["input"]
                print(f"[FactReasoner] Input: {input_query} already processed.")
                continue

            # Process the data point with FactReasoner
            scorer = FactReasoner(
                query="",
                response="",
                topic="",
                db_address="9.59.197.15",
                db_port=5000,
                db_remote=args.db_remote,
                use_merlin=args.use_merlin,
                service_type=args.service_type,
                model=args.model,
                nli_scorer=args.nli_scorer,
                nli_scoring_method="logprobs",
                nli_granularity="paragraph"
            )

            # Load the problem instance from a file or dict
            # ok = scorer.from_dict(data, is_gold=True)
            ok = scorer.from_dict_with_contexts(data)
            if not ok:
                continue # annotations are null (ignore)

            # Build the scorer
            scorer.build(
                top_k=3,
                n_results=3,
                response_granularity="fact",
                retrieval_method=retrieval_method,
                remove_duplicates=remove_duplicates,
                contexts_per_atom_only=contexts_per_atom_only,
                top_k_per_atom=top_k_per_atom,
                has_atoms=True,
                has_contexts=True,
                decontextualize_atoms=False,
                rel_atom_context=True,
                rel_context_context=rel_context_context,
                rel_atom_atom=rel_atom_atom,
                rel_threshold=rel_threshold
            )

            results, marginals = scorer.score()
            results["model_name"] = args.model
            evaluation_data.append(results)
            print(f"[FactReasoner] Marginals: {marginals}")
            print(f"[FactReasoner] Results: {results}")

            # Save results to a file
            filename = "eval_results_factreasoner{}_{}_{}_{}_{}.jsonl".format(
                option,
                args.service_type,
                retrieval_method,
                args.dataset_name,
                args.model
            )
            output_filename = os.path.join(args.output_dir, filename)
            print(f"[FactReasoner] Writing results to: {output_filename}")
            with open(output_filename, "w") as f:
                for res in evaluation_data:
                    f.write(f"{json.dumps(res)}\n")

    else: # 500 unlabeled bios
        assert args.unlabeled_dataset is not None
        assert args.unlabeled_predictions is not None

        # Read the unlabeled dataset
        with open(args.unlabeled_dataset) as f:
            lines = f.read().splitlines()
        df_inter = pd.DataFrame(lines)
        df_inter.columns = ['json_element']
        df_inter['json_element'].apply(json.loads)
        df_unlabeled_dataset = pd.json_normalize(df_inter['json_element'].apply(json.loads))

        # Read the unlabeled predictions
        with open(args.unlabeled_predictions) as f:
            lines = f.read().splitlines()
        df_inter = pd.DataFrame(lines)
        df_inter.columns = ['json_element']
        df_inter['json_element'].apply(json.loads)
        df_unlabeled_predictions = pd.json_normalize(df_inter['json_element'].apply(json.loads))

        print(f"[FactReasoner] Loading data from {args.unlabeled_dataset}")

        # Merge the two pieces
        df = pd.merge(
            df_unlabeled_dataset,
            df_unlabeled_predictions,
            left_on='input',
            right_on='prompt',
            how='left'
        ).drop('prompt', axis=1)

        unlabeled_dataset = df.to_dict('records')
        print(f"[FactReasoner] Found {len(unlabeled_dataset)} elements")

        # Check if previous results exist. If yes, load them and skip over them
        # when processing the input dataset.
        filename = "eval_results_factreasoner{}_{}_{}_{}_{}.jsonl".format(
            option,
            args.service_type,
            retrieval_method,
            args.dataset_name,
            args.model
        )
        output_filename = os.path.join(args.output_dir, filename)
        print(f"[FactReasoner] Reading previous results from: {output_filename}")
        evaluation_data = []
        if os.path.isfile(output_filename):
            with open(output_filename, "r") as f:
                lines = f.readlines()
                for line in lines:
                    evaluation_data.append(json.loads(line))

        print(f"[FactReasoner] Found {len(evaluation_data)} existing evaluations data.")

        for data in unlabeled_dataset: # 500 unlabeled bios (needs decontextualization)
            scorer = FactReasoner(
                query="",
                response="",
                topic="",
                db_address="9.59.197.15",
                db_port=5000,
                db_remote=args.db_remote,
                service_type=args.service_type,
                model=args.model,
                nli_scorer=args.nli_scorer,
                nli_scoring_method="logprobs",
                nli_granularity="sentence"
            )

            # Load the problem instance from a file
            ok = scorer.from_dict(data, is_gold=False)
            if not ok:
                continue # ignore ill-formed data points

            # Build the scorer
            scorer.build(
                top_k=3,
                n_results=3,
                response_granularity="fact",
                retrieval_method=retrieval_method,
                remove_duplicates=remove_duplicates,
                contexts_per_atom_only=contexts_per_atom_only,
                top_k_per_atom=top_k_per_atom,
                has_atoms=True,
                decontextualize_atoms=True,
                rel_atom_context=True,
                rel_context_context=rel_context_context,
                rel_atom_atom=rel_atom_atom,
                rel_threshold=rel_threshold,
            )

            results, marginals = scorer.score()
            results["model_name"] = args.model
            evaluation_data.append(results)
            print(f"[FactReasoner] Marginals: {marginals}")
            print(f"[FactReasoner] Results: {results}")

            # Save results to a file
            filename = "eval_results_factreasoner{}_{}_{}_{}_{}.jsonl".format(
                option,
                args.service_type,
                retrieval_method,
                args.dataset_name,
                args.model
            )
            output_filename = os.path.join(args.output_dir, filename)
            print(f"[FactReasoner] Writing results to: {output_filename}")
            with open(output_filename, "w") as f:
                for res in evaluation_data:
                    f.write(f"{json.dumps(res)}\n")

    print("Done.")
